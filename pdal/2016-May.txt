From stefan.ziegler.de at gmail.com  Sun May  1 05:31:21 2016
From: stefan.ziegler.de at gmail.com (Stefan Ziegler)
Date: Sun, 1 May 2016 14:31:21 +0200
Subject: [pdal] Compile error with master
In-Reply-To: <CAJvo+19vhLLX+d6jQTh9iZiDYZanGNzaj_560Dks2FBCwVk5KQ@mail.gmail.com>
References: <CAJvo+18YyefGHRiX6oeVaK9Nz9z7M9zt1GFCNHeHz_67EBn1Pw@mail.gmail.com>
 <CABUeae8oJ1fyZsMbQm0TdE7CyNHyKgaBBNRmzw0BED06xAAaKQ@mail.gmail.com>
 <CACJ51z3BoN1M9Zts41A92oao30+NSv0YK92Y_xATPXw-TY=cJg@mail.gmail.com>
 <CAJvo+19DXj0Jh-ynx8Jg+MOnzrTn9vVZQCT_ZbWc2TPPe9t1+g@mail.gmail.com>
 <CAJvo+18R6HvopMCMN=ev6XV_0=pHbgddKXY_zvj81p2AGQQj4Q@mail.gmail.com>
 <CAJvo+190J5TrVvW7hrd1-i0vbt1xjYf9En2XtkygCCni8P=zjA@mail.gmail.com>
 <CACJ51z2+ROs1nVjK7MxC=S7+x+5gJJDdgW3LqkbSxy6GhJY5PA@mail.gmail.com>
 <CAJvo+19vhLLX+d6jQTh9iZiDYZanGNzaj_560Dks2FBCwVk5KQ@mail.gmail.com>
Message-ID: <CAJvo+1_reaKpgbSCXb7VfiOpZJJdV0xHfNQAvqY5P0PCPG4+Zg@mail.gmail.com>

Now "arbiter" makes some problems:

[  2%] Building CXX object
vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o
In file included from
/home/stefan/sources/pdal/vendor/arbiter/arbiter.cpp:43:0:
/home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:61: error: ‘Json’
does not name a type
     static std::unique_ptr<Fs> create(HttpPool& pool, const Json::Value&
json);
                                                             ^
/home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
expected unqualified-id before ‘&’ token
     static std::unique_ptr<Fs> create(HttpPool& pool, const Json::Value&
json);

/home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
expected ‘)’ before ‘&’ token
/home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error: static
member function ‘static std::unique_ptr<arbiter::drivers::Fs>
arbiter::drivers::Fs::create(...)’ cannot have ref-qualifier
/home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
expected ‘;’ at end of member declaration
/home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:74: error: ‘json’
does not name a type
     static std::unique_ptr<Fs> create(HttpPool& pool, const Json::Value&
json);

etc. etc.

/home/stefan/sources/pdal/vendor/arbiter/arbiter.cpp:3046:1: error:
expected ‘}’ at end of input
 } // namespace arbiter
 ^
vendor/arbiter/CMakeFiles/pdal_arbiter.dir/build.make:62: recipe for target
'vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o' failed
make[2]: *** [vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o]
Error 1
CMakeFiles/Makefile2:3788: recipe for target
'vendor/arbiter/CMakeFiles/pdal_arbiter.dir/all' failed
make[1]: *** [vendor/arbiter/CMakeFiles/pdal_arbiter.dir/all] Error 2
Makefile:160: recipe for target 'all' failed
make: *** [all] Error 2


regards
Stefan



On Sat, Apr 30, 2016 at 10:13 AM, Stefan Ziegler <
stefan.ziegler.de at gmail.com> wrote:

> Disabling in CMakeCache.txt or by cmake does not work. Still the same
> error:
>
> -- checking for one of the modules 'spatialite>=4.2.0'
> CMake Error at
> /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108
> (message):
>   Could NOT find Nitro (missing: NITRO_LIBRARIES NITRO_INCLUDE_DIR)
> (Required
>   is at least version "2.6")
> Call Stack (most recent call first):
>   /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:315
> (_FPHSA_FAILURE_MESSAGE)
>   cmake/modules/FindNitro.cmake:73 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
>   cmake/nitro.cmake:5 (find_package)
>   tools/nitfwrap/CMakeLists.txt:2 (include)
>
>
> -- Configuring incomplete, errors occurred!
> See also "/home/stefan/sources/pdal/build/CMakeFiles/CMakeOutput.log".
> See also "/home/stefan/sources/pdal/build/CMakeFiles/CMakeError.log".
>
> Isn't there also an error on line 75 in src/CMakeList.txt? The closing
> double brackets?
>
> regr
>
> On Fri, Apr 29, 2016 at 5:14 PM, Andrew Bell <andrew.bell.ia at gmail.com>
> wrote:
>
>> On Fri, Apr 29, 2016 at 9:39 AM, Stefan Ziegler <
>> stefan.ziegler.de at gmail.com> wrote:
>>
>>> Hi
>>>
>>> can anybody give a hint a about "nitro". I get some cmake errors:
>>>
>>
>> Are you processing NITF files?  If not, you can just turn it off in
>> CMakeCache.txt with:
>>
>> BUILD_PLUGIN_NITF:BOOL=OFF
>>
>> Otherwise you can get NITRO from:
>>
>> https://github.com/hobu/nitro
>>
>> --
>> Andrew Bell
>> andrew.bell.ia at gmail.com
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160501/ec2af298/attachment.html>

From howard at hobu.co  Sun May  1 07:35:11 2016
From: howard at hobu.co (Howard Butler)
Date: Sun, 1 May 2016 09:35:11 -0500
Subject: [pdal] Compile error with master
In-Reply-To: <CAJvo+1_reaKpgbSCXb7VfiOpZJJdV0xHfNQAvqY5P0PCPG4+Zg@mail.gmail.com>
References: <CAJvo+18YyefGHRiX6oeVaK9Nz9z7M9zt1GFCNHeHz_67EBn1Pw@mail.gmail.com>
 <CABUeae8oJ1fyZsMbQm0TdE7CyNHyKgaBBNRmzw0BED06xAAaKQ@mail.gmail.com>
 <CACJ51z3BoN1M9Zts41A92oao30+NSv0YK92Y_xATPXw-TY=cJg@mail.gmail.com>
 <CAJvo+19DXj0Jh-ynx8Jg+MOnzrTn9vVZQCT_ZbWc2TPPe9t1+g@mail.gmail.com>
 <CAJvo+18R6HvopMCMN=ev6XV_0=pHbgddKXY_zvj81p2AGQQj4Q@mail.gmail.com>
 <CAJvo+190J5TrVvW7hrd1-i0vbt1xjYf9En2XtkygCCni8P=zjA@mail.gmail.com>
 <CACJ51z2+ROs1nVjK7MxC=S7+x+5gJJDdgW3LqkbSxy6GhJY5PA@mail.gmail.com>
 <CAJvo+19vhLLX+d6jQTh9iZiDYZanGNzaj_560Dks2FBCwVk5KQ@mail.gmail.com>
 <CAJvo+1_reaKpgbSCXb7VfiOpZJJdV0xHfNQAvqY5P0PCPG4+Zg@mail.gmail.com>
Message-ID: <C40067C1-4D42-412B-ADE9-B94EB184F8AC@hobu.co>

Can you clear your cmake cache and re-run? PDAL master uses a library called arbiter to support convenient fetching of files from S3 and Dropbox. Arbiter depends on libjsoncpp-dev, which is either available systemwide, or a version embedded with PDAL is used. 

I presume you don't have libjsoncpp-dev installed, but there is something incorrect with the detection of this scenario to switch on the use of the embedded version. Can we see your CMake detection phase output?

Howard

 
> On May 1, 2016, at 7:31 AM, Stefan Ziegler <stefan.ziegler.de at gmail.com> wrote:
> 
> Now "arbiter" makes some problems:
> 
> [  2%] Building CXX object vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o
> In file included from /home/stefan/sources/pdal/vendor/arbiter/arbiter.cpp:43:0:
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:61: error: ‘Json’ does not name a type
>      static std::unique_ptr<Fs> create(HttpPool& pool, const Json::Value& json);
>                                                              ^
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error: expected unqualified-id before ‘&’ token
>      static std::unique_ptr<Fs> create(HttpPool& pool, const Json::Value& json);
> 
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error: expected ‘)’ before ‘&’ token
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error: static member function ‘static std::unique_ptr<arbiter::drivers::Fs> arbiter::drivers::Fs::create(...)’ cannot have ref-qualifier
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error: expected ‘;’ at end of member declaration
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:74: error: ‘json’ does not name a type
>      static std::unique_ptr<Fs> create(HttpPool& pool, const Json::Value& json);
> 
> etc. etc.
> 
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.cpp:3046:1: error: expected ‘}’ at end of input
>  } // namespace arbiter
>  ^
> vendor/arbiter/CMakeFiles/pdal_arbiter.dir/build.make:62: recipe for target 'vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o' failed
> make[2]: *** [vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o] Error 1
> CMakeFiles/Makefile2:3788: recipe for target 'vendor/arbiter/CMakeFiles/pdal_arbiter.dir/all' failed
> make[1]: *** [vendor/arbiter/CMakeFiles/pdal_arbiter.dir/all] Error 2
> Makefile:160: recipe for target 'all' failed
> make: *** [all] Error 2
> 
> 
> regards
> Stefan
> 
> 
> 
> On Sat, Apr 30, 2016 at 10:13 AM, Stefan Ziegler <stefan.ziegler.de at gmail.com> wrote:
> Disabling in CMakeCache.txt or by cmake does not work. Still the same error:
> 
> -- checking for one of the modules 'spatialite>=4.2.0'
> CMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):
>   Could NOT find Nitro (missing: NITRO_LIBRARIES NITRO_INCLUDE_DIR) (Required
>   is at least version "2.6")
> Call Stack (most recent call first):
>   /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)
>   cmake/modules/FindNitro.cmake:73 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
>   cmake/nitro.cmake:5 (find_package)
>   tools/nitfwrap/CMakeLists.txt:2 (include)
> 
> 
> -- Configuring incomplete, errors occurred!
> See also "/home/stefan/sources/pdal/build/CMakeFiles/CMakeOutput.log".
> See also "/home/stefan/sources/pdal/build/CMakeFiles/CMakeError.log".
> 
> Isn't there also an error on line 75 in src/CMakeList.txt? The closing double brackets?
> 
> regr
> 
> On Fri, Apr 29, 2016 at 5:14 PM, Andrew Bell <andrew.bell.ia at gmail.com> wrote:
> On Fri, Apr 29, 2016 at 9:39 AM, Stefan Ziegler <stefan.ziegler.de at gmail.com> wrote:
> Hi
> 
> can anybody give a hint a about "nitro". I get some cmake errors:
> 
> Are you processing NITF files?  If not, you can just turn it off in CMakeCache.txt with:
> 
> BUILD_PLUGIN_NITF:BOOL=OFF
> 
> Otherwise you can get NITRO from:
> 
> https://github.com/hobu/nitro
> 
> -- 
> Andrew Bell
> andrew.bell.ia at gmail.com
> 
> 
> _______________________________________________
> pdal mailing list
> pdal at lists.osgeo.org
> http://lists.osgeo.org/mailman/listinfo/pdal


From stefan.ziegler.de at gmail.com  Sun May  1 09:02:29 2016
From: stefan.ziegler.de at gmail.com (Stefan Ziegler)
Date: Sun, 1 May 2016 18:02:29 +0200
Subject: [pdal] Compile error with master
In-Reply-To: <C40067C1-4D42-412B-ADE9-B94EB184F8AC@hobu.co>
References: <CAJvo+18YyefGHRiX6oeVaK9Nz9z7M9zt1GFCNHeHz_67EBn1Pw@mail.gmail.com>
 <CABUeae8oJ1fyZsMbQm0TdE7CyNHyKgaBBNRmzw0BED06xAAaKQ@mail.gmail.com>
 <CACJ51z3BoN1M9Zts41A92oao30+NSv0YK92Y_xATPXw-TY=cJg@mail.gmail.com>
 <CAJvo+19DXj0Jh-ynx8Jg+MOnzrTn9vVZQCT_ZbWc2TPPe9t1+g@mail.gmail.com>
 <CAJvo+18R6HvopMCMN=ev6XV_0=pHbgddKXY_zvj81p2AGQQj4Q@mail.gmail.com>
 <CAJvo+190J5TrVvW7hrd1-i0vbt1xjYf9En2XtkygCCni8P=zjA@mail.gmail.com>
 <CACJ51z2+ROs1nVjK7MxC=S7+x+5gJJDdgW3LqkbSxy6GhJY5PA@mail.gmail.com>
 <CAJvo+19vhLLX+d6jQTh9iZiDYZanGNzaj_560Dks2FBCwVk5KQ@mail.gmail.com>
 <CAJvo+1_reaKpgbSCXb7VfiOpZJJdV0xHfNQAvqY5P0PCPG4+Zg@mail.gmail.com>
 <C40067C1-4D42-412B-ADE9-B94EB184F8AC@hobu.co>
Message-ID: <CAJvo+18pGcaGBJUwOvRKCmH4cSdYybbEpTcL6fPmtD=Y+Q6FHw@mail.gmail.com>

libjsoncpp-dev is installed (1.7.2-1) and seems to be found.

cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_PLUGIN_HEXBIN=ON
-DBUILD_PLUGIN_P2G=ON -DBUILD_PLUGIN_PGPOINTCLOUD=ON
-DBUILD_PLUGIN_PYTHON=ON -DBUILD_PLUGIN_SQLITE=ON -DWITH_APPS=ON
-DWITH_GEOTIFF=ON -DWITH_LASZIP=ON -DWITH_COMPLETION=ON

-- The CXX compiler identification is GNU 5.3.1
-- The C compiler identification is GNU 5.3.1
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Found GDAL: /usr/lib/libgdal.so (Required is at least version "1.9.0")
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version "1.2.8")
-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version
"7.47.0")
-- Found JSONCPP: /usr/lib/x86_64-linux-gnu/libjsoncpp.so (Required is at
least version "1.6.2")
-- Found LibXml2: /usr/lib/x86_64-linux-gnu/libxml2.so (found version
"2.9.3")
-- Found Hexer: /usr/local/lib/libhexer.so
-- Found P2G: /usr/local/lib/libpts2grd.so
-- Found PostgreSQL: /usr/include/postgresql
-- Found SQLITE3: /usr/lib/x86_64-linux-gnu/libsqlite3.so
-- Found PkgConfig: /usr/bin/pkg-config (found version "0.29.1")
-- Checking for one of the modules 'spatialite>=4.2.0'
-- Found PythonInterp: /usr/bin/python (found version "2.7.11")
-- 
-- The following features have been enabled:

 * PDAL application , the PDAL command line application
 * Bash completion , completion for PDAL command line
 * Hexbin plugin , determine boundary and density of a point cloud
 * PostgreSQL PointCloud plugin , read/write PostgreSQL PointCloud objects
 * SQLite plugin , read/write SQLite objects
 * Python plugin , add features that depend on python
 * Unit tests , PDAL unit tests

-- The following OPTIONAL packages have been found:

 * GeoTIFF (required version >= 1.3.0)
 * Curl
 * JSONCPP (required version >= 1.6.2)
 * PkgConfig
 * LibXml2
 * P2G

-- The following RECOMMENDED packages have been found:

 * LASzip
   Provides LASzip compression
   Provides LASzip compression

-- The following REQUIRED packages have been found:

 * GDAL (required version >= 1.9.0)
   Provides general purpose raster, vector, and reference system support
 * GEOS (required version >= 3.3)
   Provides general purpose geometry support
 * ZLIB
   Compression support in BPF
 * Hexer
 * PostgreSQL
 * SQLite3
 * Threads , The thread library of the system
 * PythonInterp
 * PythonLibs
 * NumPy (required version >= 1.5)

-- The following features have been disabled:

 * CPD plugin , run Coherent Point Drift on two datasets
 * GeoWave plugin , Read and Write data using GeoWave
 * Greyhound plugin , read points from a Greyhound server
 * Icebridge plugin , read data in the Icebridge format
 * Matlab plugin , write data to a .mat file
 * MrSID plugin , read data in the MrSID format
 * NITF plugin , read/write LAS data wrapped in NITF
 * PCL plugin , provides PCL-based readers, writers, filters, and kernels
 * RiVLib plugin , read data in the RXP format

-- Configuring done
-- Generating done
-- Build files have been written to: /home/stefan/sources/pdal/build


regards
Stefan

On Sun, May 1, 2016 at 4:35 PM, Howard Butler <howard at hobu.co> wrote:

> Can you clear your cmake cache and re-run? PDAL master uses a library
> called arbiter to support convenient fetching of files from S3 and Dropbox.
> Arbiter depends on libjsoncpp-dev, which is either available systemwide, or
> a version embedded with PDAL is used.
>
> I presume you don't have libjsoncpp-dev installed, but there is something
> incorrect with the detection of this scenario to switch on the use of the
> embedded version. Can we see your CMake detection phase output?
>
> Howard
>
>
> > On May 1, 2016, at 7:31 AM, Stefan Ziegler <stefan.ziegler.de at gmail.com>
> wrote:
> >
> > Now "arbiter" makes some problems:
> >
> > [  2%] Building CXX object
> vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o
> > In file included from
> /home/stefan/sources/pdal/vendor/arbiter/arbiter.cpp:43:0:
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:61: error:
> ‘Json’ does not name a type
> >      static std::unique_ptr<Fs> create(HttpPool& pool, const
> Json::Value& json);
> >                                                              ^
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
> expected unqualified-id before ‘&’ token
> >      static std::unique_ptr<Fs> create(HttpPool& pool, const
> Json::Value& json);
> >
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
> expected ‘)’ before ‘&’ token
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
> static member function ‘static std::unique_ptr<arbiter::drivers::Fs>
> arbiter::drivers::Fs::create(...)’ cannot have ref-qualifier
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:72: error:
> expected ‘;’ at end of member declaration
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.hpp:298:74: error:
> ‘json’ does not name a type
> >      static std::unique_ptr<Fs> create(HttpPool& pool, const
> Json::Value& json);
> >
> > etc. etc.
> >
> > /home/stefan/sources/pdal/vendor/arbiter/arbiter.cpp:3046:1: error:
> expected ‘}’ at end of input
> >  } // namespace arbiter
> >  ^
> > vendor/arbiter/CMakeFiles/pdal_arbiter.dir/build.make:62: recipe for
> target 'vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o' failed
> > make[2]: *** [vendor/arbiter/CMakeFiles/pdal_arbiter.dir/arbiter.cpp.o]
> Error 1
> > CMakeFiles/Makefile2:3788: recipe for target
> 'vendor/arbiter/CMakeFiles/pdal_arbiter.dir/all' failed
> > make[1]: *** [vendor/arbiter/CMakeFiles/pdal_arbiter.dir/all] Error 2
> > Makefile:160: recipe for target 'all' failed
> > make: *** [all] Error 2
> >
> >
> > regards
> > Stefan
> >
> >
> >
> > On Sat, Apr 30, 2016 at 10:13 AM, Stefan Ziegler <
> stefan.ziegler.de at gmail.com> wrote:
> > Disabling in CMakeCache.txt or by cmake does not work. Still the same
> error:
> >
> > -- checking for one of the modules 'spatialite>=4.2.0'
> > CMake Error at
> /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108
> (message):
> >   Could NOT find Nitro (missing: NITRO_LIBRARIES NITRO_INCLUDE_DIR)
> (Required
> >   is at least version "2.6")
> > Call Stack (most recent call first):
> >   /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:315
> (_FPHSA_FAILURE_MESSAGE)
> >   cmake/modules/FindNitro.cmake:73 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
> >   cmake/nitro.cmake:5 (find_package)
> >   tools/nitfwrap/CMakeLists.txt:2 (include)
> >
> >
> > -- Configuring incomplete, errors occurred!
> > See also "/home/stefan/sources/pdal/build/CMakeFiles/CMakeOutput.log".
> > See also "/home/stefan/sources/pdal/build/CMakeFiles/CMakeError.log".
> >
> > Isn't there also an error on line 75 in src/CMakeList.txt? The closing
> double brackets?
> >
> > regr
> >
> > On Fri, Apr 29, 2016 at 5:14 PM, Andrew Bell <andrew.bell.ia at gmail.com>
> wrote:
> > On Fri, Apr 29, 2016 at 9:39 AM, Stefan Ziegler <
> stefan.ziegler.de at gmail.com> wrote:
> > Hi
> >
> > can anybody give a hint a about "nitro". I get some cmake errors:
> >
> > Are you processing NITF files?  If not, you can just turn it off in
> CMakeCache.txt with:
> >
> > BUILD_PLUGIN_NITF:BOOL=OFF
> >
> > Otherwise you can get NITRO from:
> >
> > https://github.com/hobu/nitro
> >
> > --
> > Andrew Bell
> > andrew.bell.ia at gmail.com
> >
> >
> > _______________________________________________
> > pdal mailing list
> > pdal at lists.osgeo.org
> > http://lists.osgeo.org/mailman/listinfo/pdal
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160501/19f4bb1f/attachment-0001.html>

From Michele.L.Maxson at erdc.dren.mil  Tue May  3 13:15:57 2016
From: Michele.L.Maxson at erdc.dren.mil (Maxson, Michele L ERDC-RDE-CRREL-NH CIV)
Date: Tue, 3 May 2016 20:15:57 +0000
Subject: [pdal] Setting buffers on point clouds
Message-ID: <8A88B728678D7B41AF7E1A236CDB779946CBB2E3@MS-EX1VKS.erdc.dren.mil>

Is it possible to set buffers on tiled point cloud data?  For example, I would like to split (spatially) a point cloud into several more manageable sized point clouds for processing.  I would like each new tile to have a few meters of overlapping points in neighboring tiles that are flagged by an attribute as 'overlap' so that when ground filters, DEMs, and other filters are run on each tile there will not be edge effects at the intersections of the abutting tiles.  I'd like them tiled first because the scans are currently overlapping and I would like to run other filters on the entire dataset and not just a per scan basis, but merging the files first creates an LAZ file that is too large to handle.  I'm open to other ideas and solutions as well.

Michele L. Maxson, GISP
Research Physical Scientist
Cold Regions Research and Engineering Lab
72 Lyme Road
Hanover, NH 03755
Office: 603-646-4122
Mobile: 937-631-1915



From scott.lewis at nsidc.org  Fri May  6 09:39:01 2016
From: scott.lewis at nsidc.org (Scott Lewis)
Date: Fri, 6 May 2016 10:39:01 -0600
Subject: [pdal] Problem with wildcards in pipeline (JSON)
Message-ID: <572CC8A5.4030402@nsidc.org>

Following the instructions at http://www.pdal.io/pipeline.html to do 
globbed inputs, I put together a simple pipeline file (pipeline.json) 
containing this:

{
   "pipeline":[
     "/data/20150505/\*.h5",
     {
       "type": "filters.merge"
     },
     "/data/ILATM1B_20150508.las"
   ]
}


However, when I run the following command:

pdal pipeline pipeline.json

I get an error saying:
PDAL: PipelineReaderJSON: unable to parse pipeline


I've tried replacing the \*.h5 with *.h5, and it gives me a different 
error saying it can't find a file called *.h5.


When I replace the line with multiple lines containing the actual 
filenames, however, the pipeline works fine.

Is there a bug with the globbing?  Or does it need to be done 
differently?  (I'm running this on the Docker image)


Thanks!

Scott Lewis
NSIDC

From andrew.bell.ia at gmail.com  Fri May  6 11:29:45 2016
From: andrew.bell.ia at gmail.com (Andrew Bell)
Date: Fri, 6 May 2016 13:29:45 -0500
Subject: [pdal] Problem with wildcards in pipeline (JSON)
In-Reply-To: <572CC8A5.4030402@nsidc.org>
References: <572CC8A5.4030402@nsidc.org>
Message-ID: <CACJ51z0qA76=4PA_HbRZiLFkShUO0SJ5NAex92sxb0fnMmkj6w@mail.gmail.com>

On Fri, May 6, 2016 at 11:39 AM, Scott Lewis <scott.lewis at nsidc.org> wrote:

> Following the instructions at http://www.pdal.io/pipeline.html to do
> globbed inputs, I put together a simple pipeline file (pipeline.json)
> containing this:
>
> {
>   "pipeline":[
>     "/data/20150505/\*.h5",
>     {
>       "type": "filters.merge"
>     },
>     "/data/ILATM1B_20150508.las"
>   ]
> }
>

I didn't think this has been implemented.  It was desired, but I don't
think it has happened yet and perhaps the documentations is ahead of the
code.  I'll check and let you know more.

However, when I run the following command:
>
> pdal pipeline pipeline.json
>
> I get an error saying:
> PDAL: PipelineReaderJSON: unable to parse pipeline
>

In any case, the above error isn't helpful.  I'll fix this at the least.

-- 
Andrew Bell
andrew.bell.ia at gmail.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160506/63d0b0cf/attachment.html>

From andrew.bell.ia at gmail.com  Fri May  6 11:36:49 2016
From: andrew.bell.ia at gmail.com (Andrew Bell)
Date: Fri, 6 May 2016 13:36:49 -0500
Subject: [pdal] Problem with wildcards in pipeline (JSON)
In-Reply-To: <572CC8A5.4030402@nsidc.org>
References: <572CC8A5.4030402@nsidc.org>
Message-ID: <CACJ51z2Z+R6kZ0D3_7kVxpznWisSkar4fN0yOfexsafGkjQbxw@mail.gmail.com>

On Fri, May 6, 2016 at 11:39 AM, Scott Lewis <scott.lewis at nsidc.org> wrote:

> Following the instructions at http://www.pdal.io/pipeline.html to do
> globbed inputs, I put together a simple pipeline file (pipeline.json)
> containing this:
>
> {
>   "pipeline":[
>     "/data/20150505/\*.h5",
>     {
>       "type": "filters.merge"
>     },
>     "/data/ILATM1B_20150508.las"
>   ]
> }
>
>
> However, when I run the following command:
>
> pdal pipeline pipeline.json
>
> I get an error saying:
> PDAL: PipelineReaderJSON: unable to parse pipeline
>

This error is correct as \* is invalid in a JSON string.

I've tried replacing the \*.h5 with *.h5, and it gives me a different error
> saying it can't find a file called *.h5.
>

This error is also correct and simply indicates that we're not supporting
globbing.  Should we provide a more specific error for this case for now?

-- 
Andrew Bell
andrew.bell.ia at gmail.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160506/665ead32/attachment.html>

From howard at hobu.co  Fri May  6 12:42:39 2016
From: howard at hobu.co (Howard Butler)
Date: Fri, 6 May 2016 14:42:39 -0500
Subject: [pdal] Setting buffers on point clouds
In-Reply-To: <8A88B728678D7B41AF7E1A236CDB779946CBB2E3@MS-EX1VKS.erdc.dren.mil>
References: <8A88B728678D7B41AF7E1A236CDB779946CBB2E3@MS-EX1VKS.erdc.dren.mil>
Message-ID: <6AD4AA07-1485-426F-AD21-1686541919CE@hobu.co>


> On May 3, 2016, at 3:15 PM, Maxson, Michele L ERDC-RDE-CRREL-NH CIV <Michele.L.Maxson at erdc.dren.mil> wrote:
> 
> Is it possible to set buffers on tiled point cloud data?  For example, I would like to split (spatially) a point cloud into several more manageable sized point clouds for processing.  I would like each new tile to have a few meters of overlapping points in neighboring tiles that are flagged by an attribute as 'overlap' so that when ground filters, DEMs, and other filters are run on each tile there will not be edge effects at the intersections of the abutting tiles.  I'd like them tiled first because the scans are currently overlapping and I would like to run other filters on the entire dataset and not just a per scan basis, but merging the files first creates an LAZ file that is too large to handle.  I'm open to other ideas and solutions as well.

Michele,

It should be possible to do this, but you're going to have to make a multistage workflow to do so, and you're going to have to programmatically generate some stuff as part of that. There might also need to be a few things developed to make it easier to achieve.

Let us know as you attempt constructing a processing scenario where you end up getting stuck. We can work through it as we go.

Howard

From jfprieur at gmail.com  Wed May 11 09:03:50 2016
From: jfprieur at gmail.com (Jean-Francois Prieur)
Date: Wed, 11 May 2016 16:03:50 +0000
Subject: [pdal] [PDAL] PGPointCloud: Updating point info
Message-ID: <CAEMWU2-ZX54dCrMpOQ9PCJo_npSPouEuYXyZDB1qQ5+Pt=JnDg@mail.gmail.com>

Hello,

Newbie to PostGIS/PGPointCLoud, used to work with MySQL many moons ago so
know just enough to be dangerous!

I am working in remote sensing research , precision forestry to be specific.

We use ALS data and tree crown segmentation on the resulting canopy height
models to segment individual trees. We then use the shapefile to extract
the points from the las files. This is done using Python, it works but it
is a slow process and we have to redo it when we want to update the info on
our points.

We are looking into doing a lot of this in PostGIS and have succesfully
imported our las files using the awesome documentation provided. So we have
a working pipeline LAS->PDAL->pgPointCloud and can perform all kinds of
queries on the data so that part works.

We can do the spatial join between the shapefiles containing all our
individual crowns (there can be thousands) and then extracting all the
points under those. What we would like to do is update all the points
selected under a crown with that crowns ID so we do not have to do these
spatial joins all the time eg. we can just query the crown number and get
the points that way. We can select all the points, we just cannot update
the points with the proper info. The spatial join is quite long with
thousands of crowns, we would like to only run it once, update the points
with the proper crown ID and then work with the data that way.

I believe part of the solution is the shema used in the pipeline process, I
think we would have to add a field in a new schema for the crownid which
could then be updated with the crown id. Just not quite sure how to get
there.

Not afraid to learn on my own and digging in, just in need of a push in the
right direction!

Thanks,
JF Prieur
PhD Candidate Remote Sensing
Université de Sherbrooke
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160511/60b34944/attachment.html>

From howard at hobu.co  Wed May 11 09:12:12 2016
From: howard at hobu.co (Howard Butler)
Date: Wed, 11 May 2016 11:12:12 -0500
Subject: [pdal] [PDAL] PGPointCloud: Updating point info
In-Reply-To: <CAEMWU2-ZX54dCrMpOQ9PCJo_npSPouEuYXyZDB1qQ5+Pt=JnDg@mail.gmail.com>
References: <CAEMWU2-ZX54dCrMpOQ9PCJo_npSPouEuYXyZDB1qQ5+Pt=JnDg@mail.gmail.com>
Message-ID: <81309154-0EB4-4F6A-8019-80D2183244C6@hobu.co>


> On May 11, 2016, at 11:03 AM, Jean-Francois Prieur <jfprieur at gmail.com> wrote:
> 
> Hello,
> 
> Newbie to PostGIS/PGPointCLoud, used to work with MySQL many moons ago so know just enough to be dangerous!
> 
> I am working in remote sensing research , precision forestry to be specific.
> 
> We use ALS data and tree crown segmentation on the resulting canopy height models to segment individual trees. We then use the shapefile to extract the points from the las files. This is done using Python, it works but it is a slow process and we have to redo it when we want to update the info on our points.
> 
> We are looking into doing a lot of this in PostGIS and have succesfully imported our las files using the awesome documentation provided. So we have a working pipeline LAS->PDAL->pgPointCloud and can perform all kinds of queries on the data so that part works.
> 
> We can do the spatial join between the shapefiles containing all our individual crowns (there can be thousands) and then extracting all the points under those. What we would like to do is update all the points selected under a crown with that crowns ID so we do not have to do these spatial joins all the time eg. we can just query the crown number and get the points that way. We can select all the points, we just cannot update the points with the proper info. The spatial join is quite long with thousands of crowns, we would like to only run it once, update the points with the proper crown ID and then work with the data that way.
> 
> I believe part of the solution is the shema used in the pipeline process, I think we would have to add a field in a new schema for the crownid which could then be updated with the crown id. Just not quite sure how to get there.
> 
> Not afraid to learn on my own and digging in, just in need of a push in the right direction!

Ok, let me attempt to rephrase. You want to update some kind of classification or other attribute for each point in a pgpatch that is inside a crown polygon? I think what I would do is ingest the crown shapefiles into PostGIS, enable the pointcloud_postgis extension, and then write SQL to update the points inside the crown polygons. There is very likely a Classification attribute as part of your data, and I would just repurpose that to flag points as your tree crowns in some way. Is this on the right track?

Also, make sure your read Stephen Mather's blog posts on pgpointcloud and PDAL. He's doing something very similar to your usage and there might be some pointers in there https://smathermather.wordpress.com/tag/pdal/

Howard

From jfprieur at gmail.com  Wed May 11 09:43:51 2016
From: jfprieur at gmail.com (Jean-Francois Prieur)
Date: Wed, 11 May 2016 16:43:51 +0000
Subject: [pdal] [PDAL] PGPointCloud: Updating point info
In-Reply-To: <81309154-0EB4-4F6A-8019-80D2183244C6@hobu.co>
References: <CAEMWU2-ZX54dCrMpOQ9PCJo_npSPouEuYXyZDB1qQ5+Pt=JnDg@mail.gmail.com>
 <81309154-0EB4-4F6A-8019-80D2183244C6@hobu.co>
Message-ID: <CAEMWU2-iZZ41aTigqMrgvebwVM_2tN2kj7t5Qn+5tQOoTbcwBA@mail.gmail.com>

Wow thank you for your quick reply.

Yes, you have the gist of it, although we do classification outside of the
database (using random forest), so it is really just assigining the crown
id (from the polygon shapefile) to all the points in the patch(es) that
fall under it. That way, we can easily pull all the points from a specific
crown by ID and not spatial join. The part I am having an issue with (the
shapefiles and point cloud is already in PostGIS/pgPointCloud) is:

"then write SQL to update the points inside the crown polygons."

I am not seeing the SQL needed to do this, it is probably trivial.

Thanks a lot for that ressource as well, it is similar to what we do and
seems to be full of examples. We are trying to identfy species at the
individual tree level using ALS and multispectral imagery or multispectral
lidar.

Thanks,
JF

On Wed, May 11, 2016 at 12:12 PM Howard Butler <howard at hobu.co> wrote:

>
> > On May 11, 2016, at 11:03 AM, Jean-Francois Prieur <jfprieur at gmail.com>
> wrote:
> >
> > Hello,
> >
> > Newbie to PostGIS/PGPointCLoud, used to work with MySQL many moons ago
> so know just enough to be dangerous!
> >
> > I am working in remote sensing research , precision forestry to be
> specific.
> >
> > We use ALS data and tree crown segmentation on the resulting canopy
> height models to segment individual trees. We then use the shapefile to
> extract the points from the las files. This is done using Python, it works
> but it is a slow process and we have to redo it when we want to update the
> info on our points.
> >
> > We are looking into doing a lot of this in PostGIS and have succesfully
> imported our las files using the awesome documentation provided. So we have
> a working pipeline LAS->PDAL->pgPointCloud and can perform all kinds of
> queries on the data so that part works.
> >
> > We can do the spatial join between the shapefiles containing all our
> individual crowns (there can be thousands) and then extracting all the
> points under those. What we would like to do is update all the points
> selected under a crown with that crowns ID so we do not have to do these
> spatial joins all the time eg. we can just query the crown number and get
> the points that way. We can select all the points, we just cannot update
> the points with the proper info. The spatial join is quite long with
> thousands of crowns, we would like to only run it once, update the points
> with the proper crown ID and then work with the data that way.
> >
> > I believe part of the solution is the shema used in the pipeline
> process, I think we would have to add a field in a new schema for the
> crownid which could then be updated with the crown id. Just not quite sure
> how to get there.
> >
> > Not afraid to learn on my own and digging in, just in need of a push in
> the right direction!
>
> Ok, let me attempt to rephrase. You want to update some kind of
> classification or other attribute for each point in a pgpatch that is
> inside a crown polygon? I think what I would do is ingest the crown
> shapefiles into PostGIS, enable the pointcloud_postgis extension, and then
> write SQL to update the points inside the crown polygons. There is very
> likely a Classification attribute as part of your data, and I would just
> repurpose that to flag points as your tree crowns in some way. Is this on
> the right track?
>
> Also, make sure your read Stephen Mather's blog posts on pgpointcloud and
> PDAL. He's doing something very similar to your usage and there might be
> some pointers in there https://smathermather.wordpress.com/tag/pdal/
>
> Howard
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160511/beec9714/attachment.html>

From remi.cura at gmail.com  Wed May 11 11:19:27 2016
From: remi.cura at gmail.com (=?UTF-8?Q?R=C3=A9mi_Cura?=)
Date: Wed, 11 May 2016 20:19:27 +0200
Subject: [pdal] [PDAL] PGPointCloud: Updating point info
In-Reply-To: <CAEMWU2-iZZ41aTigqMrgvebwVM_2tN2kj7t5Qn+5tQOoTbcwBA@mail.gmail.com>
References: <CAEMWU2-ZX54dCrMpOQ9PCJo_npSPouEuYXyZDB1qQ5+Pt=JnDg@mail.gmail.com>
 <81309154-0EB4-4F6A-8019-80D2183244C6@hobu.co>
 <CAEMWU2-iZZ41aTigqMrgvebwVM_2tN2kj7t5Qn+5tQOoTbcwBA@mail.gmail.com>
Message-ID: <CAJvUf_vxrnwsF1jGhqELCjRZhMHj+Zug-7AkUaBL2NKBn3PoTQ@mail.gmail.com>

Hey,
currently updating points inside patches is little bit of a pain,
you need to explode the patch to points, convert the point to array,
change the value for every point, then reform the patches.
I'll suppose the classification attribute class_id is in position 7.

WITH my_patch AS (-- spatial join. Should be very fast, both table indexed
    SELECT patch_id, patch
    FROM patch_table, polygon_table
    WHERE ST_Intersects(patch::geometry,geom)=TRUE
)
, my_points AS ( --converting patch to point. Note that could have been
done directly in the previous CTE
    SELECT patch_id, point[1:6]|| my_new_class_id||point[8:n_dims] AS
updated_array
    FROM my_patch, pc_explode(patch) as pt, pc_get(pt) as point
)
, updated_patch AS (
    SELECT patch_id, pc_patch(updated_array,XX) AS n_patch
    FROM my_points
    GROUP BY patch_id
)
UPDATE ...

Actually, I would do this in python outside the database (if necessary
using shapely).
Why? Because it would be much faster (basically, when you use pc-explode
you are screwed)
.
If doing this in base, I would probably embed the query in a plpgsql
function to force the index use.

We could discuss this more but there are many solutions and they strongly
depend on your requirement.
For instance Pgpointcloud would be easy to extend (in C) to update a whole
dimension at a time (setting all point class_id for instance).

In another direction, it may be a better idea to not store the
classification per point, but per patch (all points in one patch would have
the same class).
To enforce this you would need to split/merge your patches when necessary,
which is fairly easy.
It would be much much faster, and would probably makes more sense for your
application (especially if you plan to access your point by tree_id).

If you need python ressources (like read write patches in binary, and
examples):
https://github.com/Remi-C/PPPP_utilities/tree/master/pointcloud

You may be interested by my working paper on classification at patch level (
http://arxiv.org/abs/1602.06920), although only a part of the paper is not
about classification,
and I'm affraid this paper is a pain to understand (working on this).

Cheers,

Rémi C

2016-05-11 18:43 GMT+02:00 Jean-Francois Prieur <jfprieur at gmail.com>:

> Wow thank you for your quick reply.
>
> Yes, you have the gist of it, although we do classification outside of the
> database (using random forest), so it is really just assigining the crown
> id (from the polygon shapefile) to all the points in the patch(es) that
> fall under it. That way, we can easily pull all the points from a specific
> crown by ID and not spatial join. The part I am having an issue with (the
> shapefiles and point cloud is already in PostGIS/pgPointCloud) is:
>
> "then write SQL to update the points inside the crown polygons."
>
> I am not seeing the SQL needed to do this, it is probably trivial.
>
> Thanks a lot for that ressource as well, it is similar to what we do and
> seems to be full of examples. We are trying to identfy species at the
> individual tree level using ALS and multispectral imagery or multispectral
> lidar.
>
> Thanks,
> JF
>
> On Wed, May 11, 2016 at 12:12 PM Howard Butler <howard at hobu.co> wrote:
>
>>
>> > On May 11, 2016, at 11:03 AM, Jean-Francois Prieur <jfprieur at gmail.com>
>> wrote:
>> >
>> > Hello,
>> >
>> > Newbie to PostGIS/PGPointCLoud, used to work with MySQL many moons ago
>> so know just enough to be dangerous!
>> >
>> > I am working in remote sensing research , precision forestry to be
>> specific.
>> >
>> > We use ALS data and tree crown segmentation on the resulting canopy
>> height models to segment individual trees. We then use the shapefile to
>> extract the points from the las files. This is done using Python, it works
>> but it is a slow process and we have to redo it when we want to update the
>> info on our points.
>> >
>> > We are looking into doing a lot of this in PostGIS and have succesfully
>> imported our las files using the awesome documentation provided. So we have
>> a working pipeline LAS->PDAL->pgPointCloud and can perform all kinds of
>> queries on the data so that part works.
>> >
>> > We can do the spatial join between the shapefiles containing all our
>> individual crowns (there can be thousands) and then extracting all the
>> points under those. What we would like to do is update all the points
>> selected under a crown with that crowns ID so we do not have to do these
>> spatial joins all the time eg. we can just query the crown number and get
>> the points that way. We can select all the points, we just cannot update
>> the points with the proper info. The spatial join is quite long with
>> thousands of crowns, we would like to only run it once, update the points
>> with the proper crown ID and then work with the data that way.
>> >
>> > I believe part of the solution is the shema used in the pipeline
>> process, I think we would have to add a field in a new schema for the
>> crownid which could then be updated with the crown id. Just not quite sure
>> how to get there.
>> >
>> > Not afraid to learn on my own and digging in, just in need of a push in
>> the right direction!
>>
>> Ok, let me attempt to rephrase. You want to update some kind of
>> classification or other attribute for each point in a pgpatch that is
>> inside a crown polygon? I think what I would do is ingest the crown
>> shapefiles into PostGIS, enable the pointcloud_postgis extension, and then
>> write SQL to update the points inside the crown polygons. There is very
>> likely a Classification attribute as part of your data, and I would just
>> repurpose that to flag points as your tree crowns in some way. Is this on
>> the right track?
>>
>> Also, make sure your read Stephen Mather's blog posts on pgpointcloud and
>> PDAL. He's doing something very similar to your usage and there might be
>> some pointers in there https://smathermather.wordpress.com/tag/pdal/
>>
>> Howard
>
>
> _______________________________________________
> pdal mailing list
> pdal at lists.osgeo.org
> http://lists.osgeo.org/mailman/listinfo/pdal
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160511/23c6b9df/attachment-0001.html>

From jfprieur at gmail.com  Wed May 11 12:53:35 2016
From: jfprieur at gmail.com (Jean-Francois Prieur)
Date: Wed, 11 May 2016 19:53:35 +0000
Subject: [pdal] [PDAL] PGPointCloud: Updating point info
In-Reply-To: <CAJvUf_vxrnwsF1jGhqELCjRZhMHj+Zug-7AkUaBL2NKBn3PoTQ@mail.gmail.com>
References: <CAEMWU2-ZX54dCrMpOQ9PCJo_npSPouEuYXyZDB1qQ5+Pt=JnDg@mail.gmail.com>
 <81309154-0EB4-4F6A-8019-80D2183244C6@hobu.co>
 <CAEMWU2-iZZ41aTigqMrgvebwVM_2tN2kj7t5Qn+5tQOoTbcwBA@mail.gmail.com>
 <CAJvUf_vxrnwsF1jGhqELCjRZhMHj+Zug-7AkUaBL2NKBn3PoTQ@mail.gmail.com>
Message-ID: <CAEMWU28JyPnDY1F+LqBztBmW6=1_0GTidP4pNyWS-kuTJ4rJpw@mail.gmail.com>

Merci beaucoup Rémi!

Thank you for the wonderful info and alternatives, this is exactly what we
needed and will keep us busy for a while. More than I expected!

Regards,
JF

On Wed, May 11, 2016 at 2:19 PM Rémi Cura <remi.cura at gmail.com> wrote:

> Hey,
> currently updating points inside patches is little bit of a pain,
> you need to explode the patch to points, convert the point to array,
> change the value for every point, then reform the patches.
> I'll suppose the classification attribute class_id is in position 7.
>
> WITH my_patch AS (-- spatial join. Should be very fast, both table indexed
>     SELECT patch_id, patch
>     FROM patch_table, polygon_table
>     WHERE ST_Intersects(patch::geometry,geom)=TRUE
> )
> , my_points AS ( --converting patch to point. Note that could have been
> done directly in the previous CTE
>     SELECT patch_id, point[1:6]|| my_new_class_id||point[8:n_dims] AS
> updated_array
>     FROM my_patch, pc_explode(patch) as pt, pc_get(pt) as point
> )
> , updated_patch AS (
>     SELECT patch_id, pc_patch(updated_array,XX) AS n_patch
>     FROM my_points
>     GROUP BY patch_id
> )
> UPDATE ...
>
> Actually, I would do this in python outside the database (if necessary
> using shapely).
> Why? Because it would be much faster (basically, when you use pc-explode
> you are screwed)
> .
> If doing this in base, I would probably embed the query in a plpgsql
> function to force the index use.
>
> We could discuss this more but there are many solutions and they strongly
> depend on your requirement.
> For instance Pgpointcloud would be easy to extend (in C) to update a whole
> dimension at a time (setting all point class_id for instance).
>
> In another direction, it may be a better idea to not store the
> classification per point, but per patch (all points in one patch would have
> the same class).
> To enforce this you would need to split/merge your patches when necessary,
> which is fairly easy.
> It would be much much faster, and would probably makes more sense for your
> application (especially if you plan to access your point by tree_id).
>
> If you need python ressources (like read write patches in binary, and
> examples):
> https://github.com/Remi-C/PPPP_utilities/tree/master/pointcloud
>
> You may be interested by my working paper on classification at patch level
> (http://arxiv.org/abs/1602.06920), although only a part of the paper is
> not about classification,
> and I'm affraid this paper is a pain to understand (working on this).
>
> Cheers,
>
> Rémi C
>
> 2016-05-11 18:43 GMT+02:00 Jean-Francois Prieur <jfprieur at gmail.com>:
>
>> Wow thank you for your quick reply.
>>
>> Yes, you have the gist of it, although we do classification outside of
>> the database (using random forest), so it is really just assigining the
>> crown id (from the polygon shapefile) to all the points in the patch(es)
>> that fall under it. That way, we can easily pull all the points from a
>> specific crown by ID and not spatial join. The part I am having an issue
>> with (the shapefiles and point cloud is already in PostGIS/pgPointCloud) is:
>>
>> "then write SQL to update the points inside the crown polygons."
>>
>> I am not seeing the SQL needed to do this, it is probably trivial.
>>
>> Thanks a lot for that ressource as well, it is similar to what we do and
>> seems to be full of examples. We are trying to identfy species at the
>> individual tree level using ALS and multispectral imagery or multispectral
>> lidar.
>>
>> Thanks,
>> JF
>>
>> On Wed, May 11, 2016 at 12:12 PM Howard Butler <howard at hobu.co> wrote:
>>
>>>
>>> > On May 11, 2016, at 11:03 AM, Jean-Francois Prieur <jfprieur at gmail.com>
>>> wrote:
>>> >
>>> > Hello,
>>> >
>>> > Newbie to PostGIS/PGPointCLoud, used to work with MySQL many moons ago
>>> so know just enough to be dangerous!
>>> >
>>> > I am working in remote sensing research , precision forestry to be
>>> specific.
>>> >
>>> > We use ALS data and tree crown segmentation on the resulting canopy
>>> height models to segment individual trees. We then use the shapefile to
>>> extract the points from the las files. This is done using Python, it works
>>> but it is a slow process and we have to redo it when we want to update the
>>> info on our points.
>>> >
>>> > We are looking into doing a lot of this in PostGIS and have
>>> succesfully imported our las files using the awesome documentation
>>> provided. So we have a working pipeline LAS->PDAL->pgPointCloud and can
>>> perform all kinds of queries on the data so that part works.
>>> >
>>> > We can do the spatial join between the shapefiles containing all our
>>> individual crowns (there can be thousands) and then extracting all the
>>> points under those. What we would like to do is update all the points
>>> selected under a crown with that crowns ID so we do not have to do these
>>> spatial joins all the time eg. we can just query the crown number and get
>>> the points that way. We can select all the points, we just cannot update
>>> the points with the proper info. The spatial join is quite long with
>>> thousands of crowns, we would like to only run it once, update the points
>>> with the proper crown ID and then work with the data that way.
>>> >
>>> > I believe part of the solution is the shema used in the pipeline
>>> process, I think we would have to add a field in a new schema for the
>>> crownid which could then be updated with the crown id. Just not quite sure
>>> how to get there.
>>> >
>>> > Not afraid to learn on my own and digging in, just in need of a push
>>> in the right direction!
>>>
>>> Ok, let me attempt to rephrase. You want to update some kind of
>>> classification or other attribute for each point in a pgpatch that is
>>> inside a crown polygon? I think what I would do is ingest the crown
>>> shapefiles into PostGIS, enable the pointcloud_postgis extension, and then
>>> write SQL to update the points inside the crown polygons. There is very
>>> likely a Classification attribute as part of your data, and I would just
>>> repurpose that to flag points as your tree crowns in some way. Is this on
>>> the right track?
>>>
>>> Also, make sure your read Stephen Mather's blog posts on pgpointcloud
>>> and PDAL. He's doing something very similar to your usage and there might
>>> be some pointers in there https://smathermather.wordpress.com/tag/pdal/
>>>
>>> Howard
>>
>>
>> _______________________________________________
>> pdal mailing list
>> pdal at lists.osgeo.org
>> http://lists.osgeo.org/mailman/listinfo/pdal
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160511/dda8459a/attachment.html>

From stefan.ziegler.de at gmail.com  Sat May 21 08:58:32 2016
From: stefan.ziegler.de at gmail.com (Stefan Ziegler)
Date: Sat, 21 May 2016 17:58:32 +0200
Subject: [pdal] compile error isnan and isinf
Message-ID: <CAJvo+1_jisfLnR+A2d9hUeLfrizHbe__raExt0rC9rv+6SEdVQ@mail.gmail.com>

Hi

I get some compile errors with isnan/isinf methods:

error: ‘isnan’ was not declared in this scope

Using "std::inan" and "std::isinf" instead works. I'm on Ubuntu 16.04.

regards
Stefan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160521/bc35b0bd/attachment.html>

From howard at hobu.co  Sat May 21 09:04:01 2016
From: howard at hobu.co (Howard Butler)
Date: Sat, 21 May 2016 11:04:01 -0500
Subject: [pdal] compile error isnan and isinf
In-Reply-To: <CAJvo+1_jisfLnR+A2d9hUeLfrizHbe__raExt0rC9rv+6SEdVQ@mail.gmail.com>
References: <CAJvo+1_jisfLnR+A2d9hUeLfrizHbe__raExt0rC9rv+6SEdVQ@mail.gmail.com>
Message-ID: <4C50A19A-7556-45B8-8ADB-7CC52A2061AF@hobu.co>

Fixed.

https://github.com/PDAL/PDAL/commit/8e21113523df56c80a8b3f8cf1820e60c3b24641


> On May 21, 2016, at 10:58 AM, Stefan Ziegler <stefan.ziegler.de at gmail.com> wrote:
> 
> Hi
> 
> I get some compile errors with isnan/isinf methods:
> 
> error: ‘isnan’ was not declared in this scope
> 
> Using "std::inan" and "std::isinf" instead works. I'm on Ubuntu 16.04.
> 
> regards
> Stefan
> 
> 
> 
> 
> 
> 
> 
> _______________________________________________
> pdal mailing list
> pdal at lists.osgeo.org
> http://lists.osgeo.org/mailman/listinfo/pdal


From joverland at lizardtech.com  Mon May 23 15:02:44 2016
From: joverland at lizardtech.com (Jason Overland)
Date: Mon, 23 May 2016 22:02:44 +0000
Subject: [pdal] Chunking/streaming support
Message-ID: <1E3D347A5FEA2C42A722B8C31D413D6764431C@nixie.extnsis.com>

Hi,
I'm trying to use PDAL's C++ API to read BPF files and integrate it into our existing API.  In our API we expose an iterator called PointIterator which is constructed from a file and a region (bounding box).  Our PointIterator has a getNextPoints() method which walks the specified region of the point cloud until there are no more points to extract.  To work within this existing API we would like to use a chunking/streaming/stripping/iteration mechanism, i.e. read at most n points at a time from the file, stop and return execution to our API user's code, and then continue where we left off on the next call to getNextPoints(), rinse and repeat until we've read all the points we're interested in.  I've been looking at the Streaming support as exemplified in StreamingTest.cpp but haven't quite been able to wrap my head around whether or not what I'm trying to achieve is currently possible.

I suspect I'm not the first or last to want this so maybe I'm just not connecting some of the dots in the API.

Does anyone have any suggestions on how I might achieve this?

Thanks,
Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160523/fc4feeab/attachment.html>

From andrew.bell.ia at gmail.com  Mon May 23 18:16:09 2016
From: andrew.bell.ia at gmail.com (Andrew Bell)
Date: Mon, 23 May 2016 20:16:09 -0500
Subject: [pdal] Chunking/streaming support
In-Reply-To: <1E3D347A5FEA2C42A722B8C31D413D6764431C@nixie.extnsis.com>
References: <1E3D347A5FEA2C42A722B8C31D413D6764431C@nixie.extnsis.com>
Message-ID: <CACJ51z2_MCvU8hKK9MZcpGXFEwvwwcEhVKcLjRjPanNZkf88kw@mail.gmail.com>

On Mon, May 23, 2016 at 5:02 PM, Jason Overland <joverland at lizardtech.com>
wrote:

> Hi,
>
> I’m trying to use PDAL’s C++ API to read BPF files and integrate it into
> our existing API.  In our API we expose an iterator called PointIterator
> which is constructed from a file and a region (bounding box).  Our
> PointIterator has a getNextPoints() method which walks the specified region
> of the point cloud until there are no more points to extract.  To work
> within this existing API we would like to use a
> chunking/streaming/stripping/iteration mechanism, i.e. read at most n
> points at a time from the file, stop and return execution to our API user’s
> code, and then continue where we left off on the next call to
> getNextPoints(), rinse and repeat until we’ve read all the points we’re
> interested in.  I’ve been looking at the Streaming support as exemplified
> in StreamingTest.cpp but haven’t quite been able to wrap my head around
> whether or not what I’m trying to achieve is currently possible.
>

The issue is that PDAL's API works in a manner opposite to yours.  PDAL
expects to call your code when a point has been read, rather than the other
way around.  PDAL handles the point buffering and so on, relieving your
code of the burden.  PDAL wants to run an entire pipeline -- it's not
intended as a stand-along point-by-point reader.  Without seeing your
processing code, I can't provide any advice on how to make it work with the
existing API.

That said, I understand that this doesn't meet your model and your model
isn't unreasonable.  I don't know where doing something like this might fit
in our current priority list, but it's probably not too large a task.

-- 
Andrew Bell
andrew.bell.ia at gmail.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160523/f06b9085/attachment.html>

From bellengj at gmail.com  Mon May 30 15:54:09 2016
From: bellengj at gmail.com (Jim Bellenger)
Date: Mon, 30 May 2016 17:54:09 -0500
Subject: [pdal] PDAL QGIS Plugin
Message-ID: <CAGtt-46ROyUq6-tf2aV7oBeefAZ+YbwUXiUQ4OPAZuSUx1Oxng@mail.gmail.com>

Is there a QGIS Plugin to build a PDAL pipeline?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/pdal/attachments/20160530/42c208d9/attachment.html>

