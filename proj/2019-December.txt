From bugs at gnu.support  Sun Dec  1 02:11:18 2019
From: bugs at gnu.support (Jean Louis)
Date: Sun, 1 Dec 2019 11:11:18 +0100
Subject: [PROJ] How to construct proper cs2cs command line arguments
In-Reply-To: <c9005030594e46969318c0ef3b15febe@sdfe.dk>
References: <20191125121524.GK22503@protected.rcdrun.com>
 <c9005030594e46969318c0ef3b15febe@sdfe.dk>
Message-ID: <20191201101118.GC4922@protected.rcdrun.com>

* Kristian Evers <kreve at sdfe.dk> [2019-11-25 13:23]:
> Jean Louis,
> 
> Take a look at the documentation for cs2cs. I believe it should answer all your questions regarding cs2cs.
> Here it is: https://proj.org/apps/cs2cs.html

Dear Kristian,

So I chose for example input coordinate system EPSG:4326 and output
EPSG:21036 and then on https://mygeodata.cloud/cs2cs/ I have selected
that, and used coordinate: -1.122159, 29.714413 for which I get the
result: -2897602.84658;13830181.544

and same result I cannot get by using:

$ cs2cs EPSG:4326 EPSG:21036
-1.122159, 29.714413
-3394656.76	9852263.62 0.00, 29.714413

What is then wrong?

Jean

From even.rouault at spatialys.com  Sun Dec  1 02:55:09 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sun, 01 Dec 2019 11:55:09 +0100
Subject: [PROJ] How to construct proper cs2cs command line arguments
In-Reply-To: <20191201101118.GC4922@protected.rcdrun.com>
References: <20191125121524.GK22503@protected.rcdrun.com>
 <c9005030594e46969318c0ef3b15febe@sdfe.dk>
 <20191201101118.GC4922@protected.rcdrun.com>
Message-ID: <18321641.me0ZmC3Eci@even-i700>

On dimanche 1 décembre 2019 11:11:18 CET Jean Louis wrote:
> * Kristian Evers <kreve at sdfe.dk> [2019-11-25 13:23]:
> > Jean Louis,
> > 
> > Take a look at the documentation for cs2cs. I believe it should answer all
> > your questions regarding cs2cs. Here it is:
> > https://proj.org/apps/cs2cs.html
> 
> Dear Kristian,
> 
> So I chose for example input coordinate system EPSG:4326 and output
> EPSG:21036 and then on https://mygeodata.cloud/cs2cs/ I have selected
> that, and used coordinate: -1.122159, 29.714413 for which I get the
> result: -2897602.84658;13830181.544
> 
> and same result I cannot get by using:
> 
> $ cs2cs EPSG:4326 EPSG:21036
> -1.122159, 29.714413
> -3394656.76	9852263.62 0.00, 29.714413
> 
> What is then wrong?

The website you mention likely uses PROJ < 6 and the deprecated PROJ.4 syntax for CRS strings. See
https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems

PROJ 6 honours axis order of the authority when using the EPSG:XXXX syntax. See
https://proj.org/faq.html#why-is-the-axis-ordering-in-proj-not-consistent for the full explanation.

So when using EPSG:4626 you must put first latitude

$ echo "29.714413 -1.122159" | cs2cs EPSG:4326 EPSG:21036
-2897602.83	13830181.55 0.00

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From mnrengineering at gmail.com  Sun Dec  1 03:53:09 2019
From: mnrengineering at gmail.com (Michel Ruijter)
Date: Sun, 1 Dec 2019 12:53:09 +0100
Subject: [PROJ] Custom coordinate reference system
Message-ID: <CAHyMf_DG+deGa5o=BFr-Eu1yYX=aMfEreH+=35+FxgSAdioY8w@mail.gmail.com>

Ls,
I have installed the new Qgis Coruna 3.10 version and I was trying to adapt
one of my previous created Custom Coordinate reference system.
Although this previous created coordinate system is still available when I
open an existing project, I cannot change or adapt it. I can also not add a
new Custom Coordinate Reference System.
 What do I do wrong?

Kind regards,


-- 
Michel Ruijter
MNRengineering
de Kempenaerstraat 82, 2341GP, Oegstgeest
mob: +31638944484, tel: +31718897137
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191201/bb4658e6/attachment.html>

From even.rouault at spatialys.com  Sun Dec  1 04:39:08 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sun, 01 Dec 2019 13:39:08 +0100
Subject: [PROJ] Custom coordinate reference system
In-Reply-To: <CAHyMf_DG+deGa5o=BFr-Eu1yYX=aMfEreH+=35+FxgSAdioY8w@mail.gmail.com>
References: <CAHyMf_DG+deGa5o=BFr-Eu1yYX=aMfEreH+=35+FxgSAdioY8w@mail.gmail.com>
Message-ID: <5905029.pxZIq30Ijk@even-i700>

Michel,

It would be fair for readers here if you mentionned that you already reported 
this to QGIS in
https://github.com/qgis/QGIS/issues/31106

and you got a detailed answer in
https://github.com/qgis/QGIS/issues/31106#issuecomment-558383488
explaining it was a QGIS limitation currently.

You could potentially add your own CRS and its transformation to WGS84 by 
hacking directly into the PROJ database. Relevant files in PROJ to edit would 
be data/sql/projected_crs.sql (and potentially geodetic_crs.sql and 
geodetic_datum.sql if there is no existing underlying Geographic CRS and datum 
in EPSG) and data/sql/helmert_transformation.sql. Or insert directly the 
entries in proj.db
You'd probably need to rerun QGIS crssync program afterwards

Hum actually as you specifically want the +exact keywords of proj=helmert, you 
should rather edit other_transformation.sql than helmert_transformation.sql, 
to add directly the pipeline transforming between the 2 underlying geographic 
CRS. A (really dummy) example of registering a PROJ pipeline string as a 
transformation is in
https://github.com/OSGeo/PROJ/blob/master/test/unit/test_factory.cpp#L2074

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From even.rouault at spatialys.com  Sun Dec  1 15:18:23 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 02 Dec 2019 00:18:23 +0100
Subject: [PROJ] Interested in helping for inclusion of grids for Spain,
	Austria, Slovakia, Brazil ?
Message-ID: <4172696.CJjv76bh8s@even-i700>

Hi,

I'm requesting help from good wills of the community to help us grow our datasets of grids at
https://github.com/OSGeo/proj-datumgrid/
Below a few candidates I've identified, mentionned in EPSG and with a number of them with
download links, but license clarification would be needed.

Community members from those countries are probably in a better position than me to
connect with the relevant authority and see if they'd be willing to adopt one of the licenses
mentionned at https://github.com/OSGeo/proj-datumgrid#about-the-datum-grid-package

Spain
-----

* Peninsula and Baleares:

Anyone from the Spanish community to help clarifying license of
https://www.fomento.gob.es/recursos_mfom/gridshift1.zip (ED50 to ETRS89),
presumably whith IGN Spain

The download link is accessible from
https://www.fomento.gob.es/organos-colegiados/consejo-superior-geografico/csg/etrs89/etrs89-nuevo-sistema-de-referencia-geodesico-oficial-en-espana

This file has a sort of CC license
(https://gist.github.com/rouault/1ff4ad2d8053e068f9dbf451572692f4) but with a
annoying non-modification clause that makes it incompatible with our requirements
(we might want to transform it to another format like GeoTIFF),
and no version/link to the actual CC license.

* Catalonia:

Grid specific to Catalonia for ED50 to ETRS89 : 100800401.gsb
File found in http://www.icgc.cat/content/download/48766/337962/version/1/file/Malla_NTv2_11_03.zip 
but no explicit license mention.
https://www.icgc.cat/en/Web/Help/Legal-notice has a vague mention of Creative Commons,
but no precise identification of which variant of it, and which version.

Austria
-------

The AT_GIS_GRID.gsb grid for MGI to ETRS89 is available at
http://www.bev.gv.at/portal/page?_pageid=713,2157075&_dad=portal&_schema=PORTAL :

http://www.bev.gv.at/pls/portal/docs/PAGE/BEV_PORTAL_CONTENT_ALLGEMEIN/0200_PRODUKTE/UNENTGELTLICHE_PRODUKTE_DES_BEV/GIS-Grid.zip

Copyright info I could find is
"Hinsichtlich seiner Datenbanken verfügt das BEV über das ausschließliche
Werknutzungsrecht gemäß §§ 40f bis 40h UrhG und das sui generis-Recht gemäß §§ 76c bis 76e UrhG."

which by itself is not super promising.

Slovakia
--------

https://www.geoportal.sk/en/geodeticke-zaklady/download/ offers for download 3 grids
referenced by EPSG:
Slovakia_ETRS89h_to_Baltic1957.gtx
Slovakia_ETRS89h_to_EVRF2007.gtx
Slovakia_JTSK03_to_JTSK.LAS / .LOS

I've emailed the contact mentionned on the site to get info regarding the license.
But wouldn't hurt to get feedback of knowledgeable people too.

Brazil
------

Looking for official download links & license for the following:

Corrego Alegre 1961 to SIRGAS 2000: CA61_003.gsb
Corrego Alegre 1970-72 to SIRGAS 2000: CA7072_003.gsb
SAD69 to SIRGAS 2000: SAD69_003.gsb
SAD69(96) to SIRGAS 2000: SAD96_003.gsb

https://www.killetsoft.de/p_gdln_e.htm has download links but official ones from IBGE
would be preferable if possible.


Of course there are certainly gaps to fill for other countries I've not mentionned...

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From kamil.drejer at ansee.pl  Sun Dec  1 23:31:48 2019
From: kamil.drejer at ansee.pl (Kamil Drejer)
Date: Mon, 2 Dec 2019 08:31:48 +0100
Subject: [PROJ] Rotated pole coordinate system - how to transform CORDEX
	data to WGS84?
Message-ID: <CAF9FD6vLVpCwmy78A6i=KdojAd6sdqEK8cG==d9UXNh1ZBS=Zg@mail.gmail.com>

Hi,
It is my first message on mailing list, so I am sorry if I am writing
something wrong.
I downloaded climatological data from the CORDEX framework. The data is in
rotated pole coordinate system. I found on GIS Stack Exchange the solution,
how to transform layers in WGSS84 to this system using* +proj=ob_tran
+o_proj=longlat +o_lon_p=-162 +o_lat_p=39.25 +lon_0=180*. But it is a way
to reproject the data on the opposite side, to WGS84? I am interested in
Euro (ER) domain - https://www.cordex.org/domains/cordex-region-euro-cordex/
I am using QGIS with GDAL 3.x, but any advice is welcome.

Best regards
-- 

*Kamil Drejer*
-----------------------------------------------------------

*Tel:*

+48 731 95 15 23

*E-mail:*

kamil.drejer at ansee.pl
-----------------------------------------------------------

*Biuro: *ul. Św. Antoniego 2
<https://maps.google.com/?q=ul.+%C5%9Aw.+Antoniego+2&entry=gmail&source=g>/4,
Brama D, Piętro IV, Pasaż Pokoyhof, 50-073 Wrocław;
*Tel. | Fax:* +48 71 398 84 16;
*Strona:* www.ansee.p <http://www.ansee.pl/>*l <http://www.ansee.pl/>;
E-mail: *biuro at ansee.pl  <biuro at ansee.pl>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191202/7e99eaa8/attachment.html>

From ccrook at linz.govt.nz  Sun Dec  1 23:41:56 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Mon, 2 Dec 2019 07:41:56 +0000
Subject: [PROJ] Using +nadgrids with cs2cs (version 4.9.3)
Message-ID: <A87E66F06E86F14B857F2EB047CDF93231478F4A@prdassexch01.ad.linz.govt.nz>

Hi

I am trying to configure a cs2cs command line to use the New Zealand NTv2 grid to convert from NZGD49 to NZGD2000.
I have tried several variants of the command below, but so far none have resulted in the output coordinates being different
from the input coordinates?

Please can anyone offer suggestions as to what would work?

echo '173 -41 0' | cs2cs -v -f %.10f +proj=latlong +ellps=GRS80 +to +proj=latlong +datum=nzgd49  +nadgrids=nzgd2kgrid0005.gdb
# ---- From Coordinate System ----
#Lat/long (Geodetic alias)
#
# +proj=latlong +ellps=GRS80
# ---- To Coordinate System ----
#Lat/long (Geodetic alias)
#
# +proj=latlong +datum=nzgd49 +nadgrids=nzgd2kgrid0005.gdb +ellps=intl
#--- following specified but NOT used
# +towgs84=59.47,-5.04,187.44,0.47,-0.1,1.024,-4.5993
173.0000000000  -41.0000000000 0.0000000000


Thanks
Chris

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From even.rouault at spatialys.com  Mon Dec  2 01:19:13 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 02 Dec 2019 10:19:13 +0100
Subject: [PROJ] Using +nadgrids with cs2cs (version 4.9.3)
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF93231478F4A@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231478F4A@prdassexch01.ad.linz.govt.nz>
Message-ID: <4097021.DidtU07drf@even-i700>

Chris,

ah, the good old times... Actually not so good, as it shows how hard it could 
be to manage to do things correctly.

So you want to do this:

# PROJ < 6

echo '173 -41 0' | cs2cs -v -f %.10f \
	+proj=latlong +ellps=intl +nadgrids=nzgd2kgrid0005.gsb \
   +to \
   +proj=latlong +ellps=GRS80 +towgs84=0,0,0

173.0001712855	-40.9982540723 0.0000450267

The left part is the "definition" of NZGD49: based on Intl 1924 and it 
transforms to WGS84 with nzgd2kgrid0005.gsb (there was a typo in the extension 
you used). You could also have used +datum=nzgd49 instead of +ellps=intl. 
Alone +datum=nzgd49 expands to "+ellps=intl 
+towgs84=59.47,-5.04,187.44,0.47,-0.1,1.024,-4.5993", and the +nadgrids will 
override the +towgs84, but that's a confusing.

The right part is the "definition" of NZGD2000: equivalent to WGS84 for our 
purpose. You *need* the +towgs84=0,0,0 argument so that both left and right 
side have a transformation term to WGS84

Cross-checking with modern times:

$ echo "-41 173 0" | src/cs2cs -f "%.10f" NZGD49 NZGD2000

-40.9982540714	173.0001712855 0.0000000000

The results are slightly different as in PROJ 6 there's no conversion in the 
geocentric domain. The grid is applied directly in the geographic 2D domain.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From jmckenna at gatewaygeomatics.com  Mon Dec  2 05:09:16 2019
From: jmckenna at gatewaygeomatics.com (Jeff McKenna)
Date: Mon, 2 Dec 2019 09:09:16 -0400
Subject: [PROJ] Interested in helping for inclusion of grids for Spain,
 Austria, Slovakia, Brazil ?
In-Reply-To: <4172696.CJjv76bh8s@even-i700>
References: <4172696.CJjv76bh8s@even-i700>
Message-ID: <d8c52951-ebfb-d2de-1bf3-3399fc6ea061@gatewaygeomatics.com>

I've forwarded this to the vibrant QGIS-Brasil community.  -jeff



On 2019-12-01 7:18 PM, Even Rouault wrote:
> Hi,
> 
> I'm requesting help from good wills of the community to help us grow our datasets of grids at
> https://github.com/OSGeo/proj-datumgrid/
> Below a few candidates I've identified, mentionned in EPSG and with a number of them with
> download links, but license clarification would be needed.
> 
> Community members from those countries are probably in a better position than me to
> connect with the relevant authority and see if they'd be willing to adopt one of the licenses
> mentionned at https://github.com/OSGeo/proj-datumgrid#about-the-datum-grid-package
> 
> Spain
> -----
> 
> * Peninsula and Baleares:
> 
> Anyone from the Spanish community to help clarifying license of
> https://www.fomento.gob.es/recursos_mfom/gridshift1.zip (ED50 to ETRS89),
> presumably whith IGN Spain
> 
> The download link is accessible from
> https://www.fomento.gob.es/organos-colegiados/consejo-superior-geografico/csg/etrs89/etrs89-nuevo-sistema-de-referencia-geodesico-oficial-en-espana
> 
> This file has a sort of CC license
> (https://gist.github.com/rouault/1ff4ad2d8053e068f9dbf451572692f4) but with a
> annoying non-modification clause that makes it incompatible with our requirements
> (we might want to transform it to another format like GeoTIFF),
> and no version/link to the actual CC license.
> 
> * Catalonia:
> 
> Grid specific to Catalonia for ED50 to ETRS89 : 100800401.gsb
> File found in http://www.icgc.cat/content/download/48766/337962/version/1/file/Malla_NTv2_11_03.zip
> but no explicit license mention.
> https://www.icgc.cat/en/Web/Help/Legal-notice has a vague mention of Creative Commons,
> but no precise identification of which variant of it, and which version.
> 
> Austria
> -------
> 
> The AT_GIS_GRID.gsb grid for MGI to ETRS89 is available at
> http://www.bev.gv.at/portal/page?_pageid=713,2157075&_dad=portal&_schema=PORTAL :
> 
> http://www.bev.gv.at/pls/portal/docs/PAGE/BEV_PORTAL_CONTENT_ALLGEMEIN/0200_PRODUKTE/UNENTGELTLICHE_PRODUKTE_DES_BEV/GIS-Grid.zip
> 
> Copyright info I could find is
> "Hinsichtlich seiner Datenbanken verfügt das BEV über das ausschließliche
> Werknutzungsrecht gemäß §§ 40f bis 40h UrhG und das sui generis-Recht gemäß §§ 76c bis 76e UrhG."
> 
> which by itself is not super promising.
> 
> Slovakia
> --------
> 
> https://www.geoportal.sk/en/geodeticke-zaklady/download/ offers for download 3 grids
> referenced by EPSG:
> Slovakia_ETRS89h_to_Baltic1957.gtx
> Slovakia_ETRS89h_to_EVRF2007.gtx
> Slovakia_JTSK03_to_JTSK.LAS / .LOS
> 
> I've emailed the contact mentionned on the site to get info regarding the license.
> But wouldn't hurt to get feedback of knowledgeable people too.
> 
> Brazil
> ------
> 
> Looking for official download links & license for the following:
> 
> Corrego Alegre 1961 to SIRGAS 2000: CA61_003.gsb
> Corrego Alegre 1970-72 to SIRGAS 2000: CA7072_003.gsb
> SAD69 to SIRGAS 2000: SAD69_003.gsb
> SAD69(96) to SIRGAS 2000: SAD96_003.gsb
> 
> https://www.killetsoft.de/p_gdln_e.htm has download links but official ones from IBGE
> would be preferable if possible.
> 
> 
> Of course there are certainly gaps to fill for other countries I've not mentionned...
> 
> Even
> 


From martin.desruisseaux at geomatys.com  Mon Dec  2 10:19:48 2019
From: martin.desruisseaux at geomatys.com (Martin Desruisseaux)
Date: Mon, 2 Dec 2019 19:19:48 +0100
Subject: [PROJ] (Angular/Linear/Scale) speed in C++ API
In-Reply-To: <1617043.kKiYMYljJr@even-i700>
References: <09ea0ce7-dca1-f287-ce5e-d66c034fa8a8@geomatys.com>
 <1617043.kKiYMYljJr@even-i700>
Message-ID: <23427071-09f2-c0ee-36fc-142388ad134f@geomatys.com>

Le 28/11/2019 à 00:19, Even Rouault a écrit :

> Couldn't find in 
> http://docs.opengeospatial.org/is/18-010r7/18-010r7.html an example 
> for a time-dependant operation nor more formal requirements, but 
> https://www.epsg-registry.org/export.htm?wkt=urn:ogc:def:coordinateOperation:EPSG::8070 
> uses those type of nodes for the units of the rate of changes
>
Just for information (not a request for change), Units of Measurements 
in ISO 19111 are defined by ISO 19103:2015 figure C.4. In addition of 
Length, Angle and Scale measures, that standard defines also Speed and 
AngularSpeed (and more). I'm not sure why the EPSG database does not use 
them.

But anyway this email is for another question. In the C++ API, class 
WKTParser provides a guessDialect(string) method. But I do not see a way 
to tell WKTParser::createFromWKT(string) that the given string is known 
to be a GDAL flavor of WKT 1, or the OGC 01-009 standard WKT 1. Both of 
them differ in the interpretation of unit of measurement in PRIMEM and 
PARAMETER elements, and it seems to me hazardous to guess the flavor 
automatically (I realize that it the case of PRIMEM we may rely on the 
prime meridian name, but it is not guaranteed to work and it may not 
solve the PARAMETER case). Did I missed something?

     Thanks

         Martin



From hellik at web.de  Mon Dec  2 10:37:53 2019
From: hellik at web.de (Helmut Kudrnovsky)
Date: Mon, 2 Dec 2019 11:37:53 -0700 (MST)
Subject: [PROJ] Interested in helping for inclusion of grids for Spain,
 Austria, Slovakia, Brazil ?
In-Reply-To: <4172696.CJjv76bh8s@even-i700>
References: <4172696.CJjv76bh8s@even-i700>
Message-ID: <1575311873163-0.post@n6.nabble.com>

Hi Even, 

I'll have a look into Austrian ones with my colleagues here in the next
days. 



-----
best regards
Helmut
--
Sent from: http://osgeo-org.1560.x6.nabble.com/PROJ-4-f3840930.html

From even.rouault at spatialys.com  Mon Dec  2 10:42:19 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 02 Dec 2019 19:42:19 +0100
Subject: [PROJ] (Angular/Linear/Scale) speed in C++ API
In-Reply-To: <23427071-09f2-c0ee-36fc-142388ad134f@geomatys.com>
References: <09ea0ce7-dca1-f287-ce5e-d66c034fa8a8@geomatys.com>
 <1617043.kKiYMYljJr@even-i700>
 <23427071-09f2-c0ee-36fc-142388ad134f@geomatys.com>
Message-ID: <1905644.ArKLy7sufd@even-i700>

Martin,

> But anyway this email is for another question. In the C++ API, class
> WKTParser provides a guessDialect(string) method. But I do not see a way
> to tell WKTParser::createFromWKT(string) that the given string is known
> to be a GDAL flavor of WKT 1, or the OGC 01-009 standard WKT 1. Both of
> them differ in the interpretation of unit of measurement in PRIMEM and
> PARAMETER elements, and it seems to me hazardous to guess the flavor
> automatically (I realize that it the case of PRIMEM we may rely on the
> prime meridian name, but it is not guaranteed to work and it may not
> solve the PARAMETER case). Did I missed something?

No, you didn't miss anything. Only the GDAL flavor/interpretation of WKT1 has 
been implemented in createFromWKT()

(For readers not familar with the issue discussed, historical considerations 
can be found in https://gdal.org/tutorials/wktproblems.html )

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From hellik at web.de  Mon Dec  2 10:42:56 2019
From: hellik at web.de (Helmut Kudrnovsky)
Date: Mon, 2 Dec 2019 11:42:56 -0700 (MST)
Subject: [PROJ] Management of datum grid files.
In-Reply-To: <2694920.VR7iRM8BGK@even-i700>
References: <3620026.VIMQX10h4f@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323144E8E0@prdassexch01.ad.linz.govt.nz>
 <2124705.S4BY28HgJC@even-i700> <23877241.Mo6V5nEIhq@even-i700>
 <93d564aa568742178d33bd47738867a9@sdfe.dk> <2222220.jTYGqXXbEZ@even-i700>
 <1574875916818-0.post@n6.nabble.com> <2072521.EARWbhcj6n@even-i700>
 <1574886663941-0.post@n6.nabble.com> <2694920.VR7iRM8BGK@even-i700>
Message-ID: <1575312176387-0.post@n6.nabble.com>

Even Rouault-2 wrote
>> that e.g. OSGeo4W puts the proj shared things into %LOCALAPPDATA%, a
>> standalone GIS software depending on proj looks into %LOCALAPPDATA% if
>> there
>> is a subfolder called e.g. proj? or the other way around, a standalone
>> GIS
>> software depending on proj puts needed files into %LOCALAPPDATA%\proj and
>> OSGeo4W looks while updating/upgrading its proj version into
>> %LOCALAPPDATA%
>> if there is a proj subfolder with shared folders? what if in
>> %LOCALAPPDATA%\proj are already files with the same name? should these be
>> just overwritten? how will be there a track of updated/ugraded/changed
>> shared proj files if different software are working/updating/upgrading
>> them
>> at the same time?
>> 
>> or should be set %PROJ_LIB% windows system wide (for all users? for just
>> one
>> user in a multiuser network?) and a standalone GIS software depending on
>> proj/OSGeo4W while updating looks for %PROJ_LIB% instead of
>> %LOCALAPPDATA%\proj?
>> 
>> will this work in the windows side of the world? ;-)
> 
> Huh, what a mess ! No wonder I stay away from Windows as much as possible
> :-)
> 
> I'd say that applications shouldn't mess with  %LOCALAPPDATA%\proj at all. 
> What will be put there will be managed by PROJ itself (in RFC4 a sqlite 
> database with a cache of downloading grid chunks. And if
> https://github.com/OSGeo/PROJ/issues/1750 comes true, with what the script 
> will install). An implementation detail.
> They might go on with the "old way" of installing the grids in the
> "PROJ_LIB" 
> they manage (or ..\bin\share\proj if we adopt Joaquim's strategy).
> 
> So an updated logic for the pj_open_lib_ex() function that attempts to
> locate 
> a grid would be in the order they are done:
> - user callback provided by user through proj_context_set_file_finder() ? 
> (does anybody use that ? Probably not)
> - search paths set through proj_context_set_search_paths()
> - PROJ_LIB env var
> - hardcoded path at build time
> - current dir
> - ..\bin\share\proj with proposed PR
> - %LOCALAPPDATA%\proj /  ${XDG_DATA_HOME}/proj for grids as files (cf
> https://
> github.com/OSGeo/PROJ/issues/1750)
> - RFC4 + networking enabled: %LOCALAPPDATA%\proj /  ${XDG_DATA_HOME}/proj
> for 
> the SQLite3 cache.db
> - RFC4 + networking enabled: network access to fetch the grid bit
> 
> Actually the current behaviour is a bit more subtle than that. For
> example, if 
> the search paths are defined, this stops the following mechanisms to be 
> attempted even if the grid is not found. Some tweakings likely needed to 
> rationalize that a bit.
> 
> 
> -- 
> Spatialys - Geospatial professional services
> http://www.spatialys.com
> _______________________________________________
> PROJ mailing list

> PROJ at .osgeo

> https://lists.osgeo.org/mailman/listinfo/proj

Sounds reasonable 



-----
best regards
Helmut
--
Sent from: http://osgeo-org.1560.x6.nabble.com/PROJ-4-f3840930.html

From martin.desruisseaux at geomatys.com  Mon Dec  2 10:51:24 2019
From: martin.desruisseaux at geomatys.com (Martin Desruisseaux)
Date: Mon, 2 Dec 2019 19:51:24 +0100
Subject: [PROJ] (Angular/Linear/Scale) speed in C++ API
In-Reply-To: <1905644.ArKLy7sufd@even-i700>
References: <09ea0ce7-dca1-f287-ce5e-d66c034fa8a8@geomatys.com>
 <1617043.kKiYMYljJr@even-i700>
 <23427071-09f2-c0ee-36fc-142388ad134f@geomatys.com>
 <1905644.ArKLy7sufd@even-i700>
Message-ID: <0e6ffc09-4200-8b82-647a-11ec39448b4b@geomatys.com>

Le 02/12/2019 à 19:42, Even Rouault a écrit :

> No, you didn't miss anything. Only the GDAL flavor/interpretation of 
> WKT1 has been implemented in createFromWKT()
>
Thanks, I will adjust the API accordingly in Java bindings.

Is OGC 01-009 version of WKT 1 unsupported at formatting time too? If 
yes, what is the difference between WKTFormatter::Convention::WKT1_GDAL 
and WKTFormatter::Convention::WKT1_ESRI?

     Martin



From even.rouault at spatialys.com  Mon Dec  2 11:06:32 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 02 Dec 2019 20:06:32 +0100
Subject: [PROJ] (Angular/Linear/Scale) speed in C++ API
In-Reply-To: <0e6ffc09-4200-8b82-647a-11ec39448b4b@geomatys.com>
References: <09ea0ce7-dca1-f287-ce5e-d66c034fa8a8@geomatys.com>
 <1905644.ArKLy7sufd@even-i700>
 <0e6ffc09-4200-8b82-647a-11ec39448b4b@geomatys.com>
Message-ID: <4912907.9ItOM0r6rb@even-i700>

On lundi 2 décembre 2019 19:51:24 CET Martin Desruisseaux wrote:
> Le 02/12/2019 à 19:42, Even Rouault a écrit :
> > No, you didn't miss anything. Only the GDAL flavor/interpretation of
> > WKT1 has been implemented in createFromWKT()
> 
> Thanks, I will adjust the API accordingly in Java bindings.
> 
> Is OGC 01-009 version of WKT 1 unsupported at formatting time too?

Yes. But is there really an un-ambiguous interpretation of OGC 01-009... ? I 
doubt so: isn't that the reason for WKT2 ? Everyone has understood it as it 
could, sometimes "rightly", somes "wrongly" and stick with it.

> If yes, what is the difference between WKTFormatter::Convention::WKT1_GDAL
> and WKTFormatter::Convention::WKT1_ESRI?

A lot... CRS names, datum names, ellipsoid names, projection method names, 
parameter names. No AUTHORITY[] or AXIS[] nodes in WKT1_ESRI. 
Basically WKT1_ESRI is what you find in shapefile .prj files

PROJ also makes an effort to use the names found in the proj.db under the ESRI 
authority when it can match the CRS object with an existing entry.

Example:

$ projinfo EPSG:2225 -o WKT1_GDAL,WKT1_ESRI
WKT1:GDAL string:
PROJCS["NAD83 / California zone 1 (ftUS)",
    GEOGCS["NAD83",
        DATUM["North_American_Datum_1983",
            SPHEROID["GRS 1980",6378137,298.257222101,
                AUTHORITY["EPSG","7019"]],
            TOWGS84[0,0,0,0,0,0,0],
            AUTHORITY["EPSG","6269"]],
        PRIMEM["Greenwich",0,
            AUTHORITY["EPSG","8901"]],
        UNIT["degree",0.0174532925199433,
            AUTHORITY["EPSG","9122"]],
        AUTHORITY["EPSG","4269"]],
    PROJECTION["Lambert_Conformal_Conic_2SP"],
    PARAMETER["latitude_of_origin",39.3333333333333],
    PARAMETER["central_meridian",-122],
    PARAMETER["standard_parallel_1",41.6666666666667],
    PARAMETER["standard_parallel_2",40],
    PARAMETER["false_easting",6561666.667],
    PARAMETER["false_northing",1640416.667],
    UNIT["US survey foot",0.304800609601219,
        AUTHORITY["EPSG","9003"]],
    AXIS["Easting",EAST],
    AXIS["Northing",NORTH],
    AUTHORITY["EPSG","2225"]]


WKT1:ESRI string:
PROJCS["NAD_1983_StatePlane_California_I_FIPS_0401_Feet",GEOGCS["GCS_North_American_1983",DATUM["D_North_American_1983",SPHEROID["GRS_1980",
6378137.0,298.257222101]],PRIMEM["Greenwich",0.0],UNIT["Degree",
0.0174532925199433]],PROJECTION["Lambert_Conformal_Conic"],PARAMETER["False_Easting",
6561666.667],PARAMETER["False_Northing",
1640416.667],PARAMETER["Central_Meridian",-122.0],PARAMETER["Standard_Parallel_1",
41.6666666666667],PARAMETER["Standard_Parallel_2",
40.0],PARAMETER["Latitude_Of_Origin",39.3333333333333],UNIT["US survey foot",
0.304800609601219]]


I selfishly implemented "only" those 2 variants to accommodate GDAL needs.

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From ccrook at linz.govt.nz  Mon Dec  2 11:28:07 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Mon, 2 Dec 2019 19:28:07 +0000
Subject: [PROJ] Using +nadgrids with cs2cs (version 4.9.3)
In-Reply-To: <4097021.DidtU07drf@even-i700>
References: <A87E66F06E86F14B857F2EB047CDF93231478F4A@prdassexch01.ad.linz.govt.nz>
 <4097021.DidtU07drf@even-i700>
Message-ID: <A87E66F06E86F14B857F2EB047CDF93231481390@prdassexch01.ad.linz.govt.nz>

Thanks Even

Glad to provide some nostalgia.

Cheers
Chris


> -----Original Message-----
> From: Even Rouault [mailto:even.rouault at spatialys.com]
> Sent: Monday, 2 December 2019 10:19 p.m.
> To: proj at lists.osgeo.org
> Cc: Chris Crook
> Subject: Re: [PROJ] Using +nadgrids with cs2cs (version 4.9.3)
>
> Chris,
>
> ah, the good old times... Actually not so good, as it shows how hard it could
> be to manage to do things correctly.
>
> So you want to do this:
>
> # PROJ < 6
>
> echo '173 -41 0' | cs2cs -v -f %.10f \
>       +proj=latlong +ellps=intl +nadgrids=nzgd2kgrid0005.gsb \
>    +to \
>    +proj=latlong +ellps=GRS80 +towgs84=0,0,0
>
> 173.0001712855        -40.9982540723 0.0000450267
>
> The left part is the "definition" of NZGD49: based on Intl 1924 and it
> transforms to WGS84 with nzgd2kgrid0005.gsb (there was a typo in the
> extension you used). You could also have used +datum=nzgd49 instead of
> +ellps=intl.
> Alone +datum=nzgd49 expands to "+ellps=intl
> +towgs84=59.47,-5.04,187.44,0.47,-0.1,1.024,-4.5993", and the +nadgrids
> +will
> override the +towgs84, but that's a confusing.
>
> The right part is the "definition" of NZGD2000: equivalent to WGS84 for our
> purpose. You *need* the +towgs84=0,0,0 argument so that both left and
> right side have a transformation term to WGS84
>
> Cross-checking with modern times:
>
> $ echo "-41 173 0" | src/cs2cs -f "%.10f" NZGD49 NZGD2000
>
> -40.9982540714        173.0001712855 0.0000000000
>
> The results are slightly different as in PROJ 6 there's no conversion in the
> geocentric domain. The grid is applied directly in the geographic 2D domain.
>
> Even
>
> --
> Spatialys - Geospatial professional services http://www.spatialys.com

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From even.rouault at spatialys.com  Mon Dec  2 12:43:57 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 02 Dec 2019 21:43:57 +0100
Subject: [PROJ] RFC4: example of converted grid
Message-ID: <5591628.HM3He99pFo@even-i700>

Hi,

I've added an example of a converted grid to the proposed GTiff format with the output
of various tools (tiffinfo, listgeo, gdalinfo) on it:

https://github.com/rouault/PROJ/blob/rfc4_remote_and_geotiff_grid/docs/source/community/rfc/rfc-4.rst#example

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From martin.desruisseaux at geomatys.com  Mon Dec  2 13:12:38 2019
From: martin.desruisseaux at geomatys.com (Martin Desruisseaux)
Date: Mon, 2 Dec 2019 22:12:38 +0100
Subject: [PROJ] (Angular/Linear/Scale) speed in C++ API
In-Reply-To: <4912907.9ItOM0r6rb@even-i700>
References: <09ea0ce7-dca1-f287-ce5e-d66c034fa8a8@geomatys.com>
 <1905644.ArKLy7sufd@even-i700>
 <0e6ffc09-4200-8b82-647a-11ec39448b4b@geomatys.com>
 <4912907.9ItOM0r6rb@even-i700>
Message-ID: <454ceb88-0903-96f2-8928-dc9b4a3affa2@geomatys.com>

Le 02/12/2019 à 20:06, Even Rouault a écrit :

> Yes. But is there really an un-ambiguous interpretation of OGC 
> 01-009... ? I doubt so: isn't that the reason for WKT2 ?
>
I think it is mostly (not completely) unambiguous. At least the units of 
measurements of PRIMEM and PARAMETER elements were clearly defined [1]. 
Some confusion that I saw in the past was caused by the fact that GEOGCS 
(for GeographicCRS) and GEOCCS (for GeocentricCRS) keywords are too 
similar (they differ only by a "G" versus "C" letter). The difference is 
so easy to miss that a past GDAL/PROJ maintainer concluded that OGC 
01-009 was self-contradicting, and thus went away with his 
interpretation in GDAL. But it was an (understandable) misreading of the 
specification rather than a specification inconsistency.

OGC 01-009 was ambiguous in some aspects, for example the projection 
parameter names were not defined except for a short list of map 
projections, but units of measurements was not an OGC 01-009 ambiguity 
(it was an OGC 99-049 ambiguity however - I think this is the origin of 
the problem). But the reality was that no matter what OGC 01-009 said, 
the software implementations were heterogeneous regarding their 
compliance to that specification. Because we had no way to distinguish 
whether a WKT 1 string is compliant with OGC O1-009 or not, we had to 
create new keywords for making clear that a WKT is using the OGC 01-009 
rules for units of measurements. Other reasons for creating WKT 2 were 
because WKT 1 was based on a conceptual model that predated ISO 19111, 
for erasing the use of WGS84 as an universal hub, and because of WKT 1 
ambiguities other than units of measurements.


>> If yes, what is the difference between 
>> WKTFormatter::Convention::WKT1_GDAL and 
>> WKTFormatter::Convention::WKT1_ESRI?
>>
> A lot... CRS names, datum names, ellipsoid names, projection method 
> names, parameter names. No AUTHORITY[] or AXIS[] nodes in WKT1_ESRI. 
> Basically WKT1_ESRI is what you find in shapefile .prj files
>
Ah okay, the differences are in the naming and in the presence of 
AUTHORITY/AXIS elements. Thanks, I will update documentation. Thanks 
also for the example.

     Martin

[1] http://www.geoapi.org/3.0/javadoc/org/opengis/referencing/doc-files/WKT.html#PRIMEM - this is copied verbatim from OGC 01-009.


From even.rouault at spatialys.com  Tue Dec  3 10:35:39 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Tue, 03 Dec 2019 19:35:39 +0100
Subject: [PROJ] RFC4: example of converted grid
In-Reply-To: <5591628.HM3He99pFo@even-i700>
References: <5591628.HM3He99pFo@even-i700>
Message-ID: <3924289.aCDWVAnfLO@even-i700>

Hi,

You'll find updates in
https://github.com/rouault/PROJ/commit/038f488cbbba4ebb5ddc88e0cbc150b4e4f1f6c4

I've worked on a converter of NTv2 files to optimized GeoTIFF, including ones with
subgrids and applied it on	GDA94_GDA2020_conformal.gsb and ntv2_0.gsb (Canada NAD27->NAD83).
It also includes the samples containing offset accuracies. I've opted for a format
that will maximize efficiency for PROJ needs. That is: the various offsets (latitude offset,
longitude offset, latitude offset accuracy, longitude offset accuracy) are compressed in
separate blocks. Latitude and longitude offset blocks are interleaved so that when needing
correction terms for a given location they are consecutive. The accuracy samples are put
at the end of file.

Conversion scripts and sample datasets at https://github.com/rouault/sample_proj_gtiff_grids/

Compression can dramatically reduce file size in some cases.
For example GDA94_GDA2020_conformal.tif is 4.8 MB large (using DEFLATE compression),
to be compared to the 83 MB of the original file, and with the same sample values.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From ccrook at linz.govt.nz  Wed Dec  4 17:59:37 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Thu, 5 Dec 2019 01:59:37 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
Message-ID: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>

Hi All

This is an invitation to comment on a straw man format for encoding a deformation model such that it can be used directly by transformation software such as PROJ.

To view and comment on the proposal go to:

https://drive.google.com/file/d/1mRr2gPAKGOgzEqnTf07n9YlSYWgioH1x/view?usp=sharing

This is offered for discussion before formalising intohttps://webmail.linz.govt.nz/owa/?ae=Item&t=IPM.Note&a=New# a proposal for development.  This is a capability that we at Land Information New Zealand have long sought - the GeoTIFF development (link below) has provides an essential capability to support this, which is a format that provide nested grids of with multiple parameters at each node.  Previous geodetic formats have either only supported single grids, or have been limited to 2 dimensional displacement.

This also follows from an optimistic PROJ enhancement request I raised a 18 months ago  - https://github.com/OSGeo/PROJ/issues/1001.  That was proposing a custom binary format for encoding a deformation model.  This proposal is a much simpler implementation that looks relatively easy to implement and requires very little additional supporting capability as it is using existing (or under development) formats for which tooling will be available.

This proposal builds on the GeoTIFF format to allow a generic specification of a time dependent coordinate transformation.

When I started writing this I was thinking the best approach was to encode the deformation model into a single GeoTIFF file using additional TIFF or GDAL metadata tags to identify which grids belonged to which deformation model component, and to their time behaviour.  In the course of writing this I changed my mind to favour a multiple grid implementation employing a single CSV file which references one or more GeoTIFF formatted nested grid files.  The rationale for this choice is described in the straw man proposal.

Curiously this is not dissimilar to the grid catalog capability Even identifies for decommissioning in GeoTIFF proposal (link below) .  However this is a more sophisticated capability in its support for different time behaviours of the component grids.

Land Information New Zealand are keen to support development of this capability, but first we want to get input from the community to ensure the capability is as useful across the geodetic community as it can be.

Thanks in advance for looking at this and offering any criticisms/suggestions/recommendations for improvement
Chris Crook

GeoTiff proposal: https://github.com/rouault/PROJ/blob/rfc4_remote_and_geotiff_grid/docs/source/community/rfc/rfc-4.rst
Grid catalog functionality: https://github.com/rouault/PROJ/blob/rfc4_remote_and_geotiff_grid/docs/source/community/rfc/rfc-4.rst#dropping-grid-catalog-functionality

Previous related conversations:
https://lists.osgeo.org/pipermail/proj/2019-November/009005.html
https://lists.osgeo.org/pipermail/proj/2019-November/009026.html

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From martin.desruisseaux at geomatys.com  Thu Dec  5 01:51:50 2019
From: martin.desruisseaux at geomatys.com (Martin Desruisseaux)
Date: Thu, 5 Dec 2019 10:51:50 +0100
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
Message-ID: <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>

Hello Chris

Thanks for the proposal, I had a quick look on it. The explanation about 
rational, required metadata, etc. are very nice. My first question would 
be who is the intended audience. Is it a proposal for the PROJ/GDAL 
community, or e.g. a starting point for standardization at OGC? The 
reason why I ask is because in my understanding, the choice of GeoTIFF 
format for PROJ/GDAL is based on pragmatic reasons specific to that 
community (close connection between GDAL and GeoTIFF since they are 
maintained by the same peoples; more familiarity with GeoTIFF than with 
HDF5 and CF-Conventions). They are valid reasons for PROJ/GDAL, but if 
the target audience is wider then I would suggest to reconsider since, 
if it was not for PROJ/GDAL convenience, I think HDF5 + CF-Conventions 
(i.e. NetCDF 4) would be more appropriate for those kind of data.

However a "GeoTIFF versus HDF5" discussion could be left for a separated 
thread. The vast majority of the proposal document would still apply in 
both formats, except the discussion about single versus multiple files 
which would not be needed anymore. Some attributes like "extent_east", 
"extent_north" would be revisited in a HDF5 format because they overlap 
(but are not identical) with standard CF-Conventions attributes, but the 
ideas would still the same.

     Regards,

         Martin



From howard at hobu.co  Thu Dec  5 08:03:30 2019
From: howard at hobu.co (Howard Butler)
Date: Thu, 5 Dec 2019 10:03:30 -0600
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>
Message-ID: <2CF9C686-B298-411E-8E33-3531D13F2810@hobu.co>



> On Dec 5, 2019, at 3:51 AM, Martin Desruisseaux <martin.desruisseaux at geomatys.com> wrote:
> 
> Hello Chris
> 
> Thanks for the proposal, I had a quick look on it. The explanation about rational, required metadata, etc. are very nice. My first question would be who is the intended audience. Is it a proposal for the PROJ/GDAL community, or e.g. a starting point for standardization at OGC? The reason why I ask is because in my understanding, the choice of GeoTIFF format for PROJ/GDAL is based on pragmatic reasons specific to that community (close connection between GDAL and GeoTIFF since they are maintained by the same peoples; more familiarity with GeoTIFF than with HDF5 and CF-Conventions). They are valid reasons for PROJ/GDAL, but if the target audience is wider then I would suggest to reconsider since, if it was not for PROJ/GDAL convenience, I think HDF5 + CF-Conventions (i.e. NetCDF 4) would be more appropriate for those kind of data.
> 
> However a "GeoTIFF versus HDF5" discussion could be left for a separated thread.

Since you brought it up, Even extensively compared and contrasted the possible choices for formats in his RFC 4 draft [1]. I don't understand the sentiment about a wider target audience, however. Who is this wider audience you speak of? Does HDF of any flavor have more ubiquitous support than TIFFs in geospatial software? Given our goals, you haven't made the case that HDF is anything other than a better GTX/NTV. It is, but we want more.

My case for GeoTIFF:

- Every credible GIS software supports GeoTIFF. 
- GeoTIFF is now an OGC standard. That pillory that held it out of some circles is now gone
- Incremental access over HTTP is possible and regularized in Cloud Optimized GeoTIFF
- geotiff.js 
- libtiff is actively participating in oss-fuzz 

Let's get something working before we worry about standardizing anything in OGC. OGC CRS activity on the topic was dormant until we started our efforts. Please give us the space to let it develop and mature before trying to stomp it into a mold that tries (and probably fails) to make it fit it every possible software scenario.

Howard

[1] https://github.com/rouault/PROJ/blob/rfc4_remote_and_geotiff_grid/docs/source/community/rfc/rfc-4.rst#discussion-on-choice-of-format <https://github.com/rouault/PROJ/blob/rfc4_remote_and_geotiff_grid/docs/source/community/rfc/rfc-4.rst#discussion-on-choice-of-format>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191205/b87bab1d/attachment.html>

From even.rouault at spatialys.com  Thu Dec  5 09:18:38 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Thu, 05 Dec 2019 18:18:38 +0100
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
Message-ID: <1938599.T2ET77XkLR@even-i700>

Chris,

A well-thought and thorough proposal !

A few questions/observations:

- regarding "The horizontal displacement is represented by metres east and 
north. Typically base datum will be a geographic coordinate system using 
latitude and longitude and it would be more efficient to define displacement 
in terms of degrees.".
Is it the same convention as PROJ deformation method
(https://proj.org/operations/transformations/deformation.html) which uses 
grids to store corrections in the ENU space as far as I understand ? 

(more a question for Kristian: I presume this ENU space is the same as the UVW 
topocentric coordinate system described in 4.1.2 "Geocentric/topocentric 
conversions" of 
https://www.iogp.org/wp-content/uploads/2019/09/373-07-02.pdf ?)

For the easting, northing I presume yes. But I'm not sure if the convention 
you mention for vertical correction "The vertical direction will be the local 
vertical of the coordinate system." corresponds to the U of ENU. Or perhaps 
this falls in your remark "For deformation
there is no practical need to distinguish gravitational and geometric 
vertical" ?
That said, PROJ deformation method is a bit different in that it is meant to 
perate with input and output coordinates in geocentric space: it uses 
internally a geocentric to geodetic conversion to be able to retrieve the 
correction values from the grid(s), and then convert those corrections from 
the ENU space to the geocentric space.

- regarding "Strictly the inverse calculation of the deformation model is an 
iterative calculation, as the location of a point in the interpolation 
coordinate system is not known until it is transformed back to that system."
I just want to mention that PROJ uses an iterative method for the inverse of 
horizontal shift grids (NTv2 etc), so wouldn't be a problem to use that for 
deformation models as well. Actually PROJ deformation method also uses that 
iterative method.

- regarding nesting of grids, is my understanding correct that if you have a 
grid G, that has subgrids G_1 et G_2, and that a point falls in G and G_1, 
then the corections of both G and G_1 should be applied ? Just wanted to 
underline this, as it is different of the NTv2 model where you apply one or 
the other (preferably G_1), but not both.

- it could help for a better understanding to have a small example of a CSV 
file illustrating the different concepts you expose. Nested grids, different 
time functions etc.

- The approach master CSV file + separate grids in dedicated files has its 
merits indeed. The actual grid format, GTiff, HDF5 or whatever raw binary 
format that can support multiple samples per grid node will make little 
difference because the specificities/complexities of the description of the 
deformation model is in the CSV file itself.

- So if a earthquake happens, basically you'll create a new grid in the area 
that is affected, and create a new version of the CSV file where you add an 
extra row that references it, right ? (I'm probably over-simplifying)

- Any indication of the number of grids a deformation model might have ? For 
the NZ case for example

- Just a detail: regarding "velocity f(t) = (t – t0)/(365.2425 days)", if time 
and volicities are expressed respectively in decimal year and 
some_linear_unit/decimal_year, that should probably be just f(t) = t-t0

- I'm still having issues to grasp the practical implications of the "reverse 
step". Is my understanding correct that a coordinate computed for example for 
NZGD2000 @ 2010.0 with a deformation model of 2019 might potentially change 
with a deformation model of 2020 ? Should the deformation model used be also 
captured in the metadata associated with a coordinate? If so, it looks like a 
change in the deformation model would be better handled by defining a revised 
version of the datum (but I know you don't want to publish a new NZGD datum 
every 6 month). I'm super confused... I'm pretty sure we had that discussion 
already, but I've forgotten the outcome. I'd be very interested in having a 
concrete example of a workflow of how users are supposed to use such 
deformation models, and which metadata they are supposed to store to be able 
to later reprocess their coordinates.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From ccrook at linz.govt.nz  Thu Dec  5 10:00:30 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Thu, 5 Dec 2019 18:00:30 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>,
 <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>
Message-ID: <A87E66F06E86F14B857F2EB047CDF9323148BA31@prdassexch01.ad.linz.govt.nz>

Hi Martin

Thanks for your comments.

Who is the intended audience?  Short answer is anyone interested!

>From our (LINZ) point of view our primary motive is to get something going,
and having it working in PROJ/GDAL would be a huge step forward.  The GeoTIFF
format provides us the opportunity to do that, as without that there is not a suitable
grid format available to build the general deformation model with.

As you rightly say there is also a lot value in a standardisation process with OGC,
though of course that is a (much much) longer process.  Certainly that is something
that elements of this could feed in to.  It could end up with something similar to
this, in which case it would be great for us, or it may end up with something different, in
which case it wouldn't work with the current LINZ deformation model in any case.
Either way it is useful for us to have this capability now.

So initially the audience is anyone else who thinks they might use it, and particularly
anyone who thinks they could use it, but it would need to be just a bit different for
it to work for them.  Primarily I imagine that would be from geodetic agencies who
are using or developing deformation models.  I did forwarded the proposal to the IAG
working group on time dependent datums.

On HDF5 vs GeoTIFF.. As you rightly say apart from some details this proposal
is largely format agnostic.  I know the ESRI GGXF initiative is now proposing using
HDF5, and certainly that could serve as well.  Curiously I did propose HDF5 as an
option for deformation models a while ago, but at that time there wasn't much
appetite for using it as (IIRC) it wasn't so well suited to deploying with online web services,
>From a PROJ point of view GeoTIFF will already be there.  Using HDF5 would add another
huge dependency.

The choice of multiple/single file implementation was a surprise for me, as I've
been wanting to come up with a single file implementation for a long time.  But I
found as I was developing this proposal that there seemed to be more advantages
to a multiple file implementation.  Because GeoTIFF provides the nested grid capability
the number of files is not excessive (unlike the LINZ publication format as individual
grids).  One advantage is that different versions of the deformation model will only differ in a few
of the component files.  Most would be common to the versions, so using a multiple
file implementation allows these to be shared.

Cheers
Chris

________________________________________
From: PROJ [proj-bounces at lists.osgeo.org] on behalf of Martin Desruisseaux [martin.desruisseaux at geomatys.com]
Sent: Thursday, 5 December 2019 10:52 p.m.
To: proj at lists.osgeo.org
Subject: Re: [PROJ] Proposal for a geodetic deformation model format

Hello Chris

Thanks for the proposal, I had a quick look on it. The explanation about
rational, required metadata, etc. are very nice. My first question would
be who is the intended audience. Is it a proposal for the PROJ/GDAL
community, or e.g. a starting point for standardization at OGC? The
reason why I ask is because in my understanding, the choice of GeoTIFF
format for PROJ/GDAL is based on pragmatic reasons specific to that
community (close connection between GDAL and GeoTIFF since they are
maintained by the same peoples; more familiarity with GeoTIFF than with
HDF5 and CF-Conventions). They are valid reasons for PROJ/GDAL, but if
the target audience is wider then I would suggest to reconsider since,
if it was not for PROJ/GDAL convenience, I think HDF5 + CF-Conventions
(i.e. NetCDF 4) would be more appropriate for those kind of data.

However a "GeoTIFF versus HDF5" discussion could be left for a separated
thread. The vast majority of the proposal document would still apply in
both formats, except the discussion about single versus multiple files
which would not be needed anymore. Some attributes like "extent_east",
"extent_north" would be revisited in a HDF5 format because they overlap
(but are not identical) with standard CF-Conventions attributes, but the
ideas would still the same.

     Regards,

         Martin


_______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org
https://lists.osgeo.org/mailman/listinfo/proj

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From martin.desruisseaux at geomatys.com  Thu Dec  5 10:40:57 2019
From: martin.desruisseaux at geomatys.com (Martin Desruisseaux)
Date: Thu, 5 Dec 2019 19:40:57 +0100
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <2CF9C686-B298-411E-8E33-3531D13F2810@hobu.co>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>
 <2CF9C686-B298-411E-8E33-3531D13F2810@hobu.co>
Message-ID: <67bac58c-cd0b-437f-6e9c-3fbe2adc4e1f@geomatys.com>

Hello Howard

Le 05/12/2019 à 17:03, Howard Butler a écrit :

> Since you brought it up, Even extensively compared and contrasted the 
> possible choices for formats in his RFC 4 draft [1].
>
This is true that Even extensively compared the choices. But advantages 
and inconvenient beyond the ones mentioned in that page have been 
discussed by email (CF-Conventions allowing WKT in netCDF files, 
benchmarking done by other peoples, existence of standards for encoding 
uncertainties, needs for multi-dimensional grids, etc.). However it 
appeared during that discussion that GDAL/PROJ convenience was a major 
driver for the GeoTIFF choice. I do not challenge that, this is a good 
reason for this project.


> I don't understand the sentiment about a wider target audience, 
> however. Who is this wider audience you speak of?
>
I'm thinking to ESRI among others, who started experiments on this topic 
for many years. They presented their current state of work during OGC 
meeting last month (Even was present by teleconference). They 
experimented a custom format a few years ago, but finally selected 
netCDF. I think their choice is not final and I presume they are open to 
debate, but they have good reasons for selecting netCDF.

Other audience are the various geospatial projects in Java.


> Does HDF of any flavor have more ubiquitous support than TIFFs in 
> geospatial software?
>
It depends on the community. For scientists and software like MatLab or 
NASA Panoply, NetCDF is as ubiquitous, if not more, than GeoTIFF. At 
least in oceanography (my background), almost all data are in NetCDF and 
rarely in GeoTIFF. HDF5 and NetCDF are standard formats at NASA as far 
as I know, and European Sentinel 3 data (a few TB/day) are in NetCDF. 
I'm not questioning that GeoTIFF is used a lot, but I think that 
NetCDF/HDF5 are also ubiquitous enough for consideration as candidate 
formats.


> My case for GeoTIFF:
>
> - Every credible GIS software supports GeoTIFF.
> - GeoTIFF is now an OGC standard. That pillory that held it out of 
> some circles is now gone
> - Incremental access over HTTP is possible and regularized in Cloud 
> Optimized GeoTIFF
> - geotiff.js
> - libtiff is actively participating in oss-fuzz

There are good cases, but to my knowledge all those points have their 
counterpart in HDF5. There is even JavaScript HDF5 readers ("HDF5 
Node.js", "jsfive") and pure Python readers ("pyfive"). On the other 
side, there is cases for HDF5 for which the GeoTIFF counterparts require 
hacks:

  * Multi-dimensional grids. Email discussion pointed-out a need for a
    temporal axis. It can be done with multi-images GeoTIFF, but this is
    not as clean as a truly multi-dimensional file format.
  * Addition of custom attributes. The GeoTIFF approach require to put
    everything in a GDAL-specific entry.
  * Standard for expressing uncertainties. The GeoTIFF approach requires
    the invention of new conventions.


>
> OGC CRS activity on the topic was dormant until we started our efforts.

The timing between PROJ starting this effort and OGC discussing this 
topic is a coincidence. The discussion at OGC did not started because of 
PROJ effort, but because the completion of ISO 19111 and ISO 19162 
revisions released some resources that can now be used for this topic.

     Regards,

         Martin


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191205/243ca0d6/attachment.html>

From martin.desruisseaux at geomatys.com  Thu Dec  5 11:41:10 2019
From: martin.desruisseaux at geomatys.com (Martin Desruisseaux)
Date: Thu, 5 Dec 2019 20:41:10 +0100
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF9323148BA31@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <63cc57ed-9a77-ed06-4114-e1c19d8642f8@geomatys.com>
 <A87E66F06E86F14B857F2EB047CDF9323148BA31@prdassexch01.ad.linz.govt.nz>
Message-ID: <e44b7e5b-b223-c6e4-0a00-14557cec7ffd@geomatys.com>

Hello Chris

Thanks for your reply.

Le 05/12/2019 à 19:00, Chris Crook a écrit :

> As you rightly say there is also a lot value in a standardisation 
> process with OGC, though of course that is a (much much) longer process.
>
Just for information, from the presentation we had at OGC, it seems that 
ESRI wants to go relatively fast too (1~3 years in my understanding).


> On HDF5 vs GeoTIFF.. As you rightly say apart from some details this 
> proposal is largely format agnostic. I know the ESRI GGXF initiative 
> is now proposing using HDF5, and certainly that could serve as well.
>
A possible approach could be to put table(s) mapping the document 
attributes to CF-Conventions attributes when a match exists (i.e. to 
draft what it would looks like if the data were in a HDF5 file). It may 
help to compare the formats. But I realize that it would require more 
work, this is not a request.


> Curiously I did propose HDF5 as an option for deformation models a 
> while ago, but at that time there wasn't much appetite for using it as 
> (IIRC) it wasn't so well suited to deploying with online web services.
>
Its seems that HDF5 viewers exist in JavaScript since 2016 [1], and 
other readers are yet more recent [2]. Maybe they were missing at time 
of your proposal. But I admit that HDF5 support in browsers is not as 
old as GeoTIFF supports.


> (…snip…) I found as I was developing this proposal that there seemed 
> to be more advantages to a multiple file implementation. Because 
> GeoTIFF provides the nested grid capability the number of files is not 
> excessive (unlike the LINZ publication format as individual grids). 
> One advantage is that different versions of the deformation model will 
> only differ in a few of the component files. Most would be common to 
> the versions, so using a multiple file implementation allows these to 
> be shared.
>
Thanks for the insight. Together with Even questions in his previous 
email it help me to understand better.

     Regards,

         Martin

[1] https://github.com/HDF-NI
[2] https://github.com/usnistgov/jsfive

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191205/32adfb3b/attachment.html>

From ccrook at linz.govt.nz  Thu Dec  5 14:50:06 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Thu, 5 Dec 2019 22:50:06 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <1938599.T2ET77XkLR@even-i700>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
Message-ID: <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>

Hi Even

Thanks for detailed reading and questions.

> - regarding "The horizontal displacement is represented by metres east and
> north. Typically base datum will be a geographic coordinate system using
> latitude and longitude and it would be more efficient to define displacement
> in terms of degrees.".
> Is it the same convention as PROJ deformation method
> (https://proj.org/operations/transformations/deformation.html) which uses
> grids to store corrections in the ENU space as far as I understand ?
>
> (more a question for Kristian: I presume this ENU space is the same as the
> UVW topocentric coordinate system described in 4.1.2
> "Geocentric/topocentric conversions" of https://www.iogp.org/wp-
> content/uploads/2019/09/373-07-02.pdf ?)
>
> For the easting, northing I presume yes. But I'm not sure if the convention
> you mention for vertical correction "The vertical direction will be the local
> vertical of the coordinate system." corresponds to the U of ENU. Or perhaps
> this falls in your remark "For deformation there is no practical need to
> distinguish gravitational and geometric vertical" ?
> That said, PROJ deformation method is a bit different in that it is meant to
> perate with input and output coordinates in geocentric space: it uses
> internally a geocentric to geodetic conversion to be able to retrieve the
> correction values from the grid(s), and then convert those corrections from
> the ENU space to the geocentric space.

I hadn't realised PROJ would be working in a geocentric space.

What I was thinking when I suggested degrees would be more efficient was that
had converted coordinates to geographic lon/lat/ellipsoidal height (as you need lat/lon
to interpolate), then applying a correction to the lon/lat coordiantes in metres requires (a little)
more calculation than applying corrections in degrees.   This could make more difference
for an inverse calculation, as if you are iterating you could end up converting multiple times
between geocentric and geographic (and geocentric to geographic is itself iterative).

However if we are applying the correction to geocentric coordinates then
this doesn't apply.

For the inverse calculation it is probably sensible to work in the geographic coordinates
if the solution is iterated, and then convert to geocentric once the inverse
deformation has been calculated.

> - regarding "Strictly the inverse calculation of the deformation model is an
> iterative calculation, as the location of a point in the interpolation coordinate
> system is not known until it is transformed back to that system."
> I just want to mention that PROJ uses an iterative method for the inverse of
> horizontal shift grids (NTv2 etc), so wouldn't be a problem to use that for
> deformation models as well. Actually PROJ deformation method also uses
> that iterative method.

The NTv2 grids were originally designed for transformations between quite
different datums that pre-dated satellite geodesy, and for which coordinate
changes of a few hundred metres are quite likely, so in that case the iteration
could be quite important.

The coordinate changes due to deformation are likely to be much smaller (hopefully!).
I think 10 metres would be an upper bound.  On the other hand the deformation will
vary more rapidly with location in the event of earthquake related deformation.  I'll
do some testing with the LINZ deformation model to see what difference it makes.

But good to know that PROJ will do it the right way :-)

> - regarding nesting of grids, is my understanding correct that if you have a
> grid G, that has subgrids G_1 et G_2, and that a point falls in G and G_1, then
> the corections of both G and G_1 should be applied ? Just wanted to
> underline this, as it is different of the NTv2 model where you apply one or
> the other (preferably G_1), but not both.

No - I've tried to make this clearer in the document.  The proposal is to use the
same approach as eg NTv2.   A single component (eg deformation due to an
earthquake) may be defined by multiple grids.  At any location only one of those
grids will apply.

However the deformation model as a whole has multiple components (eg
secular velocity, earthquake 1, earthquake 2, ...) and these are evaluated
separately and summed together.

> - it could help for a better understanding to have a small example of a CSV
> file illustrating the different concepts you expose. Nested grids, different
> time functions etc.

Good idea - I'll build a CSV file matching the NZGD2000 deformation model.

> - The approach master CSV file + separate grids in dedicated files has its
> merits indeed. The actual grid format, GTiff, HDF5 or whatever raw binary
> format that can support multiple samples per grid node will make little
> difference because the specificities/complexities of the description of the
> deformation model is in the CSV file itself.
>
> - So if a earthquake happens, basically you'll create a new grid in the area that
> is affected, and create a new version of the CSV file where you add an extra
> row that references it, right ? (I'm probably over-simplifying)

That would be the most likely result.  For the last earthquake Kaikoura there
would be two rows, as we treated near field and far field deformation
differently (we still need to write this up).  Also we had two releases of the
deformation model.  The second release was about a year later once we
had better information about the total deformation (the update is an awkward
and largely inseparable mix of better estimation of co-seismic deformation
and additional post-seismic deformation).

> - Any indication of the number of grids a deformation model might have ? For
> the NZ case for example

The NZGD2000 has a secular velocity grid plus grids for 12 events since 2000.0
124 grids at last  count I think, but that is individual grids, not the number of nested grid.
Each event could be represented by 2 or 3 components with different time
functions, so probably between 20 and 40 in total.

However as our ability to measure and model deformation quickly is improving
then the rate of  generating grids increases.

> - Just a detail: regarding "velocity f(t) = (t – t0)/(365.2425 days)", if time and
> volicities are expressed respectively in decimal year and
> some_linear_unit/decimal_year, that should probably be just f(t) = t-t0

Yes - I copied this directly from the LINZ deformation model documentation.
Practically this is no different to expressing the time in decimal years.  I haven't
defined anywhere the way in which date/time are defined, but we use
a full date/time format rather than decimal years for defining the model as
that is more intuitive for earthquakes.  And of course most data we are transforming
has a specific date/time.   So time differences are determined in terms of "real"
time units such as seconds, days, or whatever.

> - I'm still having issues to grasp the practical implications of the "reverse
> step". Is my understanding correct that a coordinate computed for example
> for
> NZGD2000 @ 2010.0 with a deformation model of 2019 might potentially
> change with a deformation model of 2020 ? Should the deformation model
> used be also captured in the metadata associated with a coordinate? If so, it
> looks like a change in the deformation model would be better handled by
> defining a revised version of the datum (but I know you don't want to publish
> a new NZGD datum every 6 month). I'm super confused... I'm pretty sure we

It absolutely is confusing!   Technically whenever we change the deformation
model it is a new version of NZGD2000.  However the world, and particular the
users of NZGD2000 coordinates,  was not ready for this, and possibly still isn't.
Practically the time required to propagate new coordinate systems through the
made creating multiple versions of the coordinate system impractical.   Also there
are 30 projection coordinate systems associated NZGD2000, so we would need
to create multiple versions of those as well.  The very recent changes to coordinate
reference system models provide for concepts that will help with this such as
datum ensembles.  Having PROJ able to retrieve coordinate system definitions
from EPSG is a huge step towards making this practical in the open source stack.
Having software that can handle deformation is another huge step.

The way we have been managing NZGD2000 is to try to protect the NZ users
from dealing with this more than necessary.   This has been very pragmatic.
Since the tools most users have available are not "deformation aware", we
have been managing the coordinate system to allow users to ignore deformation
as far as possible.

As far as possible is to provide a coordinate reference system that hide the
deformation.  (I personally think NZGD2000 is better described as a geospatial reference
system than a geodetic datum).

To take the example of the coordinate NZGD2000 at 2010.  When we change
the deformation model we certainly are changing the relationship between
an NZGD2000 coordinate and the corresponding ITRF coordinate.
However we do preserve the relationship  between the NZGD2000 coordinate
and the physical location that it refers to.  So you don't need to change the
coordinates stored in your GIS systems etc - they are still correct for the features
they refer to.

As far as possible doesn't cover very heterogeneous deformation close to
an earthquake epicentre.  But for example in the case of the 2016 Kaikoura earthquake
the city of Wellington 150km from teh epicentre was shifted 20cm as a whole.
This is absorbed in the deformation model so that the NZGD2000 coordinates of marks do not change.

If you have a data set for Wellington observed in 2010 in terms of an ITRF based
coordinate system and convert to NZGD2000 then it will used the relationship
between ITRF and NZGD2000 that applied in 2010.  The NZGD2000 coordinate
that you get is the same as if you had observed the same physical feature in
terms of ITRF in 2019 it would have a different ITRF coordinate (both because
of the earthquake and the 5cm/year secular deformation).  However if you
and convert it to NZGD2000 using the post earthquake version of the deformation
model you will get the same NZGD2000 coordinate (give or take errors of observation
and errors in the deformation model).

Nearer the epicentre the situation is different - the deformation is too intense to hide
from users and so in that area the NZGD2000 coordinates of points do change.  LINZ
provided grids of the deformation to allow users to update their data sets to account
for the deformation (as well as updating the deformation model).  The good thing is
that in this region users expect the coordinates to change, and in fact are asking
for updates.

> had that discussion already, but I've forgotten the outcome. I'd be very
> interested in having a concrete example of a workflow of how users are
> supposed to use such deformation models, and which metadata they are
> supposed to store to be able to later reprocess their coordinates.

This is the critical question, and one that we don't have a good answer to!
At  the moment there is no workflow as virtually no software can use the
deformation model.  That is why NZGD2000 is implemented the way it is,
so that NZGD2000 data sets from different epochs can be compared and
used without having to account for deformation.

The nearest there is to a workflow is the process for users near an area
affected by deformation where they can use the grids we provide to update
their data (we provide NTv2 grids for the horizontal component).  We have
been calling this a "patch".

Your question about metadata is key.  The general answer is that (as for
all coordinates) they need to be aware of what coordinates are referenced
to when they are originally captured, and what transformations have been applied to
them since then.   In areas affected by a patch this is more critical.

Looking to the future this could change in many ways.  We could create
new versions of the datum for each update of the deformation model.  We
could imagine that coordinates are held with their effective epoch, and if
users want to compare data sets in terms of the physical bit of ground they
refer to they will transform the data sets to a common epoch on the fly.
Many possibilities!  We are not there yet, but whatever it is the thing we do
need is the ability to define and apply complex time dependent transformations.

Whew! That was a long answer.  I hope this helps a bit.  At the recent FOSS4G Oceania
conference I presented a talk on this.   https://www.youtube.com/watch?v=xdvI9Yrs4iA&t=141s
There is also some information on the LINZ website at https://www.linz.govt.nz/data/geodetic-system/datums-projections-and-heights/geodetic-datums/new-zealand-geodetic-datum-2000-nzgd2000/nzgd2000-deformation-model,
but most of this is detail rather than conceptual.

One thing we are very aware at LINZ is that we have not had sufficient resource/time
to publish some of the background to how we have implemented recent deformation
model updates, but conceptually the driver hasn't changed, which is to keep things
working as simply as we can for the users of the NZGD2000 coordinates, most of whom are
probably unaware of the deformation model or the need for one.

> Even

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From brian.shaw at noaa.gov  Thu Dec  5 16:00:26 2019
From: brian.shaw at noaa.gov (Brian Shaw)
Date: Thu, 5 Dec 2019 19:00:26 -0500
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
Message-ID: <90bf7ef9-0f97-0cfb-9db1-8eec33992ebf@noaa.gov>

Thanks Evan for the questions and Chris for the answers.

I know that at NGS we are working to solve how to implement many of the 
same challenges as LINZ except for all the US and its territories.  NGS 
plans to essentially have two models to handle deformation with an Inter 
Frame Velocity Model (IFVM) handling horizontal shifts and a Dynamic 
Geoid (DGEOID) to handle geopotential (vertical) shifts.  I know that 
New Zealand deals with earthquakes a bit more than we do but California, 
Alaska and the Pacific have many interesting events and challenges.  
Earlier this year the China Lake earthquake in California moved the 
local surface 12 feet or more in some places but thankfully it was not 
in a populated area with significant infrastructure.  Such major event 
shifts occasionally happen mostly in California, Alaska or the Pacific 
but we are working to create a system that allows us to handle such 
challenges.

Either way Im glad this is getting discussed since in the future system 
of the US time of observations or "epoch" will be essential and the 
coordinates you collect will be good for the day of observation.  The 
new paradigm acknowledges that the world is dynamic and constantly 
changing and so will coordinates over time. This will make the metadata 
of when a coordinate was determined *essential*, particularly if you are 
trying to forecast where a physical place is moving or where it has been.

While I havent got my colleagues to embrace the term since its a 
mouthful, I have been calling the future coordinates spatiotemporal 
coordinates.  I think we at NGS need to talk a lot more with LINZ about 
the grids, models and even database schemas.

Cheers
Brian

On 12/5/2019 5:50 PM, Chris Crook wrote:
> Hi Even
>
> Thanks for detailed reading and questions.
>
>> - regarding "The horizontal displacement is represented by metres east and
>> north. Typically base datum will be a geographic coordinate system using
>> latitude and longitude and it would be more efficient to define displacement
>> in terms of degrees.".
>> Is it the same convention as PROJ deformation method
>> (https://proj.org/operations/transformations/deformation.html) which uses
>> grids to store corrections in the ENU space as far as I understand ?
>>
>> (more a question for Kristian: I presume this ENU space is the same as the
>> UVW topocentric coordinate system described in 4.1.2
>> "Geocentric/topocentric conversions" of https://www.iogp.org/wp-
>> content/uploads/2019/09/373-07-02.pdf ?)
>>
>> For the easting, northing I presume yes. But I'm not sure if the convention
>> you mention for vertical correction "The vertical direction will be the local
>> vertical of the coordinate system." corresponds to the U of ENU. Or perhaps
>> this falls in your remark "For deformation there is no practical need to
>> distinguish gravitational and geometric vertical" ?
>> That said, PROJ deformation method is a bit different in that it is meant to
>> perate with input and output coordinates in geocentric space: it uses
>> internally a geocentric to geodetic conversion to be able to retrieve the
>> correction values from the grid(s), and then convert those corrections from
>> the ENU space to the geocentric space.
> I hadn't realised PROJ would be working in a geocentric space.
>
> What I was thinking when I suggested degrees would be more efficient was that
> had converted coordinates to geographic lon/lat/ellipsoidal height (as you need lat/lon
> to interpolate), then applying a correction to the lon/lat coordiantes in metres requires (a little)
> more calculation than applying corrections in degrees.   This could make more difference
> for an inverse calculation, as if you are iterating you could end up converting multiple times
> between geocentric and geographic (and geocentric to geographic is itself iterative).
>
> However if we are applying the correction to geocentric coordinates then
> this doesn't apply.
>
> For the inverse calculation it is probably sensible to work in the geographic coordinates
> if the solution is iterated, and then convert to geocentric once the inverse
> deformation has been calculated.
>
>> - regarding "Strictly the inverse calculation of the deformation model is an
>> iterative calculation, as the location of a point in the interpolation coordinate
>> system is not known until it is transformed back to that system."
>> I just want to mention that PROJ uses an iterative method for the inverse of
>> horizontal shift grids (NTv2 etc), so wouldn't be a problem to use that for
>> deformation models as well. Actually PROJ deformation method also uses
>> that iterative method.
> The NTv2 grids were originally designed for transformations between quite
> different datums that pre-dated satellite geodesy, and for which coordinate
> changes of a few hundred metres are quite likely, so in that case the iteration
> could be quite important.
>
> The coordinate changes due to deformation are likely to be much smaller (hopefully!).
> I think 10 metres would be an upper bound.  On the other hand the deformation will
> vary more rapidly with location in the event of earthquake related deformation.  I'll
> do some testing with the LINZ deformation model to see what difference it makes.
>
> But good to know that PROJ will do it the right way :-)
>
>> - regarding nesting of grids, is my understanding correct that if you have a
>> grid G, that has subgrids G_1 et G_2, and that a point falls in G and G_1, then
>> the corections of both G and G_1 should be applied ? Just wanted to
>> underline this, as it is different of the NTv2 model where you apply one or
>> the other (preferably G_1), but not both.
> No - I've tried to make this clearer in the document.  The proposal is to use the
> same approach as eg NTv2.   A single component (eg deformation due to an
> earthquake) may be defined by multiple grids.  At any location only one of those
> grids will apply.
>
> However the deformation model as a whole has multiple components (eg
> secular velocity, earthquake 1, earthquake 2, ...) and these are evaluated
> separately and summed together.
>
>> - it could help for a better understanding to have a small example of a CSV
>> file illustrating the different concepts you expose. Nested grids, different
>> time functions etc.
> Good idea - I'll build a CSV file matching the NZGD2000 deformation model.
>
>> - The approach master CSV file + separate grids in dedicated files has its
>> merits indeed. The actual grid format, GTiff, HDF5 or whatever raw binary
>> format that can support multiple samples per grid node will make little
>> difference because the specificities/complexities of the description of the
>> deformation model is in the CSV file itself.
>>
>> - So if a earthquake happens, basically you'll create a new grid in the area that
>> is affected, and create a new version of the CSV file where you add an extra
>> row that references it, right ? (I'm probably over-simplifying)
> That would be the most likely result.  For the last earthquake Kaikoura there
> would be two rows, as we treated near field and far field deformation
> differently (we still need to write this up).  Also we had two releases of the
> deformation model.  The second release was about a year later once we
> had better information about the total deformation (the update is an awkward
> and largely inseparable mix of better estimation of co-seismic deformation
> and additional post-seismic deformation).
>
>> - Any indication of the number of grids a deformation model might have ? For
>> the NZ case for example
> The NZGD2000 has a secular velocity grid plus grids for 12 events since 2000.0
> 124 grids at last  count I think, but that is individual grids, not the number of nested grid.
> Each event could be represented by 2 or 3 components with different time
> functions, so probably between 20 and 40 in total.
>
> However as our ability to measure and model deformation quickly is improving
> then the rate of  generating grids increases.
>
>> - Just a detail: regarding "velocity f(t) = (t – t0)/(365.2425 days)", if time and
>> volicities are expressed respectively in decimal year and
>> some_linear_unit/decimal_year, that should probably be just f(t) = t-t0
> Yes - I copied this directly from the LINZ deformation model documentation.
> Practically this is no different to expressing the time in decimal years.  I haven't
> defined anywhere the way in which date/time are defined, but we use
> a full date/time format rather than decimal years for defining the model as
> that is more intuitive for earthquakes.  And of course most data we are transforming
> has a specific date/time.   So time differences are determined in terms of "real"
> time units such as seconds, days, or whatever.
>
>> - I'm still having issues to grasp the practical implications of the "reverse
>> step". Is my understanding correct that a coordinate computed for example
>> for
>> NZGD2000 @ 2010.0 with a deformation model of 2019 might potentially
>> change with a deformation model of 2020 ? Should the deformation model
>> used be also captured in the metadata associated with a coordinate? If so, it
>> looks like a change in the deformation model would be better handled by
>> defining a revised version of the datum (but I know you don't want to publish
>> a new NZGD datum every 6 month). I'm super confused... I'm pretty sure we
> It absolutely is confusing!   Technically whenever we change the deformation
> model it is a new version of NZGD2000.  However the world, and particular the
> users of NZGD2000 coordinates,  was not ready for this, and possibly still isn't.
> Practically the time required to propagate new coordinate systems through the
> made creating multiple versions of the coordinate system impractical.   Also there
> are 30 projection coordinate systems associated NZGD2000, so we would need
> to create multiple versions of those as well.  The very recent changes to coordinate
> reference system models provide for concepts that will help with this such as
> datum ensembles.  Having PROJ able to retrieve coordinate system definitions
> from EPSG is a huge step towards making this practical in the open source stack.
> Having software that can handle deformation is another huge step.
>
> The way we have been managing NZGD2000 is to try to protect the NZ users
> from dealing with this more than necessary.   This has been very pragmatic.
> Since the tools most users have available are not "deformation aware", we
> have been managing the coordinate system to allow users to ignore deformation
> as far as possible.
>
> As far as possible is to provide a coordinate reference system that hide the
> deformation.  (I personally think NZGD2000 is better described as a geospatial reference
> system than a geodetic datum).
>
> To take the example of the coordinate NZGD2000 at 2010.  When we change
> the deformation model we certainly are changing the relationship between
> an NZGD2000 coordinate and the corresponding ITRF coordinate.
> However we do preserve the relationship  between the NZGD2000 coordinate
> and the physical location that it refers to.  So you don't need to change the
> coordinates stored in your GIS systems etc - they are still correct for the features
> they refer to.
>
> As far as possible doesn't cover very heterogeneous deformation close to
> an earthquake epicentre.  But for example in the case of the 2016 Kaikoura earthquake
> the city of Wellington 150km from teh epicentre was shifted 20cm as a whole.
> This is absorbed in the deformation model so that the NZGD2000 coordinates of marks do not change.
>
> If you have a data set for Wellington observed in 2010 in terms of an ITRF based
> coordinate system and convert to NZGD2000 then it will used the relationship
> between ITRF and NZGD2000 that applied in 2010.  The NZGD2000 coordinate
> that you get is the same as if you had observed the same physical feature in
> terms of ITRF in 2019 it would have a different ITRF coordinate (both because
> of the earthquake and the 5cm/year secular deformation).  However if you
> and convert it to NZGD2000 using the post earthquake version of the deformation
> model you will get the same NZGD2000 coordinate (give or take errors of observation
> and errors in the deformation model).
>
> Nearer the epicentre the situation is different - the deformation is too intense to hide
> from users and so in that area the NZGD2000 coordinates of points do change.  LINZ
> provided grids of the deformation to allow users to update their data sets to account
> for the deformation (as well as updating the deformation model).  The good thing is
> that in this region users expect the coordinates to change, and in fact are asking
> for updates.
>
>> had that discussion already, but I've forgotten the outcome. I'd be very
>> interested in having a concrete example of a workflow of how users are
>> supposed to use such deformation models, and which metadata they are
>> supposed to store to be able to later reprocess their coordinates.
> This is the critical question, and one that we don't have a good answer to!
> At  the moment there is no workflow as virtually no software can use the
> deformation model.  That is why NZGD2000 is implemented the way it is,
> so that NZGD2000 data sets from different epochs can be compared and
> used without having to account for deformation.
>
> The nearest there is to a workflow is the process for users near an area
> affected by deformation where they can use the grids we provide to update
> their data (we provide NTv2 grids for the horizontal component).  We have
> been calling this a "patch".
>
> Your question about metadata is key.  The general answer is that (as for
> all coordinates) they need to be aware of what coordinates are referenced
> to when they are originally captured, and what transformations have been applied to
> them since then.   In areas affected by a patch this is more critical.
>
> Looking to the future this could change in many ways.  We could create
> new versions of the datum for each update of the deformation model.  We
> could imagine that coordinates are held with their effective epoch, and if
> users want to compare data sets in terms of the physical bit of ground they
> refer to they will transform the data sets to a common epoch on the fly.
> Many possibilities!  We are not there yet, but whatever it is the thing we do
> need is the ability to define and apply complex time dependent transformations.
>
> Whew! That was a long answer.  I hope this helps a bit.  At the recent FOSS4G Oceania
> conference I presented a talk on this.   https://www.youtube.com/watch?v=xdvI9Yrs4iA&t=141s
> There is also some information on the LINZ website at https://www.linz.govt.nz/data/geodetic-system/datums-projections-and-heights/geodetic-datums/new-zealand-geodetic-datum-2000-nzgd2000/nzgd2000-deformation-model,
> but most of this is detail rather than conceptual.
>
> One thing we are very aware at LINZ is that we have not had sufficient resource/time
> to publish some of the background to how we have implemented recent deformation
> model updates, but conceptually the driver hasn't changed, which is to keep things
> working as simply as we can for the users of the NZGD2000 coordinates, most of whom are
> probably unaware of the deformation model or the need for one.
>
>> Even
> ________________________________
>
> This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj

-- 
*************************************
Brian Shaw
Geodesist
NOAA/NOS/National Geodetic Survey
Phone # 240-533-9522

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191205/6bced98c/attachment-0001.html>

From even.rouault at spatialys.com  Thu Dec  5 16:17:41 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Fri, 06 Dec 2019 01:17:41 +0100
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
Message-ID: <1658685.kqhlzQxNDr@even-i700>

Chris,

> I hadn't realised PROJ would be working in a geocentric space.

I just pointed at the particular working of the +proj=deformation 
transformation. Other transformations such as the +proj=hgridshift operate in 
the geographic coordinate space directly.

> What I was thinking when I suggested degrees would be more efficient was
> that had converted coordinates to geographic lon/lat/ellipsoidal height (as
> you need lat/lon to interpolate), then applying a correction to the lon/lat
> coordiantes in metres requires (a little) more calculation than applying
> corrections in degrees.   This could make more difference for an inverse
> calculation, as if you are iterating you could end up converting multiple
> times between geocentric and geographic (and geocentric to geographic is
> itself iterative).
> 
> However if we are applying the correction to geocentric coordinates then
> this doesn't apply.

That's probably mostly a matter of taste to decide if the input/output of an 
operator is geographic or geocentric coordinates. Actually, this is also 
related on how that operator relates to others you might want to chain with 
it. Looking at the examples of
https://proj.org/operations/transformations/deformation.html , the 
proj=deformation operation is surrounded by 2 proj=helmert, which do operate 
in the geocentric coordinate space, so that makes a lot of sense to have 
proj=deformation also operate on it (even if as an implementation detail it 
needs to do an internal geocentric->geographic conversion to retrieve grid 
values)
(I'm guessing in Kristian's mind. Hopefully correctly !)

> No - I've tried to make this clearer in the document.  The proposal is to
> use the same approach as eg NTv2.   A single component (eg deformation due
> to an earthquake) may be defined by multiple grids.  At any location only
> one of those grids will apply.
> 
> However the deformation model as a whole has multiple components (eg
> secular velocity, earthquake 1, earthquake 2, ...) and these are evaluated
> separately and summed together.

OK, so a line in your CSV file can be a file with potentially nested grids and 
when using that line, you use the "best" grid.
But you may find another grid in another line of the CSV file for the point 
considered, and you'll also apply it (depending on time criteria)
Am I correct ?

> > - I'm still having issues to grasp the practical implications of the
> > "reverse step". Is my understanding correct that a coordinate computed
> > for example for
> > NZGD2000 @ 2010.0 with a deformation model of 2019 might potentially
> > change with a deformation model of 2020 ? Should the deformation model
> > used be also captured in the metadata associated with a coordinate? If so,
> > it looks like a change in the deformation model would be better handled
> > by defining a revised version of the datum (but I know you don't want to
> > publish a new NZGD datum every 6 month). I'm super confused... I'm pretty
> > sure we

Looking quickly at
https://www.linz.govt.nz/data/geodetic-system/datums-projections-and-heights/
geodetic-datums/new-zealand-geodetic-datum-2000-nzgd2000/nzgd2000-deformation-
model
and
https://github.com/linz/nzgd2000-deformation-model ,
I now realize that the deformation model is for transformaing between a ITRF 
realization and NZGD2000. So the @ epoch qualifier applies to the ITRF 
realization, and in principle not to NZGD2000 which, for the sake of its use 
in ISO-19111 based modelizations (PROJ for example), is a non-dynamic datum.
Am I correct ?

> > had that discussion already, but I've forgotten the outcome. I'd be very
> > interested in having a concrete example of a workflow of how users are
> > supposed to use such deformation models, and which metadata they are
> > supposed to store to be able to later reprocess their coordinates.

Let me try to come with an hypothetical workflow of a user "playing" with 
coordinates.
Let's imagine they captured coordinates in an area that is going to be later 
affected by a earthquake. They get their input coordinates in
ITRF96 @ 2018.0 .

Because they need to mix this data in a dataset referenced to NZGD2000, they 
use the following transformation:
ITRF96 @ 2018.0  --> NZGD2000  using deformation model 20171201.

Now an earthquake happens at 2018.1 and the 20180701 deformation model is 
released to take it into account. They have 2 options to get the updated 
NZGD2000 coordinates:
- either they kept their initial coordinates in ITRF96 @ 2018.0 and use the 
new deformation model to transform them to the updated NZGD2000
- or they didn't... In that case they must remember they used model 20171201, 
to convert back to ITRF96 @ 2018.0 and apply model 20180701. So this means 
storing alongside to the NZGD2000 coordinates: the initial ITRF realization + 
the coordinate epoch + the deformation model used. 
(- or they might also just apply the patch correction if they are clever 
enough and use the appropriate tooling )

Right ?

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From hellik at web.de  Fri Dec  6 04:31:10 2019
From: hellik at web.de (Helmut Kudrnovsky)
Date: Fri, 6 Dec 2019 05:31:10 -0700 (MST)
Subject: [PROJ] Interested in helping for inclusion of grids for Spain,
 Austria, Slovakia, Brazil ?
In-Reply-To: <4172696.CJjv76bh8s@even-i700>
References: <4172696.CJjv76bh8s@even-i700>
Message-ID: <1575635470716-0.post@n6.nabble.com>

Hi Even,

>Austria
>-------
>
>The AT_GIS_GRID.gsb grid for MGI to ETRS89 is available at
>http://www.bev.gv.at/portal/page?_pageid=713,2157075&_dad=portal&_schema=PORTAL
:
>
>http://www.bev.gv.at/pls/portal/docs/PAGE/BEV_PORTAL_CONTENT_ALLGEMEIN/0200_PRODUKTE/UNENTGELTLICHE_PRODUKTE_DES_BEV/GIS-Grid.zip
>
>Copyright info I could find is
>"Hinsichtlich seiner Datenbanken verfügt das BEV über das ausschließliche
>Werknutzungsrecht gemäß §§ 40f bis 40h UrhG und das sui generis-Recht gemäß
§§ 76c bis 76e UrhG."
>
>which by itself is not super promising. 

have a look now at

https://www.data.gv.at/katalog/dataset/3b806391-71c3-4f92-a5c0-3696b521b6a9


#############
Das GIS-Grid dient der Koordinatentransformation zwischen dem nationalen
österreichischen System der Landesvermessung MGI (auch als Gebrauchssystem
bezeichnet) und dem europäischen globalen Referenzsystem ETRS89. Es basiert
auf dem in Kanada entwickelten und definierten gitternetzbasierten
Transformationsverfahren NTv2 (National Transformation version 2), dessen
Format und Methode mittlerweile zu einem Standard im GIS-Bereich geworden
ist.

Für Österreich wurde es zwischen den geographischen Breiten 46°21' und
49°03' und den geographischen Längen 9°30' und 17°09'45" in einem
regelmäßigen Raster von 30"x45" (entspricht ca. 1km x 1km) entwickelt. Jedes
Rasterelement beinhaltet Parameterwerte in den Komponenten Ost-West und
Nord-Süd. Die Transformation der Höhe ist im GIS-Grid nicht definiert.
#############

Lizenz
Rechtliche Nutzungsinformationen für die Verwendung des Datensatzes,
Dienstes oder Dokuments.
Creative Commons Namensnennung 4.0 International 

does this kind of licence work?




-----
best regards
Helmut
--
Sent from: http://osgeo-org.1560.x6.nabble.com/PROJ-4-f3840930.html

From even.rouault at spatialys.com  Fri Dec  6 04:37:23 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Fri, 06 Dec 2019 13:37:23 +0100
Subject: [PROJ] Interested in helping for inclusion of grids for Spain,
	Austria, Slovakia, Brazil ?
In-Reply-To: <1575635470716-0.post@n6.nabble.com>
References: <4172696.CJjv76bh8s@even-i700> <1575635470716-0.post@n6.nabble.com>
Message-ID: <10041286.kbpfkqvVbC@even-i700>

Helmut,

> >Austria
> >-------
> >
> >The AT_GIS_GRID.gsb grid for MGI to ETRS89 is available at
> >http://www.bev.gv.at/portal/page?_pageid=713,2157075&_dad=portal&_schema=PO
> >RTAL
[...]
> Rechtliche Nutzungsinformationen für die Verwendung des Datensatzes,
> Dienstes oder Dokuments.
> Creative Commons Namensnennung 4.0 International
> 
> does this kind of licence work?

Yes. Someone else already pointed that to me and AT_GIS_GRID.gsb has now been 
integrated in proj-datumgrid.

Even


-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From kreve at sdfe.dk  Fri Dec  6 05:42:08 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Fri, 6 Dec 2019 13:42:08 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <1658685.kqhlzQxNDr@even-i700>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700>
Message-ID: <c169afd342b84f74b87094109c4c3631@sdfe.dk>

Chris et al,

I am late to the party so let me just try to comment on a few things from the discussion below as well as provide a few use cases where I would benefit from this new grid format. Generally I also think this is a very well thought out and thorough description of a new format. It covers most bases for my needs and I think that the "multi file" solution aligns quite closely with current PROJ practice so it shouldn't be too complicated to make things work. Sorry for the long text - I got carried away, as often happens when the subject is interesting!


COORDINATE SPACE(S)

I originally wrote the deformation operation with the sole purpose of providing a practical implementation of the transformations described in the paper by Häkli et al [0]. This obviously influenced the implementation and there is the possibility that it isn't generic enough for all purposes, although I haven't yet come across a situation that wouldn't fit regarding the use 3D deformation models in coordinate transformations.

The deformation operation in PROJ works in a mix of geocentric and geographic coordinates [1]. Actual adjustments are done in geocentric/cartesian space. This is analogous to e.g. a Helmert operation which also works in geocentric space. And as Even points out the deformation operation is often use in a pipeline in conjunction with one or more Helmert operations. I generally prefer to work in geocentric space when doing this sort of work since it is the "native" coordinate space of the frame and because it is nice to work within a linear coordinate systems with axes that intersects at right angles. Unfortunately it is not really possible to create a grid of surface displacements or velocities in geocentric space without creating a massive sparse 3D matrix which is not practical in any way. This is the reason why the deformation operation requires grids in topocentric/ENU space and internally converts between the two coordinate systems. It does come with a bit of overhead, especially in the inverse mode, but it isn't too bad compared to let's say a 14-parameter Helmert operation.

Doing gridshift adjustments in degrees generally works quite well and it what has been done for years and years in PROJ. The main disadvantage, I think, is that it is difficult for the human eye to interpret values in the grid. I tend to find it quite useful to plot the grids I use in QGIS and having the values expressed in meters or millimeters gives me an immediate advantage. There's nothing in the way to support both types of gridshifts. I do think though that expressing velocities in terms of degrees is a bad idea (I don't think that was the intention, just putting it out there). 

Regarding the iterative inverse deformation operation, it works but it isn't particularly good to be honest. I reworked the existing iterative algorithm used for horizontal grid shifts into a 3D version and I never quite got it right. It doesn't perform well in a round-trip test and after about ten trips back and forth it has introduced too much noise due to numerical inaccuracy in the algorithm. Improving this could be a fun challenge for  some one more mathematically inclined than me.

I agree that it doesn't make much of a difference whether adjustments in the up components are done in relation to a gravimetric or geometric reference.


GRID DELIVERY METHOD (SINGLE VS. MULTI FILE)

I agree with the proposal that the multi file solution is preferable. Especially if zipped up a in aggregate package. This was also touched upon in the original GitHub discussion 18 months ago. I think JSON would be preferable to a CSV based format since it offers a bit more structure and you would be able to define a proper JSON schema that can be used to validate the format of the file. PROJ has the ability to parse JSON already. 

A package of a number of grid files and a metadata file describing the use of the grids should be fairly simple to translate to a PROJ pipeline, provided that the usage of the grids align with operations available in PROJ. It would of course be much simpler if everything was just expressed as a PROJ pipeline but I guess we can't assume that everyone will be using PROJ :-)


RELEVANT USE CASE: GREENLAND

Greenland is subject to significant elastic deformation due to rapid mass-loss from the ice sheet. The deformation varies a lot from year to year which creates a need for a rather complicated deformation model. At the moment I have solved the problem by creating a set deformation models that cover a specific year each. That is, for every year since 1996 I have a 3D deformation model that only applies for that particular year. The plan is to create a pipeline of deformation operations that applies as many adjustments from the deformation models as needed. E.g. if a coordinate from 2010 is transformed it would have to pass through 14 deformation operations (2010-1996) to account for all the deformation. I think for the final transformations I will narrow the number of gridded models down so that they each cover a three year period. The point here is that it would be nice to be able to support this type of transformation in the new deformation model format. If I read the proposal correctly this would be possible but I may have missed some details. PROJ is actually not able to do this as of now but I will extend the deformation operation to allow this in the future (functionality similar to the +t_epoch/+t_final setup in hgridshift is needed). 


RELEVANT USE CASE: ICELAND

Similarly to New Zealand, Iceland exists in very a geodynamically complicated setting. Transformations in Iceland should ideally take care of secular (GIA, divergent plate boundary) and episodic deformation (earth quakes, volcanism) as well as the regular Helmert transformations involving ITRFxxxx. In PROJ this can be handled as a pipeline including the helmert, hgridshift, vgridshift and deformation operations. I gave a talk on the subject last year which covers this use case and some thoughts about a practical implementation [2]. Recently that work described in that presentation has culminated in a pull request for the proj-datumgrid repository [3].


/Kristian


[0] Häkli, P., Lidberg, M., Jivall, L., Nørbech, T., Tangen, O., Weber, M., Pihlak, P., Aleksejenko, I., and Paršeliunas, E. The NKG2008 GPS campaign – final transformation results and a new common Nordic reference frame. Journal of Geodetic Science, 6(1):1–33, 2016. https://doi.org/10.1515/jogs-2016-0001 

[1] Yes, I use formulas equivalent to what is described in section 4.1.2 "Geocentric/topocentric 
conversions" of https://www.iogp.org/wp-content/uploads/2019/09/373-07-02.pdf 

[2] https://www.fig.net/fig2018/rfip/6_RFIPIstanbulKristianEvers.pdf 

[3] https://github.com/OSGeo/proj-datumgrid/pull/67 

From ccrook at linz.govt.nz  Fri Dec  6 10:36:10 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Fri, 6 Dec 2019 18:36:10 +0000
Subject: [PROJ] FW:  Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF9323148D1FB@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148D1FB@prdassexch01.ad.linz.govt.nz>
Message-ID: <A87E66F06E86F14B857F2EB047CDF9323148EA25@prdassexch01.ad.linz.govt.nz>

Even

> OK, so a line in your CSV file can be a file with potentially nested
> grids and when using that line, you use the "best" grid.
> But you may find another grid in another line of the CSV file for the
> point considered, and you'll also apply it (depending on time criteria) Am I correct ?

Yes that is it.  With the multiple file implementation each component line in the master CSV file (renamed from metadata file in the document an hour or so ago) would correspond to a GeoTIFF file.

When evaluating the deformation model at a given location and time the algorithm scans CSV file to find components for which the extents included the evaluation point and the time function evaluated to a non-zero value.

For each of these it searches the corresponding GeoTIFF to find the preferred grid for the evaluation point (as I understand the RFC4 that would be the last grid in the IFD containing the point) and uses that to interpolate the deformation parameters de,dn,du at the location.

The interpolated values would be multiplied by the time function and added together to obtain the total de,dn,du displacment.

> > > - I'm still having issues to grasp the practical implications of
> > > the "reverse step". Is my understanding correct that a coordinate
> > > computed for example for
> > > NZGD2000 @ 2010.0 with a deformation model of 2019 might
> > > potentially change with a deformation model of 2020 ? Should the
> > > deformation model used be also captured in the metadata associated
> > > with a coordinate? If so, it looks like a change in the
> > > deformation model would be better handled by defining a revised
> > > version of the datum (but I know you don't want to publish a new
> > > NZGD datum every 6 month). I'm super confused... I'm pretty sure
> > > we
>
> Looking quickly at
> https://www.linz.govt.nz/data/geodetic-system/datums-projections-and-
> heights/
> geodetic-datums/new-zealand-geodetic-datum-2000-nzgd2000/nzgd2000-
> deformation-
> model
> and
> https://github.com/linz/nzgd2000-deformation-model , I now realize
> that the deformation model is for transformaing between a ITRF
> realization and NZGD2000. So the @ epoch qualifier applies to the ITRF
> realization, and in principle not to NZGD2000 which, for the sake of
> its use in ISO-19111 based modelizations (PROJ for example), is a non-dynamic datum.
> Am I correct ?

Pretty much.  The nomenclature of "dynamic" and "static" is to my mind
very unhelpful.   NZGD2000 is based on a dynamic datum - it is realised by
GNSS based ITRF coordinates at CORS reference stations.  But it is modified
by the deformation model to emulate a static datum.   Much of the original
documentation refers to it as a semi-dynamic datum, but I'm not sure that helps!

I'm not sure about using @epoch.  I did start using that myself in the LINZ coordinate conversion software a long time ago but I found it created more
confusion than clarity (for me!)   It associates the epoch with the datum rather than the
coordinate/data set, which I find misleading.

The ITRF coordinate in 2010 refers  to a location measured at that time, as does the NZGD2000 coordinate in 2010.  The same coordinates (ie numerically) in 2019 refer to different locations.  The NZGD2000 tends to track deformation so will more or less refer to the same physical point on the ground as it did in 2010.  The ITRF coordinate will refer to a different physical point on the ground.

So as far as a "fixed feature" (something intuitively fixed such as a building) the
NZGD2000 coordinate does not need an epoch.  Nonetheless I would still want to keep it as important metadata.

The ITRF coordinate still correctly defines where the object was in 2010.  If you want to know where it is in 2019 then you can use the deformation model as a point motion model to predict where it should be.

This applies in an area where we haven't applied an earthquake "patch" (or more strictly a "reverse path").

Where we have applied a patch the situation is different.
In those areas we essentially have given up tracking the ground movement with
NZGD2000 as the distortion is untenable for something purporting to be a geodetic datum.  By that I mean that most users do not/cannot use the deformation model, but still expect to be able to use the NZGD2000 coordinates to do calculations as if they were geodetic coordinates, and
calculate distances etc from them.   If they want to do very accurate calculations
they will need to transform back to ITRF.  But at a lower accuracy most users
do not do this.   To manage the expectation of using the NZGD2000 coordinates
as if they are geodetic coordinates we limit the distortion in them.

Where the ground deformation creates a distortion of more than say
10 part per million we apply a "reverse patch", which in the affected area amounts
to a new local datum and updates the NZGD2000 coordinates.   The deformation model
is still tracking the movement of the featuers, but the NZGD2000 coordinate does not.

In this case the new deformation model includes the earthquake deformation, but it applies to coordinates before the earthquake.  So in an area affected by a patch if you take the ITRF 2010 coordinate and apply the patched deformation model to it (at epoch 2010) to calculate the NZGD2000 coordinate you will get a different value to what you would have obtained using the previous version of the deformation model.  It will still refer to the same physical feature, but the NZGD2000 coordinates will be numerically different.

Still closer to the epicentre there is a region where the deformation is too complex to model in the deformation model at all (at least in any useful way), so while the deformation model will still calculate an NGD2000 coordinate from the ITRF epoch 2010
coordinate, it will no longer refer to the same physical feature.

So essentially there are three situations for users when we update the deformation model with a new reverse patch, depending  on where they are located.

1) Far from the earthquake where the deformation model has absorbed the movement and NZGD2000 coordinates are not updated.
* NZGD2000 coordinates from pre-earthquake data still refer to the same physical location
* ITRF coordinates at any epoch can still be converted using the updated deformation model to get NZGD2000 coordinates (which will be no different the those calculated from the previous model)

2) Close to the earthquake where we have applied a reverse patch, updating both the coordinates and the deformation model.
* NZGD2000 coordinates from before the earthquake need to be updated using a reverse patch model to refer to the same physical feature (ie a new datum locally)
* ITRF coordinates at any epoch can still be converted to the new NZGD2000 coordinates using the deformation model.  This will give different NZGD2000 coordinates to those calculated with the old deformation model).

3) Very close to the earthquake where the deformation is too complex to include in the deformation model.
* NZGD2000 coordinates no longer refer to the same feature even after applying the patch
* ITRF coordinates can be converted to NZGD2000 as in case 2 but will also not refer to the same physical feature.

Depending on the nature of the event then not all the regions will apply.  For example an offshore earthquake may be completely handled by the deformation model, so only 1 will apply.  A deep earthquake may not cause significant surface distortion, so only 1 and 2 may apply.

Apart from earthquake there is also the 5cm per year secular deformation which in NZ is continuously distorting the country.  This is handle is the same as case 1 above - the NZGD2000 coordinates of features do not change.

Note that this description really only applies to horizontal coordinates. Vertical deformation is handled differently.  For horizontal deformation it is useful to "hide" the deformation from the users.  The usage of heights is very different - the relationship to gravity, direction of flow etc means that we want heights to reflect their true current value as far as possible. In the terminology above height movements are always treated as case 2 - reverse patch where coordinates are updated with known vertical movements.

Also our vertical deformation model is much less complete.  We have models for some of the earthquakes but no secular models.  Plus heights are much more subject to smaller scale regional and temporal changes, eg due to mining, groundwater changes, etc.

> > > had that discussion already, but I've forgotten the outcome. I'd
> > > be very interested in having a concrete example of a workflow of
> > > how users are supposed to use such deformation models, and which
> > > metadata they are supposed to store to be able to later reprocess
> > > their
> coordinates.
>
> Let me try to come with an hypothetical workflow of a user "playing"
> with coordinates.
> Let's imagine they captured coordinates in an area that is going to be
> later affected by a earthquake. They get their input coordinates in
> ITRF96 @ 2018.0 .
>
> Because they need to mix this data in a dataset referenced to
> NZGD2000, they use the following transformation:
> ITRF96 @ 2018.0  --> NZGD2000  using deformation model 20171201.
>
> Now an earthquake happens at 2018.1 and the 20180701 deformation model
> is released to take it into account. They have 2 options to get the
> updated
> NZGD2000 coordinates:
> - either they kept their initial coordinates in ITRF96 @ 2018.0 and
> use the new deformation model to transform them to the updated
> NZGD2000
> - or they didn't... In that case they must remember they used model
> 20171201, to convert back to ITRF96 @ 2018.0 and apply model 20180701.
> So this means storing alongside to the NZGD2000 coordinates: the
> initial ITRF realization + the coordinate epoch + the deformation model used.
> (- or they might also just apply the patch correction if they are
> clever enough and use the appropriate tooling )

Hopefully above answers this a bit.

Cheers
Chris

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From ccrook at linz.govt.nz  Fri Dec  6 11:20:02 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Fri, 6 Dec 2019 19:20:02 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <c169afd342b84f74b87094109c4c3631@sdfe.dk>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700> <c169afd342b84f74b87094109c4c3631@sdfe.dk>
Message-ID: <A87E66F06E86F14B857F2EB047CDF9323148EAE1@prdassexch01.ad.linz.govt.nz>

Kristian

> I am late to the party so let me just try to comment on a few things

I think the party has hardly started :-)

> COORDINATE SPACE(S)
>
> I originally wrote the deformation operation with the sole purpose of
> providing a practical implementation of the transformations described in the
> paper by Häkli et al [0]. This obviously influenced the implementation and
> there is the possibility that it isn't generic enough for all purposes, although I
> haven't yet come across a situation that wouldn't fit regarding the use 3D
> deformation models in coordinate transformations.
>
> The deformation operation in PROJ works in a mix of geocentric and
> geographic coordinates [1]. Actual adjustments are done in
> geocentric/cartesian space. This is analogous to e.g. a Helmert operation
> which also works in geocentric space. And as Even points out the
> deformation operation is often use in a pipeline in conjunction with one or
> more Helmert operations. I generally prefer to work in geocentric space
> when doing this sort of work since it is the "native" coordinate space of the
> frame and because it is nice to work within a linear coordinate systems with
> axes that intersects at right angles. Unfortunately it is not really possible to
> create a grid of surface displacements or velocities in geocentric space
> without creating a massive sparse 3D matrix which is not practical in any way.
> This is the reason why the deformation operation requires grids in
> topocentric/ENU space and internally converts between the two coordinate
> systems. It does come with a bit of overhead, especially in the inverse mode,
> but it isn't too bad compared to let's say a 14-parameter Helmert operation.

In NZ case we have generally different implementations of horizontal and vertical
deformation, so the ENU system is much more practical.  Also for the most part the
measurement systems and modelling approaches (not to mention tectonic behaviour)
can have quite different behaviours for horizontal and vertical (or indeed only provide
horizontal or vertical deformation).   So at the moment for LINZ the more pragmatic
approach is to work in ENU space for a while longer.  Or more briefly - gravity makes
a difference.

> Doing gridshift adjustments in degrees generally works quite well and it what
> has been done for years and years in PROJ. The main disadvantage, I think, is
> that it is difficult for the human eye to interpret values in the grid. I tend to
> find it quite useful to plot the grids I use in QGIS and having the values
> expressed in meters or millimeters gives me an immediate advantage.
> There's nothing in the way to support both types of gridshifts. I do think
> though that expressing velocities in terms of degrees is a bad idea (I don't
> think that was the intention, just putting it out there).

Thanks.  I also favour using metres rather than degrees.  Having human readable/easily
plottable grids is another plus.  Plus as in the proposal uncertainties in degrees are nonsensical
(especially as I am proposing a single circular horizontal uncertainty) and it feels very unclean
to use different units for displacement and its uncertainty.

> Regarding the iterative inverse deformation operation, it works but it isn't
> particularly good to be honest. I reworked the existing iterative algorithm
> used for horizontal grid shifts into a 3D version and I never quite got it right. It
> doesn't perform well in a round-trip test and after about ten trips back and
> forth it has introduced too much noise due to numerical inaccuracy in the
> algorithm. Improving this could be a fun challenge for  some one more
> mathematically inclined than me.

I wonder if it is easiest to do the inversion in the lat/lon space (even a simple
approximation of metre to degrees should get  a fairly accurate result).  While an
imperfect inverse is not ideal, it is nothing compared to the imperfections in the
deformation model in the first place.  Ultimately a trade off between perfect and
efficient.  Maybe this could be a user definable option somehow if it is a concern?

> I agree that it doesn't make much of a difference whether adjustments in
> the up components are done in relation to a gravimetric or geometric
> reference.

Nice to have a second opinion on this assertion :-)

> GRID DELIVERY METHOD (SINGLE VS. MULTI FILE)
>
> I agree with the proposal that the multi file solution is preferable. Especially if
> zipped up a in aggregate package. This was also touched upon in the original
> GitHub discussion 18 months ago. I think JSON would be preferable to a CSV
> based format since it offers a bit more structure and you would be able to
> define a proper JSON schema that can be used to validate the format of the
> file. PROJ has the ability to parse JSON already.

I'm sold!  I'll amend the document to propose JSON next week.   It is also much more
suitable for web services and so on.

I realised another benefit of the multiple file format is that it provides the possibility
of optimising conversions between different versions of a datum for which most of the
grids are the same.   I'll add that to the document also.

> A package of a number of grid files and a metadata file describing the use of
> the grids should be fairly simple to translate to a PROJ pipeline, provided that
> the usage of the grids align with operations available in PROJ. It would of
> course be much simpler if everything was just expressed as a PROJ pipeline
> but I guess we can't assume that everyone will be using PROJ :-)

I haven't raised the implementation approach in PROJ in this proposal (yet!).
I'm wondering if it would be better implemented as a +proj=(deformation is taken,
something like that) method, rather than breaking up into multiple steps in
in the pipeline as I think you are suggesting.

Reasons are:
1) Inversion should be of the deformation as a whole, not individual steps
2) The pipeline would be different for different epochs.   Generally I imagine that data
will be of a consistent epoch but I don't think that would necessarily be true
3) I suspect it would be slower to run

> RELEVANT USE CASE: GREENLAND
>   ...

Thanks for these  - very interesting.  Is it Ok if I include these in the document
(attributed to you).  I'm thinking I'll add an appendix summarising the discussion
with Even around the NZ usage, and it would be great to add these too.

Regards
Chris


________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From even.rouault at spatialys.com  Sat Dec  7 11:25:34 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sat, 07 Dec 2019 20:25:34 +0100
Subject: [PROJ] Units of NTv2 accuracy/error channels ?
Message-ID: <19221812.iKQVBK8pbO@even-i700>

Hi,

Question for advanced NTv2 users and agencies producing NTv2 grids.

I have a hard time figuring out what is the unit of the Latitude error/accuracy
(3rd channel) and Longitude error/accuracy (4th channel) of NTv2 products.
In a number of products, it is set to 0 or -1 to indicate no errors. So far, so good.
But what about products which have meaningful values ?

The original NTv2 specification at
https://web.archive.org/web/20091227232322if_/http://www.mgs.gov.on.ca:80/stdprodconsume/groups/content/@mgs/@iandit/documents/resourcelist/stel02_047447.pdf
is not super explicit about the expected units for the accuracy channels.
One could reasonably expect them to be in the same GS_UNITS as the latitude
and longitude shift channels (arc-seconds in all products I'm aware of)

The technical documentation (in French) of the ntf_r93.gsb product in
https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf
clearly mentions the use of arc-seconds unit for the error channels.

But... https://github.com/Esri/ntv2-file-routines/blob/master/README.md mentions 
"""
Latitude  accuracy (in meters  ) - optional
Longitude accuracy (in meters  ) - optional
"""

I'm wondering if this statement is not derived by looking at the original NTV2 canadian
grid, ntv2_0.gsb. Using the ntv2_100325 executable from
https://webapp.geod.nrcan.gc.ca/geod/process/download-helper.php?file_id=ntv2 confirms
that the shift values are in arc seconds, and the errors in metres.
Their online calculator at https://webapp.geod.nrcan.gc.ca/geod/tools-outils/ntv2.php
also confirms it.
The maximum longitude/latitude error/accuracy value as shown in [0] are also greater
than the absolute maximum shift value (also seen on a few random points, but not in
the general case), so while this is not a solid proof, this is compatible of
error values are in metre, rathr than arc-seconds.

Now, looking at the statistics and samplings for IGN France (ntf_r93.gsb) [1],
Germany (Baden-Württemberg) LGL (BWTA2017.gsb) [2] and LINZ (nzgd2kgrid0005.gsb) [3],
I see that the values of LINZ are of a magnitude similar to IGN, so by induction they are
likely in arc-seconds too. For LGL, the error/inaccuracy is much lower, or the order of 1e-4, so
there's also a high chance this is arc-seconds (1e-4 arc second ~= 3.2 mm), rather
than metre.

Does that ring a bell to anyone ? This is not really important for PROJ, which
doesn't make use of that information, but if converting those products to GeoTIFF
and adding units in metadata, it would be better to properly register the right unit...

Even

[0] ntv2_0.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=2.7053599357605
    STATISTICS_MEAN=0.22938163123877
    STATISTICS_MINIMUM=-1.2412799596786
    STATISTICS_STDDEV=0.36487517625985
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=0.52859002351761
    STATISTICS_MEAN=-2.4296108875319
    STATISTICS_MINIMUM=-6.5360999107361
    STATISTICS_STDDEV=1.5646204102361
Band 3 
  Description = Latitude Error
    STATISTICS_MAXIMUM=13.354999542236
    STATISTICS_MEAN=0.27474378975396
    STATISTICS_MINIMUM=0
    STATISTICS_STDDEV=0.48454424815682
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=8.7159996032715
    STATISTICS_MEAN=0.29625671244826
    STATISTICS_MINIMUM=0
    STATISTICS_STDDEV=0.48875059551347

Sampling at a "random" point
  Location: (50P,50L)
  Band 1:
    Value: 0.407240003347397
  Band 2:
    Value: -0.417419999837875
  Band 3:
    Value: 2.06200003623962
  Band 4:
    Value: 2.26300001144409


[1] ntf_r93.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=0.41066199541092
    STATISTICS_MEAN=-0.14290635876691
    STATISTICS_MINIMUM=-0.43130299448967
    STATISTICS_STDDEV=0.17467622900461
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=3.9832758903503
    STATISTICS_MEAN=2.5114900115167
    STATISTICS_MINIMUM=1.2807140350342
    STATISTICS_STDDEV=0.64540102922043
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=0.064833000302315
    STATISTICS_MEAN=0.035132184284462
    STATISTICS_MINIMUM=0.0016179999802262
    STATISTICS_STDDEV=0.03134354382028
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=0.10483700037003
    STATISTICS_MEAN=0.05122512365606
    STATISTICS_MINIMUM=0.0021490000654012
    STATISTICS_STDDEV=0.045974087201794

Sampling at a "random" point
  Location: (50P,50L)
  Band 1:
    Value: -0.257952004671097
  Band 2:
    Value: 2.9302670955658
  Band 3:
    Value: 0.00161899998784065
  Band 4:
    Value: 0.00236700009554625


[2] BWTA2017.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=-3.058042049408
    STATISTICS_MEAN=-3.5957838572742
    STATISTICS_MINIMUM=-4.0953946113586
    STATISTICS_STDDEV=0.27330481075759
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=4.5378975868225
    STATISTICS_MEAN=3.6829893469828
    STATISTICS_MINIMUM=2.8559820652008
    STATISTICS_STDDEV=0.44504277350973
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=0.00013734232925344
    STATISTICS_MEAN=6.3284008649676e-05
    STATISTICS_MINIMUM=1.6185947970371e-05
    STATISTICS_STDDEV=1.9119506806922e-05
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=0.00020735718135256
    STATISTICS_MEAN=9.5545150040269e-05
    STATISTICS_MINIMUM=2.4437278625555e-05
    STATISTICS_STDDEV=2.88663154724e-05

Sampling at a "random" point
  Location: (1000P,1000L)
  Band 1:
    Value: -3.91857481002808
  Band 2:
    Value: 3.3124144077301
  Band 3:
    Value: 6.86711646267213e-05
  Band 4:
    Value: 0.000103678590676282


[3] nzgd2kgrid0005.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=7.0297069549561
    STATISTICS_MEAN=6.2095865303887
    STATISTICS_MINIMUM=5.2811942100525
    STATISTICS_STDDEV=0.31210796006199
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=0.35285499691963
    STATISTICS_MEAN=-0.57949275997587
    STATISTICS_MINIMUM=-1.6765320301056
    STATISTICS_STDDEV=0.29261751972554
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=0.11701499670744
    STATISTICS_MEAN=0.026246032400208
    STATISTICS_MINIMUM=0.00059700000565499
    STATISTICS_STDDEV=0.02753783661212
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=0.16906200349331
    STATISTICS_MEAN=0.035039901355755
    STATISTICS_MINIMUM=0.00079299998469651
    STATISTICS_STDDEV=0.037121379791356

Sampling at a "random" point
  Location: (50P,50L)
  Band 1:
    Value: 6.31486082077026
  Band 2:
    Value: -0.991177976131439
  Band 3:
    Value: 0.0246409997344017
  Band 4:
    Value: 0.0317060016095638

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Jochem.Lesparre at kadaster.nl  Sun Dec  8 02:34:26 2019
From: Jochem.Lesparre at kadaster.nl (Lesparre, Jochem)
Date: Sun, 8 Dec 2019 10:34:26 +0000
Subject: [PROJ] Units of NTv2 accuracy/error channels ?
In-Reply-To: <19221812.iKQVBK8pbO@even-i700>
References: <19221812.iKQVBK8pbO@even-i700>
Message-ID: <VI1P192MB0016A7EE85E3A9C9EE877148EF590@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>

Hi Even,

The NTv2 grids for the Netherlands use the metre as unit for the accuracy. All accuracy values are set to 0.001, thus 1 mm. As reference for creating the files, we used: "NTv2 National Transformation Version 2; Developer's Guide" by D.R. Junkins and S.A. Farley (Geodetic Survey Division, Geomatics Canada), September 1995. To my surprise, this document doesn't specify the units. Since all other values in NTv2 are in arcseconds (even the grid bounds), it is not very logical to use metres for the accuracy. So we must have used some other source to decide to use the metre for the accuracy, but I did not yet find this source. 

Regards, Jochem


J. Lesparre MSc 
Cadastre, Land Registry and Mapping Agency (Kadaster)
Hofstraat 110
7311KZ Apeldoorn
The Netherlands


-----Original Message-----
From: PROJ <proj-bounces at lists.osgeo.org> On Behalf Of Even Rouault
Sent: zaterdag 7 december 2019 20:26
To: proj at lists.osgeo.org
Subject: [PROJ] Units of NTv2 accuracy/error channels ?

Hi,

Question for advanced NTv2 users and agencies producing NTv2 grids.

I have a hard time figuring out what is the unit of the Latitude error/accuracy (3rd channel) and Longitude error/accuracy (4th channel) of NTv2 products.
In a number of products, it is set to 0 or -1 to indicate no errors. So far, so good.
But what about products which have meaningful values ?

The original NTv2 specification at
https://web.archive.org/web/20091227232322if_/http://www.mgs.gov.on.ca:80/stdprodconsume/groups/content/@mgs/@iandit/documents/resourcelist/stel02_047447.pdf
is not super explicit about the expected units for the accuracy channels.
One could reasonably expect them to be in the same GS_UNITS as the latitude and longitude shift channels (arc-seconds in all products I'm aware of)

The technical documentation (in French) of the ntf_r93.gsb product in https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf
clearly mentions the use of arc-seconds unit for the error channels.

But... https://github.com/Esri/ntv2-file-routines/blob/master/README.md mentions """
Latitude  accuracy (in meters  ) - optional Longitude accuracy (in meters  ) - optional """

I'm wondering if this statement is not derived by looking at the original NTV2 canadian grid, ntv2_0.gsb. Using the ntv2_100325 executable from
https://webapp.geod.nrcan.gc.ca/geod/process/download-helper.php?file_id=ntv2 confirms that the shift values are in arc seconds, and the errors in metres.
Their online calculator at https://webapp.geod.nrcan.gc.ca/geod/tools-outils/ntv2.php
also confirms it.
The maximum longitude/latitude error/accuracy value as shown in [0] are also greater than the absolute maximum shift value (also seen on a few random points, but not in the general case), so while this is not a solid proof, this is compatible of error values are in metre, rathr than arc-seconds.

Now, looking at the statistics and samplings for IGN France (ntf_r93.gsb) [1], Germany (Baden-Württemberg) LGL (BWTA2017.gsb) [2] and LINZ (nzgd2kgrid0005.gsb) [3], I see that the values of LINZ are of a magnitude similar to IGN, so by induction they are likely in arc-seconds too. For LGL, the error/inaccuracy is much lower, or the order of 1e-4, so there's also a high chance this is arc-seconds (1e-4 arc second ~= 3.2 mm), rather than metre.

Does that ring a bell to anyone ? This is not really important for PROJ, which doesn't make use of that information, but if converting those products to GeoTIFF and adding units in metadata, it would be better to properly register the right unit...

Even

[0] ntv2_0.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=2.7053599357605
    STATISTICS_MEAN=0.22938163123877
    STATISTICS_MINIMUM=-1.2412799596786
    STATISTICS_STDDEV=0.36487517625985
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=0.52859002351761
    STATISTICS_MEAN=-2.4296108875319
    STATISTICS_MINIMUM=-6.5360999107361
    STATISTICS_STDDEV=1.5646204102361
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=13.354999542236
    STATISTICS_MEAN=0.27474378975396
    STATISTICS_MINIMUM=0
    STATISTICS_STDDEV=0.48454424815682
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=8.7159996032715
    STATISTICS_MEAN=0.29625671244826
    STATISTICS_MINIMUM=0
    STATISTICS_STDDEV=0.48875059551347

Sampling at a "random" point
  Location: (50P,50L)
  Band 1:
    Value: 0.407240003347397
  Band 2:
    Value: -0.417419999837875
  Band 3:
    Value: 2.06200003623962
  Band 4:
    Value: 2.26300001144409


[1] ntf_r93.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=0.41066199541092
    STATISTICS_MEAN=-0.14290635876691
    STATISTICS_MINIMUM=-0.43130299448967
    STATISTICS_STDDEV=0.17467622900461
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=3.9832758903503
    STATISTICS_MEAN=2.5114900115167
    STATISTICS_MINIMUM=1.2807140350342
    STATISTICS_STDDEV=0.64540102922043
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=0.064833000302315
    STATISTICS_MEAN=0.035132184284462
    STATISTICS_MINIMUM=0.0016179999802262
    STATISTICS_STDDEV=0.03134354382028
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=0.10483700037003
    STATISTICS_MEAN=0.05122512365606
    STATISTICS_MINIMUM=0.0021490000654012
    STATISTICS_STDDEV=0.045974087201794

Sampling at a "random" point
  Location: (50P,50L)
  Band 1:
    Value: -0.257952004671097
  Band 2:
    Value: 2.9302670955658
  Band 3:
    Value: 0.00161899998784065
  Band 4:
    Value: 0.00236700009554625


[2] BWTA2017.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=-3.058042049408
    STATISTICS_MEAN=-3.5957838572742
    STATISTICS_MINIMUM=-4.0953946113586
    STATISTICS_STDDEV=0.27330481075759
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=4.5378975868225
    STATISTICS_MEAN=3.6829893469828
    STATISTICS_MINIMUM=2.8559820652008
    STATISTICS_STDDEV=0.44504277350973
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=0.00013734232925344
    STATISTICS_MEAN=6.3284008649676e-05
    STATISTICS_MINIMUM=1.6185947970371e-05
    STATISTICS_STDDEV=1.9119506806922e-05
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=0.00020735718135256
    STATISTICS_MEAN=9.5545150040269e-05
    STATISTICS_MINIMUM=2.4437278625555e-05
    STATISTICS_STDDEV=2.88663154724e-05

Sampling at a "random" point
  Location: (1000P,1000L)
  Band 1:
    Value: -3.91857481002808
  Band 2:
    Value: 3.3124144077301
  Band 3:
    Value: 6.86711646267213e-05
  Band 4:
    Value: 0.000103678590676282


[3] nzgd2kgrid0005.gsb:

Band 1
  Description = Latitude Offset (arc seconds)
    STATISTICS_MAXIMUM=7.0297069549561
    STATISTICS_MEAN=6.2095865303887
    STATISTICS_MINIMUM=5.2811942100525
    STATISTICS_STDDEV=0.31210796006199
Band 2
  Description = Longitude Offset (arc seconds)
    STATISTICS_MAXIMUM=0.35285499691963
    STATISTICS_MEAN=-0.57949275997587
    STATISTICS_MINIMUM=-1.6765320301056
    STATISTICS_STDDEV=0.29261751972554
Band 3
  Description = Latitude Error
    STATISTICS_MAXIMUM=0.11701499670744
    STATISTICS_MEAN=0.026246032400208
    STATISTICS_MINIMUM=0.00059700000565499
    STATISTICS_STDDEV=0.02753783661212
Band 4
  Description = Longitude Error
    STATISTICS_MAXIMUM=0.16906200349331
    STATISTICS_MEAN=0.035039901355755
    STATISTICS_MINIMUM=0.00079299998469651
    STATISTICS_STDDEV=0.037121379791356

Sampling at a "random" point
  Location: (50P,50L)
  Band 1:
    Value: 6.31486082077026
  Band 2:
    Value: -0.991177976131439
  Band 3:
    Value: 0.0246409997344017
  Band 4:
    Value: 0.0317060016095638

--
Spatialys - Geospatial professional services http://www.spatialys.com _______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org
https://lists.osgeo.org/mailman/listinfo/proj


Disclaimer:
De inhoud van dit bericht is uitsluitend bestemd voor geadresseerde.
Gebruik van de inhoud van dit bericht door anderen zonder toestemming van het Kadaster 
is onrechtmatig. Mocht dit bericht ten onrechte bij u terecht komen, dan verzoeken wij u 
dit direct te melden aan de verzender en het bericht te vernietigen. 
Aan de inhoud van dit bericht kunnen geen rechten worden ontleend.

Disclaimer:
The content of this message is meant to be received by the addressee only.
Use of the content of this message by anyone other than the addressee without the consent 
of the Kadaster is unlawful. If you have received this message, but are not the addressee, 
please contact the sender immediately and destroy the message.
No rights can be derived from the content of this message

From Jochem.Lesparre at kadaster.nl  Sun Dec  8 05:06:26 2019
From: Jochem.Lesparre at kadaster.nl (Lesparre, Jochem)
Date: Sun, 8 Dec 2019 13:06:26 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <c169afd342b84f74b87094109c4c3631@sdfe.dk>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700> <c169afd342b84f74b87094109c4c3631@sdfe.dk>
Message-ID: <VI1P192MB0016BF4771176B2C63C924EDEF590@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>

Hi,



Kristian wrote:

> Regarding the iterative inverse deformation operation, it works but it isn't particularly good to be honest.

[...]

> Improving this could be a fun challenge for  some one more mathematically inclined than me.





I would like to add two theoretical issues:



1. There is no guarantee that the iterative inverse grid shift operation will always convert to a solution.



The obvious case is when the forward grid shift operation shifts the coordinates of the point of interest outside the bounds of the grid. This often happens near two of the four grid edges. In this case, the inverse operation can't find a solution. This case is handled by PROJ. But I'm not sure about more subtle cases.



During the iteration, the coordinates of the point of interest get shifted multiple times. If in this process the coordinates are shifted back to a previous location, the process will get in an infinite loop. In this loop a point is endlessly shifted back and forth, or in a more complicated pattern like a circle. Of course, such behaviour could not occur in a sensible grid. But most creators of a grid shift file will not check their grid on this.



Another, more likely reason for the inverse grid shift operation not to be able to find a solution is when during the iteration process the coordinates of the point of interest are shifted (temporarily) outside the bounds of the grid.



It could be useful to be able to check grids on these cases?





2. In a NTv2 grid shift file with a subgrid, discontinuities could occur at the edge between the parent grid and the subgrid.



I will explain this with an example. Take a look at the small grid below, with a parent grid (grid points marked with x) and a subgrid (additional grid points marked with +). To interpolate the grid shift at a point of interest on the edge (e.g. point marked with o), one could do this based on the four values of the parent grid points (x) north of the point of interest, or based on the four values of the subgrid points (+) south of the point of interest. Often, the grid shift interpolated from the parent grid and the grid shift interpolated subgrid for a point at the edge (o) will be different. This could cause a discontinuity, as a point slightly north of the point of interest gets a different grid shift as a point slightly south of the point of interest. For a sensible grid, the difference will be so small that it is not significant, but there are no guarantees. A way to check a grid on this could be useful?



It is also possible to prevent such discontinuities to occur at all: by first interpolating two auxiliary points (marked with . ) and then interpolating points with the these two auxiliary points (.) and the points of the subgrid (+). Maybe PROJ. already does interpolate in that way? I didn’t check the source code.



x     x     x     x





x . . x     x     x





x +o+ x + + x     x

+ + + + + + +

+ + + + + + +

x + + x + + x     x



Regards, Jochem



PS: This theoretical issue at the grid edge is not related to the practical issue a grid edges that is reported here: https://github.com/OSGeo/PROJ/issues/1663





-----Original Message-----
From: PROJ <proj-bounces at lists.osgeo.org> On Behalf Of Kristian Evers
Sent: vrijdag 6 december 2019 14:42
To: Even Rouault <even.rouault at spatialys.com>; Chris Crook <ccrook at linz.govt.nz>
Cc: 'proj at lists.osgeo.org' <proj at lists.osgeo.org>
Subject: Re: [PROJ] Proposal for a geodetic deformation model format



Chris et al,



I am late to the party so let me just try to comment on a few things from the discussion below as well as provide a few use cases where I would benefit from this new grid format. Generally I also think this is a very well thought out and thorough description of a new format. It covers most bases for my needs and I think that the "multi file" solution aligns quite closely with current PROJ practice so it shouldn't be too complicated to make things work. Sorry for the long text - I got carried away, as often happens when the subject is interesting!





COORDINATE SPACE(S)



I originally wrote the deformation operation with the sole purpose of providing a practical implementation of the transformations described in the paper by Häkli et al [0]. This obviously influenced the implementation and there is the possibility that it isn't generic enough for all purposes, although I haven't yet come across a situation that wouldn't fit regarding the use 3D deformation models in coordinate transformations.



The deformation operation in PROJ works in a mix of geocentric and geographic coordinates [1]. Actual adjustments are done in geocentric/cartesian space. This is analogous to e.g. a Helmert operation which also works in geocentric space. And as Even points out the deformation operation is often use in a pipeline in conjunction with one or more Helmert operations. I generally prefer to work in geocentric space when doing this sort of work since it is the "native" coordinate space of the frame and because it is nice to work within a linear coordinate systems with axes that intersects at right angles. Unfortunately it is not really possible to create a grid of surface displacements or velocities in geocentric space without creating a massive sparse 3D matrix which is not practical in any way. This is the reason why the deformation operation requires grids in topocentric/ENU space and internally converts between the two coordinate systems. It does come with a bit of overhead, especially in the inverse mode, but it isn't too bad compared to let's say a 14-parameter Helmert operation.



Doing gridshift adjustments in degrees generally works quite well and it what has been done for years and years in PROJ. The main disadvantage, I think, is that it is difficult for the human eye to interpret values in the grid. I tend to find it quite useful to plot the grids I use in QGIS and having the values expressed in meters or millimeters gives me an immediate advantage. There's nothing in the way to support both types of gridshifts. I do think though that expressing velocities in terms of degrees is a bad idea (I don't think that was the intention, just putting it out there).



Regarding the iterative inverse deformation operation, it works but it isn't particularly good to be honest. I reworked the existing iterative algorithm used for horizontal grid shifts into a 3D version and I never quite got it right. It doesn't perform well in a round-trip test and after about ten trips back and forth it has introduced too much noise due to numerical inaccuracy in the algorithm. Improving this could be a fun challenge for  some one more mathematically inclined than me.



I agree that it doesn't make much of a difference whether adjustments in the up components are done in relation to a gravimetric or geometric reference.





GRID DELIVERY METHOD (SINGLE VS. MULTI FILE)



I agree with the proposal that the multi file solution is preferable. Especially if zipped up a in aggregate package. This was also touched upon in the original GitHub discussion 18 months ago. I think JSON would be preferable to a CSV based format since it offers a bit more structure and you would be able to define a proper JSON schema that can be used to validate the format of the file. PROJ has the ability to parse JSON already.



A package of a number of grid files and a metadata file describing the use of the grids should be fairly simple to translate to a PROJ pipeline, provided that the usage of the grids align with operations available in PROJ. It would of course be much simpler if everything was just expressed as a PROJ pipeline but I guess we can't assume that everyone will be using PROJ :-)





RELEVANT USE CASE: GREENLAND



Greenland is subject to significant elastic deformation due to rapid mass-loss from the ice sheet. The deformation varies a lot from year to year which creates a need for a rather complicated deformation model. At the moment I have solved the problem by creating a set deformation models that cover a specific year each. That is, for every year since 1996 I have a 3D deformation model that only applies for that particular year. The plan is to create a pipeline of deformation operations that applies as many adjustments from the deformation models as needed. E.g. if a coordinate from 2010 is transformed it would have to pass through 14 deformation operations (2010-1996) to account for all the deformation. I think for the final transformations I will narrow the number of gridded models down so that they each cover a three year period. The point here is that it would be nice to be able to support this type of transformation in the new deformation model format. If I read the proposal correctly this would be possible but I may have missed some details. PROJ is actually not able to do this as of now but I will extend the deformation operation to allow this in the future (functionality similar to the +t_epoch/+t_final setup in hgridshift is needed).





RELEVANT USE CASE: ICELAND



Similarly to New Zealand, Iceland exists in very a geodynamically complicated setting. Transformations in Iceland should ideally take care of secular (GIA, divergent plate boundary) and episodic deformation (earth quakes, volcanism) as well as the regular Helmert transformations involving ITRFxxxx. In PROJ this can be handled as a pipeline including the helmert, hgridshift, vgridshift and deformation operations. I gave a talk on the subject last year which covers this use case and some thoughts about a practical implementation [2]. Recently that work described in that presentation has culminated in a pull request for the proj-datumgrid repository [3].





/Kristian





[0] Häkli, P., Lidberg, M., Jivall, L., Nørbech, T., Tangen, O., Weber, M., Pihlak, P., Aleksejenko, I., and Paršeliunas, E. The NKG2008 GPS campaign – final transformation results and a new common Nordic reference frame. Journal of Geodetic Science, 6(1):1–33, 2016. https://doi.org/10.1515/jogs-2016-0001



[1] Yes, I use formulas equivalent to what is described in section 4.1.2 "Geocentric/topocentric conversions" of https://www.iogp.org/wp-content/uploads/2019/09/373-07-02.pdf



[2] https://www.fig.net/fig2018/rfip/6_RFIPIstanbulKristianEvers.pdf



[3] https://github.com/OSGeo/proj-datumgrid/pull/67

_______________________________________________

PROJ mailing list

PROJ at lists.osgeo.org<mailto:PROJ at lists.osgeo.org>

https://lists.osgeo.org/mailman/listinfo/proj


Disclaimer:
De inhoud van dit bericht is uitsluitend bestemd voor geadresseerde.
Gebruik van de inhoud van dit bericht door anderen zonder toestemming van het Kadaster 
is onrechtmatig. Mocht dit bericht ten onrechte bij u terecht komen, dan verzoeken wij u 
dit direct te melden aan de verzender en het bericht te vernietigen. 
Aan de inhoud van dit bericht kunnen geen rechten worden ontleend.

Disclaimer:
The content of this message is meant to be received by the addressee only.
Use of the content of this message by anyone other than the addressee without the consent 
of the Kadaster is unlawful. If you have received this message, but are not the addressee, 
please contact the sender immediately and destroy the message.
No rights can be derived from the content of this message
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191208/5a020886/attachment-0001.html>

From even.rouault at spatialys.com  Sun Dec  8 07:10:08 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sun, 08 Dec 2019 16:10:08 +0100
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric error
	in grids (sometimes) acceptable ?
Message-ID: <29187200.ZMgZr9vmcM@even-i700>

Hi,

I'm experimenting with encoding grids with 16-bit integer values, with an 
offset and scale, instead of using IEEE 32-bit floats. There's some connection 
with my previous thread about accuracies of NTv2...

Let's for example take the example of egm08_25.gtx. It is 149.3 MB large
If converting it into a IEEE 32-bit float deflate-compressed (with floating-
point predictor) tiled geotiff, it becomes 80.5 MB large (the compression 
method and floatin-point predictor are fully lossless. There's bit-to-bit 
equivalence for the elevation values regarding the original gtx file)
Now observing that the range of values is in [-107,86], I can remap that to 
[0,65535] with an offset of -107 and scale of (86 - -107) / 65535 ~= 0.0029
The resulting deflate-compressed (with integer predictor) tiled GeoTIFF is now 
23.1 MB large !

Looking at the difference between the unscaled quantized values and the 
original ones, the error is in the [-1.5 mm, 1.5 mm] range (which is expected, 
being the half of the scale value), with a mean value of 4.5e-6 metre (so 
centered), and a standard deviation of 0.85 mm

After that experimentation, I found this interesting page of the GeographicLib 
documentation
https://geographiclib.sourceforge.io/html/geoid.html
which compares the errors introduced by the gridding itself and interpolation 
methods (the EGM model is originally a continuous model), with/without 
quantization. And one conclusion is "If, instead, Geoid were to use data files 
without such quantization artifacts, the overall error would be reduced but 
only modestly". Actually with the bilinear interpolation we use, the max and 
RMS errors with and without quantization are the same... So it seems perfectly 
valid to use such quantized products, at least for EGM2008, right ?

Now looking at horizontal grids, let's consider Australia's 
GDA94_GDA2020_conformal.gsb. It is 83 MB large (half of this size due to the 
error channels, which are set to a dummy value of -1...)
Converting it to a compressed tiled Float32 tif (without those useless error 
channels), make it down to 4.5 MB.
And as a quantitized uint16 compressed tif, down to 1.4 MB (yes, almost 60 
times smaller than original .gsb file). The maximum scale factor is 1.5e-7 
arcsecond, hence a maximum error of 2.3 micrometre... I'm pretty sure we're 
several order of magnitudes beyond the accuracy of the original model, right ?
In EPSG this transformation is reported to have an accuracy of 5cm.
The fact that we get such a small scale factor is due to GDA94 -> GDA2020 
conformal being mostly a uniform shift of ~1.8 m and that the grids is 
mentioned to "Gives identical results to Helmert transformation GDA94 to 
GDA2020 (1)"

If we look at the France' ntf_r93.gsb, which has shifts of an amplitude up to 
130m, the maximum error introduced by the quantization is 0.6 mm. I would tend 
to think this is also acceptable (given the size of that particular file is 
small, compression gains are quite neglectable, but this is mostly to look if 
we can generalize such mechanism). What puzzles me is that in
https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/
NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf where they compare the  
NTv2 approach regarding their native 3D geocentric correction approach, they 
underline in red a sample point where the difference between the 2 models is 
1.2 mm, as if it had really some importance. For that test point, using the 
quantized approach would increase this difference to 1.3 mm. But when looking 
at the accuracy reported in the grid at that point it is 1.6e-3 arc-second 
(which is the minimum value for the latitude error of the product, by the 
way), ie 5cm, so it seems to me that discussing about millimetric error 
doesn't make sense.
In EPSG this transformation is reported to have an accuracy of 1 metre (which 
is consistent with the mean value of the latitude shift error)

Now, let's look at the freshly introduced BWTA2017.gsb file. 392 MB large
As a Float32 compressed geotiff: 73 MB (5.4x compression rate)
As a Int16 compressed geotiff: 26 MB (15x compression rate)
Maximum error added by quantization for latitude shift: 0.25 mm
Minimum error value advertized for latitude shift: 1.61e-5 arc-second (not 
completely sure about the units...), ie 0.5mm
Mean error value advertized for latitude shift: 6.33e-5 arc-second, ie 1.9mm
Interestingly when looking at the ASCII version of the grid, the values of the 
shifts are given with a precision of 1e-6 arcseconds, that is 0.03 mm !

For Canadian NTv2_0.gsb, on the first subgrid, the quantization error is
0.9 mm for the latitude shift. The advertized error for latitude is in [0, 
13.35 m] (the 0.000 value is really surprising. reached on a couple points 
only), with a mean at 0.27 m and stddev at 0.48 m. In EPSG this transformation 
is reported to have an accuracy of 1.5 metre

~~~~~~~~~~~~~

So, TLDR, is it safe (and worth) to generate quantized products to reducte by 
about a factor of 2 to 3 the size of our grids compared to unquantized 
products, when the maximum error added by the quantization is ~ 1mm or less ? 
or will data producers consider we damage the quality of products they 
carefully crafted ? do some users need millimetric / sub-millimetric accuracy 
?

Or do we need to condition quantization to a criterion or a combination of 
criteria like:
- a maximum absolute error that quantization introduces (1 mm ? 0.1 mm ?)
- a maximum value for the ratio between the maximum absolute error that 
quantization introduces over the minimum error value advertized (when known) 
below some value ? For the BWTA2017 product, this ratio is 0.5. For 
ntf_r93.gsb, 0.012. For NTv2_0.gsb, cannot be computed given sothe min error 
value advertized is 0...
- or, variant of the above, a maximum value for the ratio between the maximum 
absolute that quantization introduces  over the mean of the error value 
advertized (when known). For the BWTA2017 product, this ratio is 0.13. For 
ntf_r93.gsb, 5.5e-4. For NTv2_0.gsb, 3.3e-3
- and perhaps consider that only for products above a given size (still larger 
than 10 MB after lossless compression ?)

~~~~~~~~~~~~~~~~

Jochem mentionned in the previous thread that the Netherlands grids have an 
accuracy of 1mm. I'm really intrigued by what that means. Does that mean that 
the position of control points used to build the grid is known at that 
accuracy, both in the source and target systems, and that when using bilinear 
interpolation with the grid, one remains within that accuracy ? Actually both 
the rdtrans2008.gsb and rdtrans2018.gsb grids report an accuracy of 1mm, but 
when comparing the positions corrected by those 2 grids, I get differences 
above 1mm.

echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2008.gsb
   5.999476046    52.998912757

echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2018.gsb
   5.999476020    52.998912753

echo "52.998912757 5.999476046 52.998912753 5.999476020" | geod -I 
+ellps=GRS80
-104d18'21.49"	75d41'38.51"	0.002

That's a difference of 2 mm. I get that difference on a few other "random" 
points.

If applying the quantization on rdtrans2018.gsb, we'd add an additional 
maximal error of 0.6 mm. The grid being 1.5 MB uncompressed, and 284 KB as a 
losslessly compressed TIFF, quantization isn't really worth considering 
(reduces the file size to 78 KB)

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From even.rouault at spatialys.com  Sun Dec  8 07:54:43 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sun, 08 Dec 2019 16:54:43 +0100
Subject: [PROJ] Proposal for a geodetic deformation model format
Message-ID: <5023925.2VTTFS5N6N@even-i700>

Hi Jochem,

(top-posting, because, for some reason, part of the list traffic doesn't reach 
my email box...)

> 2. In a NTv2 grid shift file with a subgrid, discontinuities could occur at 
the edge between the parent grid and the subgrid.
> Maybe PROJ. already does interpolate in that way? I didn’t check the source 
code.

No PROJ does it the easy way: if the point to correct falls in a (sub)grid, it 
interpolates only within that subgrid.
Actually, this is slightly more subtle than that: PROJ selects a subgrid, even 
if the point to transform is slightly outside it (epsilon based on a 1 / 10000 
of the (sub)grid resolution: see
https://github.com/OSGeo/PROJ/blob/master/src/apply_gridshift.cpp#L127 )
And for the reverse transformation, it also stays within the subgrid it 
initially selected.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Jochem.Lesparre at kadaster.nl  Sun Dec  8 08:25:12 2019
From: Jochem.Lesparre at kadaster.nl (Lesparre, Jochem)
Date: Sun, 8 Dec 2019 16:25:12 +0000
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric
	error	in grids (sometimes) acceptable ?
In-Reply-To: <29187200.ZMgZr9vmcM@even-i700>
References: <29187200.ZMgZr9vmcM@even-i700>
Message-ID: <VI1P192MB0016633C14D0C6FB2F667313EF590@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>

Hi Even,

I will try the clarify the accuracy of the grid shift files for the Netherlands. 

The core control points (CORS) in the Netherlands have a precision of a few millimetres to about a centimetre (depending on what you use as reference). The lower order control points have a precision in the order of a centimetre. 

However, our national coordinate system RD is defined as the transformation from ETRS89 as we publish it. This transformation is therefore perfect. The only inaccuracy comes from numerical issues like iterations etc. in the order of 0.1 mm.

The difference between rdtrans2008.gsb and rdtrans2018.gsb is at maximum 1 cm in on-shore Netherlands. Strictly speaking, this is a redefinition of RD and the 2008 definition should only be used with ETRF2000(R05) coordinates and the 2018 definition only with ETRF2000(R14) coordinates. In practise, the difference doesn't matter to the users, as long as the most precise users do not mix the 2008 and 2018 definition within one project.

The most precise use of RD is in the order of 1 cm. To make sure that implementations of the transformation between RD and ETRS89 are one order of magnitude better, we demand software developers that want to use the trademark RDNAPTRANS to implement the transformation with an accuracy of 1 mm. It is therefore important that when the grid shift file of the Netherlands is converted from NTv2 to another data format, the differences are well below 1 mm, and preferably in the order of 0.1 mm.

The idea behind this is that when surveyors measure with 1 cm precision, the accuracy of the implementation of the transformation they use should be 1 mm. We compare implementations of the transformation in surveying and GIS software with the PROJ. implementation to check if it is within 1 mm. For a fair comparison, the PROJ. implementation should have an accuracy of 0.1 mm and also the different grids we publish should be consistent within 0.1 mm.  

Regards, Jochem 


-----Original Message-----
From: PROJ <proj-bounces at lists.osgeo.org> On Behalf Of Even Rouault
Sent: zondag 8 december 2019 16:10
To: proj at lists.osgeo.org
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric error in grids (sometimes) acceptable ?

Hi,

I'm experimenting with encoding grids with 16-bit integer values, with an offset and scale, instead of using IEEE 32-bit floats. There's some connection with my previous thread about accuracies of NTv2...

Let's for example take the example of egm08_25.gtx. It is 149.3 MB large If converting it into a IEEE 32-bit float deflate-compressed (with floating- point predictor) tiled geotiff, it becomes 80.5 MB large (the compression method and floatin-point predictor are fully lossless. There's bit-to-bit equivalence for the elevation values regarding the original gtx file) Now observing that the range of values is in [-107,86], I can remap that to [0,65535] with an offset of -107 and scale of (86 - -107) / 65535 ~= 0.0029 The resulting deflate-compressed (with integer predictor) tiled GeoTIFF is now
23.1 MB large !

Looking at the difference between the unscaled quantized values and the original ones, the error is in the [-1.5 mm, 1.5 mm] range (which is expected, being the half of the scale value), with a mean value of 4.5e-6 metre (so centered), and a standard deviation of 0.85 mm

After that experimentation, I found this interesting page of the GeographicLib documentation https://geographiclib.sourceforge.io/html/geoid.html
which compares the errors introduced by the gridding itself and interpolation methods (the EGM model is originally a continuous model), with/without quantization. And one conclusion is "If, instead, Geoid were to use data files without such quantization artifacts, the overall error would be reduced but only modestly". Actually with the bilinear interpolation we use, the max and RMS errors with and without quantization are the same... So it seems perfectly valid to use such quantized products, at least for EGM2008, right ?

Now looking at horizontal grids, let's consider Australia's GDA94_GDA2020_conformal.gsb. It is 83 MB large (half of this size due to the error channels, which are set to a dummy value of -1...) Converting it to a compressed tiled Float32 tif (without those useless error channels), make it down to 4.5 MB.
And as a quantitized uint16 compressed tif, down to 1.4 MB (yes, almost 60 times smaller than original .gsb file). The maximum scale factor is 1.5e-7 arcsecond, hence a maximum error of 2.3 micrometre... I'm pretty sure we're several order of magnitudes beyond the accuracy of the original model, right ?
In EPSG this transformation is reported to have an accuracy of 5cm.
The fact that we get such a small scale factor is due to GDA94 -> GDA2020 conformal being mostly a uniform shift of ~1.8 m and that the grids is mentioned to "Gives identical results to Helmert transformation GDA94 to
GDA2020 (1)"

If we look at the France' ntf_r93.gsb, which has shifts of an amplitude up to 130m, the maximum error introduced by the quantization is 0.6 mm. I would tend to think this is also acceptable (given the size of that particular file is small, compression gains are quite neglectable, but this is mostly to look if we can generalize such mechanism). What puzzles me is that in https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/
NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf where they compare the
NTv2 approach regarding their native 3D geocentric correction approach, they underline in red a sample point where the difference between the 2 models is
1.2 mm, as if it had really some importance. For that test point, using the quantized approach would increase this difference to 1.3 mm. But when looking at the accuracy reported in the grid at that point it is 1.6e-3 arc-second (which is the minimum value for the latitude error of the product, by the way), ie 5cm, so it seems to me that discussing about millimetric error doesn't make sense.
In EPSG this transformation is reported to have an accuracy of 1 metre (which is consistent with the mean value of the latitude shift error)

Now, let's look at the freshly introduced BWTA2017.gsb file. 392 MB large As a Float32 compressed geotiff: 73 MB (5.4x compression rate) As a Int16 compressed geotiff: 26 MB (15x compression rate) Maximum error added by quantization for latitude shift: 0.25 mm Minimum error value advertized for latitude shift: 1.61e-5 arc-second (not completely sure about the units...), ie 0.5mm Mean error value advertized for latitude shift: 6.33e-5 arc-second, ie 1.9mm Interestingly when looking at the ASCII version of the grid, the values of the shifts are given with a precision of 1e-6 arcseconds, that is 0.03 mm !

For Canadian NTv2_0.gsb, on the first subgrid, the quantization error is
0.9 mm for the latitude shift. The advertized error for latitude is in [0,
13.35 m] (the 0.000 value is really surprising. reached on a couple points only), with a mean at 0.27 m and stddev at 0.48 m. In EPSG this transformation is reported to have an accuracy of 1.5 metre

~~~~~~~~~~~~~

So, TLDR, is it safe (and worth) to generate quantized products to reducte by about a factor of 2 to 3 the size of our grids compared to unquantized products, when the maximum error added by the quantization is ~ 1mm or less ? 
or will data producers consider we damage the quality of products they carefully crafted ? do some users need millimetric / sub-millimetric accuracy ?

Or do we need to condition quantization to a criterion or a combination of criteria like:
- a maximum absolute error that quantization introduces (1 mm ? 0.1 mm ?)
- a maximum value for the ratio between the maximum absolute error that quantization introduces over the minimum error value advertized (when known) below some value ? For the BWTA2017 product, this ratio is 0.5. For ntf_r93.gsb, 0.012. For NTv2_0.gsb, cannot be computed given sothe min error value advertized is 0...
- or, variant of the above, a maximum value for the ratio between the maximum absolute that quantization introduces  over the mean of the error value advertized (when known). For the BWTA2017 product, this ratio is 0.13. For ntf_r93.gsb, 5.5e-4. For NTv2_0.gsb, 3.3e-3
- and perhaps consider that only for products above a given size (still larger than 10 MB after lossless compression ?)

~~~~~~~~~~~~~~~~

Jochem mentionned in the previous thread that the Netherlands grids have an accuracy of 1mm. I'm really intrigued by what that means. Does that mean that the position of control points used to build the grid is known at that accuracy, both in the source and target systems, and that when using bilinear interpolation with the grid, one remains within that accuracy ? Actually both the rdtrans2008.gsb and rdtrans2018.gsb grids report an accuracy of 1mm, but when comparing the positions corrected by those 2 grids, I get differences above 1mm.

echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2008.gsb
   5.999476046    52.998912757

echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2018.gsb
   5.999476020    52.998912753

echo "52.998912757 5.999476046 52.998912753 5.999476020" | geod -I 
+ellps=GRS80
-104d18'21.49"	75d41'38.51"	0.002

That's a difference of 2 mm. I get that difference on a few other "random" 
points.

If applying the quantization on rdtrans2018.gsb, we'd add an additional maximal error of 0.6 mm. The grid being 1.5 MB uncompressed, and 284 KB as a losslessly compressed TIFF, quantization isn't really worth considering (reduces the file size to 78 KB)

Even

--
Spatialys - Geospatial professional services http://www.spatialys.com _______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org
https://lists.osgeo.org/mailman/listinfo/proj


Disclaimer:
De inhoud van dit bericht is uitsluitend bestemd voor geadresseerde.
Gebruik van de inhoud van dit bericht door anderen zonder toestemming van het Kadaster 
is onrechtmatig. Mocht dit bericht ten onrechte bij u terecht komen, dan verzoeken wij u 
dit direct te melden aan de verzender en het bericht te vernietigen. 
Aan de inhoud van dit bericht kunnen geen rechten worden ontleend.

Disclaimer:
The content of this message is meant to be received by the addressee only.
Use of the content of this message by anyone other than the addressee without the consent 
of the Kadaster is unlawful. If you have received this message, but are not the addressee, 
please contact the sender immediately and destroy the message.
No rights can be derived from the content of this message

From charles at karney.com  Sun Dec  8 08:34:54 2019
From: charles at karney.com (Charles Karney)
Date: Sun, 8 Dec 2019 11:34:54 -0500
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric
 error in grids (sometimes) acceptable ?
In-Reply-To: <29187200.ZMgZr9vmcM@even-i700>
References: <29187200.ZMgZr9vmcM@even-i700>
Message-ID: <0427d617-ce40-5a4d-f381-6fe87c1a765a@karney.com>

I adopted 16-bit integer storage for geoid models with GeographicLib 10
years ago.  Back then the main motivation was to minimize the memory
consumption in cases where it was desirable to read the whole model into
memory.  This is less of a concern nowadays.

A couple of other observations:

Fixed-point numbers are clearly a preferable format compared with
floating-point numbers for several quantities in the GIS arena.  For
latitude, longitudes, azimuths, and heights we usually don't care about
representing huge values (1e100, say) nor with minimizing the *relative*
error for tiny values (1e-100, say).  So fixed-point numbers are
sometimes a natural choice.  (Representing this data is floats
inevitably confounds compression algorithms because of what is
essentially noise in the least significant bits of the numbers.)

GeographicLib's Geoid class makes an attempt to ameliorate the
quantization errors when using cubic interpolation by doing an
over-determined fit to 12 points in the grid.  This reduces the RMS
error somewhat, but presumably does not affect the maximum error.

I confess that I've always had qualms about this representation for
geoid heights.  16-bits is just barely enough for EGM2008.

If the concern is the size of the compressed data on disk, perhaps it is
worth being a little bit more ambitious.  How about representing the
data as integers (let's say 32-bit unsigned, but maybe it would be good
to allow for 64-bit and for signed) with an offset and scale?  Then the
question is what is the "best" way of compressing this data with the
following criteria:

* do a good job is the range of the data is restricted, i.e., if the
   integer might have fit into 16-bits.

* do a good job given that the data is usually a slowly varying function
   in 1, 2, or 3 dimensions.

* allow random access without too much overhead.

* an implementation is freely available.

Some of these issues are addressed by the LASzip compression scheme for
LAS data.


On 12/8/19 10:10 AM, Even Rouault wrote:
> Hi,
> 
> I'm experimenting with encoding grids with 16-bit integer values, with an
> offset and scale, instead of using IEEE 32-bit floats. There's some connection
> with my previous thread about accuracies of NTv2...
> 
> Let's for example take the example of egm08_25.gtx. It is 149.3 MB large
> If converting it into a IEEE 32-bit float deflate-compressed (with floating-
> point predictor) tiled geotiff, it becomes 80.5 MB large (the compression
> method and floatin-point predictor are fully lossless. There's bit-to-bit
> equivalence for the elevation values regarding the original gtx file)
> Now observing that the range of values is in [-107,86], I can remap that to
> [0,65535] with an offset of -107 and scale of (86 - -107) / 65535 ~= 0.0029
> The resulting deflate-compressed (with integer predictor) tiled GeoTIFF is now
> 23.1 MB large !
> 
> Looking at the difference between the unscaled quantized values and the
> original ones, the error is in the [-1.5 mm, 1.5 mm] range (which is expected,
> being the half of the scale value), with a mean value of 4.5e-6 metre (so
> centered), and a standard deviation of 0.85 mm
> 
> After that experimentation, I found this interesting page of the GeographicLib
> documentation
> https://geographiclib.sourceforge.io/html/geoid.html
> which compares the errors introduced by the gridding itself and interpolation
> methods (the EGM model is originally a continuous model), with/without
> quantization. And one conclusion is "If, instead, Geoid were to use data files
> without such quantization artifacts, the overall error would be reduced but
> only modestly". Actually with the bilinear interpolation we use, the max and
> RMS errors with and without quantization are the same... So it seems perfectly
> valid to use such quantized products, at least for EGM2008, right ?
> 
> Now looking at horizontal grids, let's consider Australia's
> GDA94_GDA2020_conformal.gsb. It is 83 MB large (half of this size due to the
> error channels, which are set to a dummy value of -1...)
> Converting it to a compressed tiled Float32 tif (without those useless error
> channels), make it down to 4.5 MB.
> And as a quantitized uint16 compressed tif, down to 1.4 MB (yes, almost 60
> times smaller than original .gsb file). The maximum scale factor is 1.5e-7
> arcsecond, hence a maximum error of 2.3 micrometre... I'm pretty sure we're
> several order of magnitudes beyond the accuracy of the original model, right ?
> In EPSG this transformation is reported to have an accuracy of 5cm.
> The fact that we get such a small scale factor is due to GDA94 -> GDA2020
> conformal being mostly a uniform shift of ~1.8 m and that the grids is
> mentioned to "Gives identical results to Helmert transformation GDA94 to
> GDA2020 (1)"
> 
> If we look at the France' ntf_r93.gsb, which has shifts of an amplitude up to
> 130m, the maximum error introduced by the quantization is 0.6 mm. I would tend
> to think this is also acceptable (given the size of that particular file is
> small, compression gains are quite neglectable, but this is mostly to look if
> we can generalize such mechanism). What puzzles me is that in
> https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/
> NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf where they compare the
> NTv2 approach regarding their native 3D geocentric correction approach, they
> underline in red a sample point where the difference between the 2 models is
> 1.2 mm, as if it had really some importance. For that test point, using the
> quantized approach would increase this difference to 1.3 mm. But when looking
> at the accuracy reported in the grid at that point it is 1.6e-3 arc-second
> (which is the minimum value for the latitude error of the product, by the
> way), ie 5cm, so it seems to me that discussing about millimetric error
> doesn't make sense.
> In EPSG this transformation is reported to have an accuracy of 1 metre (which
> is consistent with the mean value of the latitude shift error)
> 
> Now, let's look at the freshly introduced BWTA2017.gsb file. 392 MB large
> As a Float32 compressed geotiff: 73 MB (5.4x compression rate)
> As a Int16 compressed geotiff: 26 MB (15x compression rate)
> Maximum error added by quantization for latitude shift: 0.25 mm
> Minimum error value advertized for latitude shift: 1.61e-5 arc-second (not
> completely sure about the units...), ie 0.5mm
> Mean error value advertized for latitude shift: 6.33e-5 arc-second, ie 1.9mm
> Interestingly when looking at the ASCII version of the grid, the values of the
> shifts are given with a precision of 1e-6 arcseconds, that is 0.03 mm !
> 
> For Canadian NTv2_0.gsb, on the first subgrid, the quantization error is
> 0.9 mm for the latitude shift. The advertized error for latitude is in [0,
> 13.35 m] (the 0.000 value is really surprising. reached on a couple points
> only), with a mean at 0.27 m and stddev at 0.48 m. In EPSG this transformation
> is reported to have an accuracy of 1.5 metre
> 
> ~~~~~~~~~~~~~
> 
> So, TLDR, is it safe (and worth) to generate quantized products to reducte by
> about a factor of 2 to 3 the size of our grids compared to unquantized
> products, when the maximum error added by the quantization is ~ 1mm or less ?
> or will data producers consider we damage the quality of products they
> carefully crafted ? do some users need millimetric / sub-millimetric accuracy
> ?
> 
> Or do we need to condition quantization to a criterion or a combination of
> criteria like:
> - a maximum absolute error that quantization introduces (1 mm ? 0.1 mm ?)
> - a maximum value for the ratio between the maximum absolute error that
> quantization introduces over the minimum error value advertized (when known)
> below some value ? For the BWTA2017 product, this ratio is 0.5. For
> ntf_r93.gsb, 0.012. For NTv2_0.gsb, cannot be computed given sothe min error
> value advertized is 0...
> - or, variant of the above, a maximum value for the ratio between the maximum
> absolute that quantization introduces  over the mean of the error value
> advertized (when known). For the BWTA2017 product, this ratio is 0.13. For
> ntf_r93.gsb, 5.5e-4. For NTv2_0.gsb, 3.3e-3
> - and perhaps consider that only for products above a given size (still larger
> than 10 MB after lossless compression ?)
> 
> ~~~~~~~~~~~~~~~~
> 
> Jochem mentionned in the previous thread that the Netherlands grids have an
> accuracy of 1mm. I'm really intrigued by what that means. Does that mean that
> the position of control points used to build the grid is known at that
> accuracy, both in the source and target systems, and that when using bilinear
> interpolation with the grid, one remains within that accuracy ? Actually both
> the rdtrans2008.gsb and rdtrans2018.gsb grids report an accuracy of 1mm, but
> when comparing the positions corrected by those 2 grids, I get differences
> above 1mm.
> 
> echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2008.gsb
>     5.999476046    52.998912757
> 
> echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2018.gsb
>     5.999476020    52.998912753
> 
> echo "52.998912757 5.999476046 52.998912753 5.999476020" | geod -I
> +ellps=GRS80
> -104d18'21.49"	75d41'38.51"	0.002
> 
> That's a difference of 2 mm. I get that difference on a few other "random"
> points.
> 
> If applying the quantization on rdtrans2018.gsb, we'd add an additional
> maximal error of 0.6 mm. The grid being 1.5 MB uncompressed, and 284 KB as a
> losslessly compressed TIFF, quantization isn't really worth considering
> (reduces the file size to 78 KB)
> 
> Even
> 

From dagnew at ucsd.edu  Sun Dec  8 08:20:09 2019
From: dagnew at ucsd.edu (Duncan Agnew)
Date: Sun, 8 Dec 2019 08:20:09 -0800
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric
 error in grids (sometimes) acceptable ?
In-Reply-To: <29187200.ZMgZr9vmcM@even-i700>
References: <29187200.ZMgZr9vmcM@even-i700>
Message-ID: <CAG6Em0A79Q48M-xOPTGXoAoK4mpFRMznmM_7pCamgSAGOYTVQw@mail.gmail.com>

In my understanding (derived from metrology), "accuracy" means "how close
to an actual measured or measurable quantity is the number our system
gives?"
and "precision" means "how many decimal places does the system give?". If
the claimed precision is much more than the actual accuracy, what you get
is a
lot of meaningless numbers (insignificant digits, if you will). 1 mm
accuracy is hard, 0.1 mm attainable only under very special circumstances
and over short
distances.

However, for the purpose of checking algorithms against each other, the
"accuracy" becomes "does my algorithm give the same number as the one I'm
comparing
against?" (which might be some "official" one). Since it is all arithmetic,
the accuracy can in principle be zero, and so (in principle) you want very
high precision.

So, when talking about the level of precision that is appropriate, I think
it is important to separate mathematical and physical accuracy.

Duncan Agnew

On Sun, Dec 8, 2019 at 7:10 AM Even Rouault <even.rouault at spatialys.com>
wrote:

> Hi,
>
> I'm experimenting with encoding grids with 16-bit integer values, with an
> offset and scale, instead of using IEEE 32-bit floats. There's some
> connection
> with my previous thread about accuracies of NTv2...
>
> Let's for example take the example of egm08_25.gtx. It is 149.3 MB large
> If converting it into a IEEE 32-bit float deflate-compressed (with
> floating-
> point predictor) tiled geotiff, it becomes 80.5 MB large (the compression
> method and floatin-point predictor are fully lossless. There's bit-to-bit
> equivalence for the elevation values regarding the original gtx file)
> Now observing that the range of values is in [-107,86], I can remap that
> to
> [0,65535] with an offset of -107 and scale of (86 - -107) / 65535 ~= 0.0029
> The resulting deflate-compressed (with integer predictor) tiled GeoTIFF is
> now
> 23.1 MB large !
>
> Looking at the difference between the unscaled quantized values and the
> original ones, the error is in the [-1.5 mm, 1.5 mm] range (which is
> expected,
> being the half of the scale value), with a mean value of 4.5e-6 metre (so
> centered), and a standard deviation of 0.85 mm
>
> After that experimentation, I found this interesting page of the
> GeographicLib
> documentation
> https://geographiclib.sourceforge.io/html/geoid.html
> which compares the errors introduced by the gridding itself and
> interpolation
> methods (the EGM model is originally a continuous model), with/without
> quantization. And one conclusion is "If, instead, Geoid were to use data
> files
> without such quantization artifacts, the overall error would be reduced
> but
> only modestly". Actually with the bilinear interpolation we use, the max
> and
> RMS errors with and without quantization are the same... So it seems
> perfectly
> valid to use such quantized products, at least for EGM2008, right ?
>
> Now looking at horizontal grids, let's consider Australia's
> GDA94_GDA2020_conformal.gsb. It is 83 MB large (half of this size due to
> the
> error channels, which are set to a dummy value of -1...)
> Converting it to a compressed tiled Float32 tif (without those useless
> error
> channels), make it down to 4.5 MB.
> And as a quantitized uint16 compressed tif, down to 1.4 MB (yes, almost 60
> times smaller than original .gsb file). The maximum scale factor is 1.5e-7
> arcsecond, hence a maximum error of 2.3 micrometre... I'm pretty sure
> we're
> several order of magnitudes beyond the accuracy of the original model,
> right ?
> In EPSG this transformation is reported to have an accuracy of 5cm.
> The fact that we get such a small scale factor is due to GDA94 -> GDA2020
> conformal being mostly a uniform shift of ~1.8 m and that the grids is
> mentioned to "Gives identical results to Helmert transformation GDA94 to
> GDA2020 (1)"
>
> If we look at the France' ntf_r93.gsb, which has shifts of an amplitude up
> to
> 130m, the maximum error introduced by the quantization is 0.6 mm. I would
> tend
> to think this is also acceptable (given the size of that particular file
> is
> small, compression gains are quite neglectable, but this is mostly to look
> if
> we can generalize such mechanism). What puzzles me is that in
> https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/
> NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf
> <https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf>
> where they compare the
> NTv2 approach regarding their native 3D geocentric correction approach,
> they
> underline in red a sample point where the difference between the 2 models
> is
> 1.2 mm, as if it had really some importance. For that test point, using
> the
> quantized approach would increase this difference to 1.3 mm. But when
> looking
> at the accuracy reported in the grid at that point it is 1.6e-3 arc-second
> (which is the minimum value for the latitude error of the product, by the
> way), ie 5cm, so it seems to me that discussing about millimetric error
> doesn't make sense.
> In EPSG this transformation is reported to have an accuracy of 1 metre
> (which
> is consistent with the mean value of the latitude shift error)
>
> Now, let's look at the freshly introduced BWTA2017.gsb file. 392 MB large
> As a Float32 compressed geotiff: 73 MB (5.4x compression rate)
> As a Int16 compressed geotiff: 26 MB (15x compression rate)
> Maximum error added by quantization for latitude shift: 0.25 mm
> Minimum error value advertized for latitude shift: 1.61e-5 arc-second (not
> completely sure about the units...), ie 0.5mm
> Mean error value advertized for latitude shift: 6.33e-5 arc-second, ie
> 1.9mm
> Interestingly when looking at the ASCII version of the grid, the values of
> the
> shifts are given with a precision of 1e-6 arcseconds, that is 0.03 mm !
>
> For Canadian NTv2_0.gsb, on the first subgrid, the quantization error is
> 0.9 mm for the latitude shift. The advertized error for latitude is in [0,
> 13.35 m] (the 0.000 value is really surprising. reached on a couple points
> only), with a mean at 0.27 m and stddev at 0.48 m. In EPSG this
> transformation
> is reported to have an accuracy of 1.5 metre
>
> ~~~~~~~~~~~~~
>
> So, TLDR, is it safe (and worth) to generate quantized products to reducte
> by
> about a factor of 2 to 3 the size of our grids compared to unquantized
> products, when the maximum error added by the quantization is ~ 1mm or
> less ?
> or will data producers consider we damage the quality of products they
> carefully crafted ? do some users need millimetric / sub-millimetric
> accuracy
> ?
>
> Or do we need to condition quantization to a criterion or a combination of
> criteria like:
> - a maximum absolute error that quantization introduces (1 mm ? 0.1 mm ?)
> - a maximum value for the ratio between the maximum absolute error that
> quantization introduces over the minimum error value advertized (when
> known)
> below some value ? For the BWTA2017 product, this ratio is 0.5. For
> ntf_r93.gsb, 0.012. For NTv2_0.gsb, cannot be computed given sothe min
> error
> value advertized is 0...
> - or, variant of the above, a maximum value for the ratio between the
> maximum
> absolute that quantization introduces  over the mean of the error value
> advertized (when known). For the BWTA2017 product, this ratio is 0.13. For
> ntf_r93.gsb, 5.5e-4. For NTv2_0.gsb, 3.3e-3
> - and perhaps consider that only for products above a given size (still
> larger
> than 10 MB after lossless compression ?)
>
> ~~~~~~~~~~~~~~~~
>
> Jochem mentionned in the previous thread that the Netherlands grids have
> an
> accuracy of 1mm. I'm really intrigued by what that means. Does that mean
> that
> the position of control points used to build the grid is known at that
> accuracy, both in the source and target systems, and that when using
> bilinear
> interpolation with the grid, one remains within that accuracy ? Actually
> both
> the rdtrans2008.gsb and rdtrans2018.gsb grids report an accuracy of 1mm,
> but
> when comparing the positions corrected by those 2 grids, I get differences
> above 1mm.
>
> echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2008.gsb
>    5.999476046    52.998912757
>
> echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2018.gsb
>    5.999476020    52.998912753
>
> echo "52.998912757 5.999476046 52.998912753 5.999476020" | geod -I
> +ellps=GRS80
> -104d18'21.49"  75d41'38.51"    0.002
>
> That's a difference of 2 mm. I get that difference on a few other "random"
> points.
>
> If applying the quantization on rdtrans2018.gsb, we'd add an additional
> maximal error of 0.6 mm. The grid being 1.5 MB uncompressed, and 284 KB as
> a
> losslessly compressed TIFF, quantization isn't really worth considering
> (reduces the file size to 78 KB)
>
> Even
>
> --
> Spatialys - Geospatial professional services
> http://www.spatialys.com
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191208/120d424b/attachment-0001.html>

From even.rouault at spatialys.com  Sun Dec  8 09:28:00 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sun, 08 Dec 2019 18:28:00 +0100
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric
	error in grids (sometimes) acceptable ?
In-Reply-To: <0427d617-ce40-5a4d-f381-6fe87c1a765a@karney.com>
References: <29187200.ZMgZr9vmcM@even-i700>
 <0427d617-ce40-5a4d-f381-6fe87c1a765a@karney.com>
Message-ID: <5554072.amV74hVU7R@even-i700>

> This is less of a concern nowadays.

If that were only true... I've been bugged this week about an issue with 
reprojecting data in the US from NAD27 to WGS84 with one software package and 
having errors in the range 30 to 100 m. After digging, I found that this 
software didn't ship with the proj-datumgrid-1.8 base package and thus lacked 
the CONUS grid. The maintainer of the package is hesitating a bit to add an 
extra 6 MB to it...

> If the concern is the size of the compressed data on disk, perhaps it is
> worth being a little bit more ambitious.  How about representing the
> data as integers (let's say 32-bit unsigned, but maybe it would be good
> to allow for 64-bit and for signed) with an offset and scale?

I've tried that with 20,- 24- and 32-bit integers and scaling/offseting on 
EGM2008 and the resulting files are larger than using IEEE 32-bit float with 
horizontal floating-point predictor & DEFLATE. Only going down to 16-bit 
brings substential compression rate improvements. That's about all we can do 
if we stay with what libtiff currently supports and not invent another codec. 
A design constraint is that we should be able to open the grids with QGIS and 
that they display correctly out-of-the box (which is the case we the solutions 
I've investigated)

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Jochem.Lesparre at kadaster.nl  Sun Dec  8 11:23:20 2019
From: Jochem.Lesparre at kadaster.nl (Lesparre, Jochem)
Date: Sun, 8 Dec 2019 19:23:20 +0000
Subject: [PROJ] Is 16-bit quantization of values / (sub-)millimetric
 error in grids (sometimes) acceptable ?
In-Reply-To: <CAG6Em0A79Q48M-xOPTGXoAoK4mpFRMznmM_7pCamgSAGOYTVQw@mail.gmail.com>
References: <29187200.ZMgZr9vmcM@even-i700>
 <CAG6Em0A79Q48M-xOPTGXoAoK4mpFRMznmM_7pCamgSAGOYTVQw@mail.gmail.com>
Message-ID: <VI1P192MB0016C4668A139DECD0D4FC84EF590@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>

Hi Duncan,

You might be right for metrology, but in geodesy the definitions are different (although many geodesists mix them up or disagree with each other on the exact definitions). This is my opinion:

Accuracy is the systematic error (bias);
Precision is de standard deviation (sigma);
Accuracy and precision together are the root mean square error (RMSE);
Number of digits is the registration resolution;
Internal reliability is de minimal detectable bias (MDB) due to redundancy;
External reliability is the bias to noise ratio (BNR) due to the MDB.

Regards, Jochem


From: PROJ <proj-bounces at lists.osgeo.org> On Behalf Of Duncan Agnew
Sent: zondag 8 december 2019 17:20
To: Even Rouault <even.rouault at spatialys.com>
Cc: proj at lists.osgeo.org
Subject: Re: [PROJ] Is 16-bit quantization of values / (sub-)millimetric error in grids (sometimes) acceptable ?

In my understanding (derived from metrology), "accuracy" means "how close to an actual measured or measurable quantity is the number our system gives?"
and "precision" means "how many decimal places does the system give?". If the claimed precision is much more than the actual accuracy, what you get is a
lot of meaningless numbers (insignificant digits, if you will). 1 mm accuracy is hard, 0.1 mm attainable only under very special circumstances and over short
distances.

However, for the purpose of checking algorithms against each other, the "accuracy" becomes "does my algorithm give the same number as the one I'm comparing
against?" (which might be some "official" one). Since it is all arithmetic, the accuracy can in principle be zero, and so (in principle) you want very high precision.

So, when talking about the level of precision that is appropriate, I think it is important to separate mathematical and physical accuracy.

Duncan Agnew

On Sun, Dec 8, 2019 at 7:10 AM Even Rouault <even.rouault at spatialys.com<mailto:even.rouault at spatialys.com>> wrote:
Hi,

I'm experimenting with encoding grids with 16-bit integer values, with an
offset and scale, instead of using IEEE 32-bit floats. There's some connection
with my previous thread about accuracies of NTv2...

Let's for example take the example of egm08_25.gtx. It is 149.3 MB large
If converting it into a IEEE 32-bit float deflate-compressed (with floating-
point predictor) tiled geotiff, it becomes 80.5 MB large (the compression
method and floatin-point predictor are fully lossless. There's bit-to-bit
equivalence for the elevation values regarding the original gtx file)
Now observing that the range of values is in [-107,86], I can remap that to
[0,65535] with an offset of -107 and scale of (86 - -107) / 65535 ~= 0.0029
The resulting deflate-compressed (with integer predictor) tiled GeoTIFF is now
23.1 MB large !

Looking at the difference between the unscaled quantized values and the
original ones, the error is in the [-1.5 mm, 1.5 mm] range (which is expected,
being the half of the scale value), with a mean value of 4.5e-6 metre (so
centered), and a standard deviation of 0.85 mm

After that experimentation, I found this interesting page of the GeographicLib
documentation
https://geographiclib.sourceforge.io/html/geoid.html
which compares the errors introduced by the gridding itself and interpolation
methods (the EGM model is originally a continuous model), with/without
quantization. And one conclusion is "If, instead, Geoid were to use data files
without such quantization artifacts, the overall error would be reduced but
only modestly". Actually with the bilinear interpolation we use, the max and
RMS errors with and without quantization are the same... So it seems perfectly
valid to use such quantized products, at least for EGM2008, right ?

Now looking at horizontal grids, let's consider Australia's
GDA94_GDA2020_conformal.gsb. It is 83 MB large (half of this size due to the
error channels, which are set to a dummy value of -1...)
Converting it to a compressed tiled Float32 tif (without those useless error
channels), make it down to 4.5 MB.
And as a quantitized uint16 compressed tif, down to 1.4 MB (yes, almost 60
times smaller than original .gsb file). The maximum scale factor is 1.5e-7
arcsecond, hence a maximum error of 2.3 micrometre... I'm pretty sure we're
several order of magnitudes beyond the accuracy of the original model, right ?
In EPSG this transformation is reported to have an accuracy of 5cm.
The fact that we get such a small scale factor is due to GDA94 -> GDA2020
conformal being mostly a uniform shift of ~1.8 m and that the grids is
mentioned to "Gives identical results to Helmert transformation GDA94 to
GDA2020 (1)"

If we look at the France' ntf_r93.gsb, which has shifts of an amplitude up to
130m, the maximum error introduced by the quantization is 0.6 mm. I would tend
to think this is also acceptable (given the size of that particular file is
small, compression gains are quite neglectable, but this is mostly to look if
we can generalize such mechanism). What puzzles me is that in
https://geodesie.ign.fr/contenu/fichiers/documentation/algorithmes/notice/
NT111_V1_HARMEL_TransfoNTF-RGF93_FormatGrilleNTV2.pdf where they compare the
NTv2 approach regarding their native 3D geocentric correction approach, they
underline in red a sample point where the difference between the 2 models is
1.2 mm, as if it had really some importance. For that test point, using the
quantized approach would increase this difference to 1.3 mm. But when looking
at the accuracy reported in the grid at that point it is 1.6e-3 arc-second
(which is the minimum value for the latitude error of the product, by the
way), ie 5cm, so it seems to me that discussing about millimetric error
doesn't make sense.
In EPSG this transformation is reported to have an accuracy of 1 metre (which
is consistent with the mean value of the latitude shift error)

Now, let's look at the freshly introduced BWTA2017.gsb file. 392 MB large
As a Float32 compressed geotiff: 73 MB (5.4x compression rate)
As a Int16 compressed geotiff: 26 MB (15x compression rate)
Maximum error added by quantization for latitude shift: 0.25 mm
Minimum error value advertized for latitude shift: 1.61e-5 arc-second (not
completely sure about the units...), ie 0.5mm
Mean error value advertized for latitude shift: 6.33e-5 arc-second, ie 1.9mm
Interestingly when looking at the ASCII version of the grid, the values of the
shifts are given with a precision of 1e-6 arcseconds, that is 0.03 mm !

For Canadian NTv2_0.gsb, on the first subgrid, the quantization error is
0.9 mm for the latitude shift. The advertized error for latitude is in [0,
13.35 m] (the 0.000 value is really surprising. reached on a couple points
only), with a mean at 0.27 m and stddev at 0.48 m. In EPSG this transformation
is reported to have an accuracy of 1.5 metre

~~~~~~~~~~~~~

So, TLDR, is it safe (and worth) to generate quantized products to reducte by
about a factor of 2 to 3 the size of our grids compared to unquantized
products, when the maximum error added by the quantization is ~ 1mm or less ?
or will data producers consider we damage the quality of products they
carefully crafted ? do some users need millimetric / sub-millimetric accuracy
?

Or do we need to condition quantization to a criterion or a combination of
criteria like:
- a maximum absolute error that quantization introduces (1 mm ? 0.1 mm ?)
- a maximum value for the ratio between the maximum absolute error that
quantization introduces over the minimum error value advertized (when known)
below some value ? For the BWTA2017 product, this ratio is 0.5. For
ntf_r93.gsb, 0.012. For NTv2_0.gsb, cannot be computed given sothe min error
value advertized is 0...
- or, variant of the above, a maximum value for the ratio between the maximum
absolute that quantization introduces  over the mean of the error value
advertized (when known). For the BWTA2017 product, this ratio is 0.13. For
ntf_r93.gsb, 5.5e-4. For NTv2_0.gsb, 3.3e-3
- and perhaps consider that only for products above a given size (still larger
than 10 MB after lossless compression ?)

~~~~~~~~~~~~~~~~

Jochem mentionned in the previous thread that the Netherlands grids have an
accuracy of 1mm. I'm really intrigued by what that means. Does that mean that
the position of control points used to build the grid is known at that
accuracy, both in the source and target systems, and that when using bilinear
interpolation with the grid, one remains within that accuracy ? Actually both
the rdtrans2008.gsb and rdtrans2018.gsb grids report an accuracy of 1mm, but
when comparing the positions corrected by those 2 grids, I get differences
above 1mm.

echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2008.gsb
   5.999476046    52.998912757

echo "6 53 0" | cct -d 9 +proj=hgridshift +grids=./rdtrans2018.gsb
   5.999476020    52.998912753

echo "52.998912757 5.999476046 52.998912753 5.999476020" | geod -I
+ellps=GRS80
-104d18'21.49"  75d41'38.51"    0.002

That's a difference of 2 mm. I get that difference on a few other "random"
points.

If applying the quantization on rdtrans2018.gsb, we'd add an additional
maximal error of 0.6 mm. The grid being 1.5 MB uncompressed, and 284 KB as a
losslessly compressed TIFF, quantization isn't really worth considering
(reduces the file size to 78 KB)

Even

--
Spatialys - Geospatial professional services
http://www.spatialys.com
_______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org<mailto:PROJ at lists.osgeo.org>
https://lists.osgeo.org/mailman/listinfo/proj


Disclaimer:
De inhoud van dit bericht is uitsluitend bestemd voor geadresseerde.
Gebruik van de inhoud van dit bericht door anderen zonder toestemming van het Kadaster 
is onrechtmatig. Mocht dit bericht ten onrechte bij u terecht komen, dan verzoeken wij u 
dit direct te melden aan de verzender en het bericht te vernietigen. 
Aan de inhoud van dit bericht kunnen geen rechten worden ontleend.

Disclaimer:
The content of this message is meant to be received by the addressee only.
Use of the content of this message by anyone other than the addressee without the consent 
of the Kadaster is unlawful. If you have received this message, but are not the addressee, 
please contact the sender immediately and destroy the message.
No rights can be derived from the content of this message
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191208/5dbd97d0/attachment-0001.html>

From even.rouault at spatialys.com  Sun Dec  8 11:30:36 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sun, 08 Dec 2019 20:30:36 +0100
Subject: [PROJ] Units of NTv2 accuracy/error channels ?
In-Reply-To: <19221812.iKQVBK8pbO@even-i700>
References: <19221812.iKQVBK8pbO@even-i700>
Message-ID: <7200975.s99gsqr1c6@even-i700>

I've initiated in PROJ wiki a page with the NTv2 products currently in 
the proj-datumgrid repository and which have accuracy channels with 
significant values:

https://github.com/OSGeo/PROJ/wiki/Units-of-NTv2-accuracy-samples-%3F

Please help filling it if you're from, or have contacts with, LGL, LINZ, 
Ordnance survey, IGN Belgium, Swisstopo, DGTerritorio or ICSM....

Even

PS: For clarity, I've omitted all the NRCan NTv2 grids other than ntv2_0.gsb, 
as I assume that NRCan consistently uses metres, and similarly just kept one 
product for Swisstopo and DGTerritorio as they seem to be consistent too.

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From kreve at sdfe.dk  Mon Dec  9 00:27:53 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Mon, 9 Dec 2019 08:27:53 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF9323148EAE1@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700> <c169afd342b84f74b87094109c4c3631@sdfe.dk>
 <A87E66F06E86F14B857F2EB047CDF9323148EAE1@prdassexch01.ad.linz.govt.nz>
Message-ID: <dc4edc84717b42c9b7d6f028d11c7a19@sdfe.dk>

> I wonder if it is easiest to do the inversion in the lat/lon space (even a simple
> approximation of metre to degrees should get  a fairly accurate result).  While an
> imperfect inverse is not ideal, it is nothing compared to the imperfections in the
> deformation model in the first place.  Ultimately a trade off between perfect and
> efficient.  Maybe this could be a user definable option somehow if it is a concern?

I don't think it makes much differene whether you work in lat/lon or geocentric space.
The main problem is retrofitting an extra dimension to an algorithm that was made
to work in 2D. I am sure the problem has been solved previously, I just have not had
the time to search for it in the algorithmic literature.

> I haven't raised the implementation approach in PROJ in this proposal (yet!).
> I'm wondering if it would be better implemented as a +proj=(deformation is taken,
> something like that) method, rather than breaking up into multiple steps in
> in the pipeline as I think you are suggesting.

That is a possibility, yes. From a PROJ stand-point I could see this becoming some sort
of meta-operation that internally would expand to a pipeline. Other applications would
probably implement it differently.

> Reasons are:
> 1) Inversion should be of the deformation as a whole, not individual steps

We can deal with inversion of a pipeline. This is already being done in some complicated
situations. As long as the description of how to combine the various grids in a transformation
is good we should be able to construct a pipeline that goes both ways. 

> 2) The pipeline would be different for different epochs.   Generally I imagine that data
> will be of a consistent epoch but I don't think that would necessarily be true

We should be able to deal with that in a pipeline, provided that info on the epochs is
available in the metadata. As I mentioned in a previous email we still need a bit of work
regarding time-depending transformations. This would be a nice opportunity to change
that.

> 3) I suspect it would be slower to run

Marginally, yes. When Thomas implemented the pipeline operator he and I did a bunch
Of performance tests and generally the pipeline operator itself is quite efficient. As a
general rule of thumb premature optimization should be avoided - using existing operations
in PROJ should make the initial implementation a lot smoother.

> Thanks for these  - very interesting.  Is it Ok if I include these in the document
> (attributed to you).  I'm thinking I'll add an appendix summarising the discussion
> with Even around the NZ usage, and it would be great to add these too.

Yes, of course. 

/Kristian

-----Original Message-----
From: Chris Crook <ccrook at linz.govt.nz> 
Sent: 6. december 2019 20:20
To: Kristian Evers <kreve at sdfe.dk>; 'PROJ (proj at lists.osgeo.org)' <proj at lists.osgeo.org>
Subject: RE: [PROJ] Proposal for a geodetic deformation model format

Kristian

> I am late to the party so let me just try to comment on a few things

I think the party has hardly started :-)

> COORDINATE SPACE(S)
>
> I originally wrote the deformation operation with the sole purpose of
> providing a practical implementation of the transformations described in the
> paper by Häkli et al [0]. This obviously influenced the implementation and
> there is the possibility that it isn't generic enough for all purposes, although I
> haven't yet come across a situation that wouldn't fit regarding the use 3D
> deformation models in coordinate transformations.
>
> The deformation operation in PROJ works in a mix of geocentric and
> geographic coordinates [1]. Actual adjustments are done in
> geocentric/cartesian space. This is analogous to e.g. a Helmert operation
> which also works in geocentric space. And as Even points out the
> deformation operation is often use in a pipeline in conjunction with one or
> more Helmert operations. I generally prefer to work in geocentric space
> when doing this sort of work since it is the "native" coordinate space of the
> frame and because it is nice to work within a linear coordinate systems with
> axes that intersects at right angles. Unfortunately it is not really possible to
> create a grid of surface displacements or velocities in geocentric space
> without creating a massive sparse 3D matrix which is not practical in any way.
> This is the reason why the deformation operation requires grids in
> topocentric/ENU space and internally converts between the two coordinate
> systems. It does come with a bit of overhead, especially in the inverse mode,
> but it isn't too bad compared to let's say a 14-parameter Helmert operation.

In NZ case we have generally different implementations of horizontal and vertical
deformation, so the ENU system is much more practical.  Also for the most part the
measurement systems and modelling approaches (not to mention tectonic behaviour)
can have quite different behaviours for horizontal and vertical (or indeed only provide
horizontal or vertical deformation).   So at the moment for LINZ the more pragmatic
approach is to work in ENU space for a while longer.  Or more briefly - gravity makes
a difference.

> Doing gridshift adjustments in degrees generally works quite well and it what
> has been done for years and years in PROJ. The main disadvantage, I think, is
> that it is difficult for the human eye to interpret values in the grid. I tend to
> find it quite useful to plot the grids I use in QGIS and having the values
> expressed in meters or millimeters gives me an immediate advantage.
> There's nothing in the way to support both types of gridshifts. I do think
> though that expressing velocities in terms of degrees is a bad idea (I don't
> think that was the intention, just putting it out there).

Thanks.  I also favour using metres rather than degrees.  Having human readable/easily
plottable grids is another plus.  Plus as in the proposal uncertainties in degrees are nonsensical
(especially as I am proposing a single circular horizontal uncertainty) and it feels very unclean
to use different units for displacement and its uncertainty.

> Regarding the iterative inverse deformation operation, it works but it isn't
> particularly good to be honest. I reworked the existing iterative algorithm
> used for horizontal grid shifts into a 3D version and I never quite got it right. It
> doesn't perform well in a round-trip test and after about ten trips back and
> forth it has introduced too much noise due to numerical inaccuracy in the
> algorithm. Improving this could be a fun challenge for  some one more
> mathematically inclined than me.

I wonder if it is easiest to do the inversion in the lat/lon space (even a simple
approximation of metre to degrees should get  a fairly accurate result).  While an
imperfect inverse is not ideal, it is nothing compared to the imperfections in the
deformation model in the first place.  Ultimately a trade off between perfect and
efficient.  Maybe this could be a user definable option somehow if it is a concern?

> I agree that it doesn't make much of a difference whether adjustments in
> the up components are done in relation to a gravimetric or geometric
> reference.

Nice to have a second opinion on this assertion :-)

> GRID DELIVERY METHOD (SINGLE VS. MULTI FILE)
>
> I agree with the proposal that the multi file solution is preferable. Especially if
> zipped up a in aggregate package. This was also touched upon in the original
> GitHub discussion 18 months ago. I think JSON would be preferable to a CSV
> based format since it offers a bit more structure and you would be able to
> define a proper JSON schema that can be used to validate the format of the
> file. PROJ has the ability to parse JSON already.

I'm sold!  I'll amend the document to propose JSON next week.   It is also much more
suitable for web services and so on.

I realised another benefit of the multiple file format is that it provides the possibility
of optimising conversions between different versions of a datum for which most of the
grids are the same.   I'll add that to the document also.

> A package of a number of grid files and a metadata file describing the use of
> the grids should be fairly simple to translate to a PROJ pipeline, provided that
> the usage of the grids align with operations available in PROJ. It would of
> course be much simpler if everything was just expressed as a PROJ pipeline
> but I guess we can't assume that everyone will be using PROJ :-)

I haven't raised the implementation approach in PROJ in this proposal (yet!).
I'm wondering if it would be better implemented as a +proj=(deformation is taken,
something like that) method, rather than breaking up into multiple steps in
in the pipeline as I think you are suggesting.

Reasons are:
1) Inversion should be of the deformation as a whole, not individual steps
2) The pipeline would be different for different epochs.   Generally I imagine that data
will be of a consistent epoch but I don't think that would necessarily be true
3) I suspect it would be slower to run

> RELEVANT USE CASE: GREENLAND
>   ...

Thanks for these  - very interesting.  Is it Ok if I include these in the document
(attributed to you).  I'm thinking I'll add an appendix summarising the discussion
with Even around the NZ usage, and it would be great to add these too.

Regards
Chris


________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From ccrook at linz.govt.nz  Mon Dec  9 12:39:16 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Mon, 9 Dec 2019 20:39:16 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <dc4edc84717b42c9b7d6f028d11c7a19@sdfe.dk>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700> <c169afd342b84f74b87094109c4c3631@sdfe.dk>
 <A87E66F06E86F14B857F2EB047CDF9323148EAE1@prdassexch01.ad.linz.govt.nz>
 <dc4edc84717b42c9b7d6f028d11c7a19@sdfe.dk>
Message-ID: <A87E66F06E86F14B857F2EB047CDF932314923C3@prdassexch01.ad.linz.govt.nz>

Hi Kristian

> > I haven't raised the implementation approach in PROJ in this proposal
> (yet!).
> > I'm wondering if it would be better implemented as a
> > +proj=(deformation is taken, something like that) method, rather than
> > breaking up into multiple steps in in the pipeline as I think you are
> suggesting.
>
> That is a possibility, yes. From a PROJ stand-point I could see this becoming
> some sort of meta-operation that internally would expand to a pipeline.
> Other applications would probably implement it differently.

That sounds an ideal approach to me.  Definitely I would want a +defmodel=xxxx
command line option to allow testing, otherwise I have no way confidently building
the JSON master file.   But internally the using a pipeline sounds great as it
it is does possibly provide options for optimising conversion between different
versions of the deformation model etc.

> > Reasons are:
> > 1) Inversion should be of the deformation as a whole, not individual
> > steps
>
> We can deal with inversion of a pipeline. This is already being done in some
> complicated situations. As long as the description of how to combine the
> various grids in a transformation is good we should be able to construct a
> pipeline that goes both ways.

In retrospect I'm not sure that it needs to be for the deformation model
as a whole in any case.  I'm realising there are mathematical issues whichever
way it is done.  I'm going to document this in the proposal in more detail.

> > 2) The pipeline would be different for different epochs.   Generally I
> imagine that data
> > will be of a consistent epoch but I don't think that would necessarily
> > be true
>
> We should be able to deal with that in a pipeline, provided that info on the
> epochs is available in the metadata. As I mentioned in a previous email we
> still need a bit of work regarding time-depending transformations. This would
> be a nice opportunity to change that.

Yes - so long as the pipeline supports a piecewise linear  scale factor function
of time then it can be included.  The implementation will presumably evaluate
the time function and then ignore the corresponding grid if it evaluations to
zero.

> > 3) I suspect it would be slower to run
>
> Marginally, yes. When Thomas implemented the pipeline operator he and I
> did a bunch Of performance tests and generally the pipeline operator itself is
> quite efficient. As a general rule of thumb premature optimization should be
> avoided - using existing operations in PROJ should make the initial
> implementation a lot smoother.

I think you are right - it probably wouldn't make a lot of difference.

On premature optimisation, it should be avoided if it adds unnecessary complexity
 to the code.  That doesn't mean we should ignore performance in implementing algorithms though!

>
> > Thanks for these  - very interesting.  Is it Ok if I include these in
> > the document (attributed to you).  I'm thinking I'll add an appendix
> > summarising the discussion with Even around the NZ usage, and it would
> be great to add these too.
>
> Yes, of course.

Thanks
Chris

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From even.rouault at spatialys.com  Mon Dec  9 13:33:23 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 09 Dec 2019 22:33:23 +0100
Subject: [PROJ] NTv1/NTv2/CTable2 and west-oriented longitude shifts
Message-ID: <2605079.QiNUBcvgQ7@even-i700>

Hi,

Another messiness of NTv2 I (re)discover. As NTv2 was designed for Canada, it 
uses a "west longitude is positive" convention.
That applies to the extent of the grid, the order of the columns... and the 
longitude shift values.
NTv1 also uses that convention.

Our venerable CTable2 format uses a more classic convention of negative values 
for west longitudes and use a west-to-east column ordering, but has also 
inherited that annoying characteristics that the delta longitudes are postive 
to the west... So this affects the +proj=deformation . Kristian, are you aware 
of that ? But as you hijack CTable2 to store values in mm/yr, you probably 
compensated that issue with the negative sign.

So what about GeoTIFF storage for horizontal grids ? Do we want to perpetuate 
that tradition of positive longitude shift meaning correction to the west ? Or 
use the "natural" convention: positive longitude offset values are corrections 
to the east ?
It seems that at the very least I should add a new metadata item: 
positive_value=west/east to document the convention.

Perhaps for grids converted from NTv2, I will keep the original value and set
positive_value=west so that it is obvious we didn't alter the values.
People wanting to use the "natural" convention will be strongly encouraged to 
use positive_value=east (which will be the default)

Thoughts ?

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From schwehr at gmail.com  Mon Dec  9 14:10:40 2019
From: schwehr at gmail.com (Kurt Schwehr)
Date: Mon, 9 Dec 2019 14:10:40 -0800
Subject: [PROJ] NTv1/NTv2/CTable2 and west-oriented longitude shifts
In-Reply-To: <2605079.QiNUBcvgQ7@even-i700>
References: <2605079.QiNUBcvgQ7@even-i700>
Message-ID: <CACmBxyvnv95LiTcLFY-DgiJ1RHh0rFxWgeaC7spaYMjtrdjtdg@mail.gmail.com>

Why keep the old convention for GeoTIFFs?  I believe the sooner things
become more uniform, the better.

On Mon, Dec 9, 2019 at 1:33 PM Even Rouault <even.rouault at spatialys.com>
wrote:

> Hi,
>
> Another messiness of NTv2 I (re)discover. As NTv2 was designed for Canada,
> it
> uses a "west longitude is positive" convention.
> That applies to the extent of the grid, the order of the columns... and
> the
> longitude shift values.
> NTv1 also uses that convention.
>
> Our venerable CTable2 format uses a more classic convention of negative
> values
> for west longitudes and use a west-to-east column ordering, but has also
> inherited that annoying characteristics that the delta longitudes are
> postive
> to the west... So this affects the +proj=deformation . Kristian, are you
> aware
> of that ? But as you hijack CTable2 to store values in mm/yr, you probably
> compensated that issue with the negative sign.
>
> So what about GeoTIFF storage for horizontal grids ? Do we want to
> perpetuate
> that tradition of positive longitude shift meaning correction to the west
> ? Or
> use the "natural" convention: positive longitude offset values are
> corrections
> to the east ?
> It seems that at the very least I should add a new metadata item:
> positive_value=west/east to document the convention.
>
> Perhaps for grids converted from NTv2, I will keep the original value and
> set
> positive_value=west so that it is obvious we didn't alter the values.
> People wanting to use the "natural" convention will be strongly encouraged
> to
> use positive_value=east (which will be the default)
>
> Thoughts ?
>
> Even
>
> --
> Spatialys - Geospatial professional services
> http://www.spatialys.com
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191209/4891fbc7/attachment.html>

From even.rouault at spatialys.com  Mon Dec  9 14:18:12 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 09 Dec 2019 23:18:12 +0100
Subject: [PROJ] NTv1/NTv2/CTable2 and west-oriented longitude shifts
In-Reply-To: <CACmBxyvnv95LiTcLFY-DgiJ1RHh0rFxWgeaC7spaYMjtrdjtdg@mail.gmail.com>
References: <2605079.QiNUBcvgQ7@even-i700>
 <CACmBxyvnv95LiTcLFY-DgiJ1RHh0rFxWgeaC7spaYMjtrdjtdg@mail.gmail.com>
Message-ID: <8958687.KLfvp9ya6n@even-i700>

On lundi 9 décembre 2019 14:10:40 CET Kurt Schwehr wrote:
> Why keep the old convention for GeoTIFFs?  I believe the sooner things
> become more uniform, the better.

You're right. I will use the positive_value=east for transcoding from NTv2, so 
that's its clear that we changed the sign.

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Jochem.Lesparre at kadaster.nl  Tue Dec 10 00:58:49 2019
From: Jochem.Lesparre at kadaster.nl (Lesparre, Jochem)
Date: Tue, 10 Dec 2019 08:58:49 +0000
Subject: [PROJ] NTv1/NTv2/CTable2 and west-oriented longitude shifts
In-Reply-To: <8958687.KLfvp9ya6n@even-i700>
References: <2605079.QiNUBcvgQ7@even-i700>
 <CACmBxyvnv95LiTcLFY-DgiJ1RHh0rFxWgeaC7spaYMjtrdjtdg@mail.gmail.com>
 <8958687.KLfvp9ya6n@even-i700>
Message-ID: <VI1P192MB0016B7C43727206EC00D6375EF5B0@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>

Hi Even,

I agree. Please, abandon the strange conventions of NTv2 and use east positive, normal column ordering and preferably also decimal degrees instead of arcseconds.

I would prefer very accurate (0.1 mm) grid shift files and an encoding that makes it simple to write and read it. To me this is more important than the best possible data compression. So if possible, use signed integers with no offset (0 is no correction) and 1*10^-9 degree as (default) unit.

By the way, GeoTIFFs also have a map projection. I assume you are planning to use latlon thus platte-carree projection. It would be interesting to also allow other projections (with 1*10^-4 metre as (default) unit). Although this is against ISO, it would make it possible for countries that use horizontal grid shifts on projected coordinates to implement their transformation in PROJ., so they don't have to redefine their national coordinate system like the Netherlands did to accomplish this.

Regards, Jochem



Disclaimer:
De inhoud van dit bericht is uitsluitend bestemd voor geadresseerde.
Gebruik van de inhoud van dit bericht door anderen zonder toestemming van het Kadaster 
is onrechtmatig. Mocht dit bericht ten onrechte bij u terecht komen, dan verzoeken wij u 
dit direct te melden aan de verzender en het bericht te vernietigen. 
Aan de inhoud van dit bericht kunnen geen rechten worden ontleend.

Disclaimer:
The content of this message is meant to be received by the addressee only.
Use of the content of this message by anyone other than the addressee without the consent 
of the Kadaster is unlawful. If you have received this message, but are not the addressee, 
please contact the sender immediately and destroy the message.
No rights can be derived from the content of this message

From even.rouault at spatialys.com  Tue Dec 10 04:22:45 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Tue, 10 Dec 2019 13:22:45 +0100
Subject: [PROJ] NTv1/NTv2/CTable2 and west-oriented longitude shifts
In-Reply-To: <VI1P192MB0016B7C43727206EC00D6375EF5B0@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>
References: <2605079.QiNUBcvgQ7@even-i700> <8958687.KLfvp9ya6n@even-i700>
 <VI1P192MB0016B7C43727206EC00D6375EF5B0@VI1P192MB0016.EURP192.PROD.OUTLOOK.COM>
Message-ID: <2100258.xdAmLBcP7e@even-i700>

Jochem,

> I agree. Please, abandon the strange conventions of NTv2 and use east
> positive, normal column ordering and preferably also decimal degrees
> instead of arcseconds.

For the NTv2 to GeoTIFF conversion script, I'll keep arcseconds, so that we 
can "trivially" check the strict binary equivalence. But degree/radian/arc-
second will all be possible.
 
> I would prefer very accurate (0.1 mm) grid shift files and an encoding that
> makes it simple to write and read it. To me this is more important than the
> best possible data compression. So if possible, use signed integers with no
> offset (0 is no correction) and 1*10^-9 degree as (default) unit.

For the NTv2 to GeoTIFF conversion script, I'll keep IEEE-Float32. As you see 
value for the encoding you suggest, I will also add support for reading 
signed/unsigned Int32 with offset/scale.
 
> By the way, GeoTIFFs also have a map projection. I assume you are planning
> to use latlon thus platte-carree projection. It would be interesting to
> also allow other projections (with 1*10^-4 metre as (default) unit).
> Although this is against ISO, it would make it possible for countries that
> use horizontal grid shifts on projected coordinates to implement their
> transformation in PROJ., so they don't have to redefine their national
> coordinate system like the Netherlands did to accomplish this.

The GeoTIFF encoding by itself would indeed allow to use CRS other than 
geographic one, but this will not be in the scope of my RFC-4 work. Further 
work in PROJ will be necessary.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From t.j.dijkema at gmail.com  Thu Dec 12 04:31:37 2019
From: t.j.dijkema at gmail.com (Tammo Jan Dijkema)
Date: Thu, 12 Dec 2019 13:31:37 +0100
Subject: [PROJ] Affine transformation in proj < 6?
Message-ID: <C41921B7-1070-49DA-89A5-FE551D2E5D3A@gmail.com>

Hi all,

I'm new to proj, so sorry if this is a trivial question (I've read through docs for an hour without finding an answer, so it can't be too trivial).

I have a custom coordinate system, which is defined through an affine transformation from ETRS89. All coordinates I care about are in xyz Cartesian coordinates. In particular, to get pqr coordinates, I do

coords_pqr = (coords_etrs - pqrorigin_etrs) * (3x3 matrix)

Now I'm generating raster images in PQR, which I would like to store in GeoTiff, so if I understand correctly I need to write a proj string into the GeoTiff to describe the projection. More background: I'm writing the PQR-images in python, and want to export them so that I can load them into QGIS, with appropriate coordinates.

I see that an affine transformation is part of proj6, but I cannot expect users to have that installed. Is there a way to abuse another projection to achieve the same thing?

If there is another channel to ask this question, please let me know. Thanks in advance!

Tammo Jan Dijkema

From kreve at sdfe.dk  Thu Dec 12 06:23:52 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Thu, 12 Dec 2019 14:23:52 +0000
Subject: [PROJ] Graduation day
Message-ID: <0cf1506807644ceea5f139426a163d8d@sdfe.dk>

All,

I am happy to announce that as of a few weeks ago PROJ is now recognized as a full OSGeo project.
The project has been through the incubation process established by OSGeo and has proven that it
lives up to the high standards of project organization that is expected of OSGeo projects. The OSGeo
board approved the graduation the 25th of November [0].

OSGeo graduation is a big milestone for the project. While graduation is nice in itself as recognition
of the PROJ community, it also comes with access to other OSGeo benefits, such as an annual budget,
marketing and support from the system administration group. We have already benefitted immensely
from the latter for years. Of cause all this OSGeo goodness comes with a responsibility, so from now
on PROJ will have to report to the OSGeo board on a regular basis. For now I will serve as the
"project officer".

Thanks to Michael Smith and Jody Garnet for guiding PROJ through the incubation process. Also
thanks to everyone from the PROJ community who has pitched in along the way.

Best regards,
Kristian


[0] https://wiki.osgeo.org/wiki/Board_Meeting_2019-11-25
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191212/1ac9d69d/attachment.html>

From schwehr at gmail.com  Thu Dec 12 06:30:08 2019
From: schwehr at gmail.com (Kurt Schwehr)
Date: Thu, 12 Dec 2019 06:30:08 -0800
Subject: [PROJ] Graduation day
In-Reply-To: <0cf1506807644ceea5f139426a163d8d@sdfe.dk>
References: <0cf1506807644ceea5f139426a163d8d@sdfe.dk>
Message-ID: <CACmBxyunV9LTwOnUh-6NhpBm3Ec=ZFKCdbTTV0Q4f08QxZmvkw@mail.gmail.com>

Woohoo!!!

On Thu, Dec 12, 2019, 6:24 AM Kristian Evers <kreve at sdfe.dk> wrote:

> All,
>
>
>
> I am happy to announce that as of a few weeks ago PROJ is now recognized
> as a full OSGeo project.
>
> The project has been through the incubation process established by OSGeo
> and has proven that it
>
> lives up to the high standards of project organization that is expected of
> OSGeo projects. The OSGeo
>
> board approved the graduation the 25th of November [0].
>
>
>
> OSGeo graduation is a big milestone for the project. While graduation is
> nice in itself as recognition
>
> of the PROJ community, it also comes with access to other OSGeo benefits,
> such as an annual budget,
>
> marketing and support from the system administration group. We have
> already benefitted immensely
>
> from the latter for years. Of cause all this OSGeo goodness comes with a
> responsibility, so from now
>
> on PROJ will have to report to the OSGeo board on a regular basis. For now
> I will serve as the
>
> “project officer”.
>
>
>
> Thanks to Michael Smith and Jody Garnet for guiding PROJ through the
> incubation process. Also
>
> thanks to everyone from the PROJ community who has pitched in along the
> way.
>
>
>
> Best regards,
>
> Kristian
>
>
>
>
>
> [0] https://wiki.osgeo.org/wiki/Board_Meeting_2019-11-25
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191212/1463820a/attachment.html>

From knudsen.thomas at gmail.com  Thu Dec 12 07:57:26 2019
From: knudsen.thomas at gmail.com (Thomas Knudsen)
Date: Thu, 12 Dec 2019 16:57:26 +0100
Subject: [PROJ] Affine transformation in proj < 6?
In-Reply-To: <C41921B7-1070-49DA-89A5-FE551D2E5D3A@gmail.com>
References: <C41921B7-1070-49DA-89A5-FE551D2E5D3A@gmail.com>
Message-ID: <CAH0YoEMi44MJ5m_Wpbzwiw8v62kexU+JEEzETHQYfSmqOUY=XQ@mail.gmail.com>

If the 3x3 matrix describes a pure rotation you could probably use
proj=helmert. If it includes shear, I think you are short of luck

Den tor. 12. dec. 2019 kl. 13.31 skrev Tammo Jan Dijkema <
t.j.dijkema at gmail.com>:

> Hi all,
>
> I'm new to proj, so sorry if this is a trivial question (I've read through
> docs for an hour without finding an answer, so it can't be too trivial).
>
> I have a custom coordinate system, which is defined through an affine
> transformation from ETRS89. All coordinates I care about are in xyz
> Cartesian coordinates. In particular, to get pqr coordinates, I do
>
> coords_pqr = (coords_etrs - pqrorigin_etrs) * (3x3 matrix)
>
> Now I'm generating raster images in PQR, which I would like to store in
> GeoTiff, so if I understand correctly I need to write a proj string into
> the GeoTiff to describe the projection. More background: I'm writing the
> PQR-images in python, and want to export them so that I can load them into
> QGIS, with appropriate coordinates.
>
> I see that an affine transformation is part of proj6, but I cannot expect
> users to have that installed. Is there a way to abuse another projection to
> achieve the same thing?
>
> If there is another channel to ask this question, please let me know.
> Thanks in advance!
>
> Tammo Jan Dijkema
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191212/53997fe7/attachment.html>

From even.rouault at spatialys.com  Thu Dec 12 08:32:57 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Thu, 12 Dec 2019 17:32:57 +0100
Subject: [PROJ] Affine transformation in proj < 6?
In-Reply-To: <C41921B7-1070-49DA-89A5-FE551D2E5D3A@gmail.com>
References: <C41921B7-1070-49DA-89A5-FE551D2E5D3A@gmail.com>
Message-ID: <1627782.ps0Z9AkksQ@even-i700>

On jeudi 12 décembre 2019 13:31:37 CET Tammo Jan Dijkema wrote:
> Hi all,
> 
> I'm new to proj, so sorry if this is a trivial question (I've read through
> docs for an hour without finding an answer, so it can't be too trivial).
> 
> I have a custom coordinate system, which is defined through an affine
> transformation from ETRS89. All coordinates I care about are in xyz
> Cartesian coordinates. In particular, to get pqr coordinates, I do
> 
> coords_pqr = (coords_etrs - pqrorigin_etrs) * (3x3 matrix)
> 
> Now I'm generating raster images in PQR, which I would like to store in
> GeoTiff, so if I understand correctly I need to write a proj string into
> the GeoTiff to describe the projection.

Except that GeoTIFF doesn't store PROJ strings...
Provided that you manage a PROJ string to expression what you want, you could 
(ab)use a GDAL WKT1 string to embed a PROJ string in it, and embed that WKT 
string into GeoTIFF, but the compatibility of this will be really limited.

Example of this:
$ gdal_translate in.tif foo.tif -a_srs "+proj=geos +sweep=x +datum=WGS84 
+h=123456789"

$ listgeo foo.tif

      PCSCitationGeoKey (Ascii,630): "ESRI PE String = 
PROJCS["unknown",GEOGCS["unknown",DATUM["WGS_1984",SPHEROID["WGS 84",
6378137,298.257223563,AUTHORITY["EPSG","7030"]],AUTHORITY["EPSG","6326"]],PRIMEM["Greenwich",
0,AUTHORITY["EPSG","8901"]],UNIT["degree",
0.0174532925199433,AUTHORITY["EPSG","9122"]]],PROJECTION["Geostationary_Satellite"],PARAMETER["central_meridian",
0],PARAMETER["satellite_height",123456789],PARAMETER["false_easting",
0],PARAMETER["false_northing",0],UNIT["metre",
1,AUTHORITY["EPSG","9001"]],AXIS["Easting",EAST],AXIS["Northing",NORTH],EXTENSION["PROJ4","+proj=geos 
+sweep=x +lon_0=0 +h=123456789 +x_0=0 +y_0=0 +datum=WGS84 +units=m 
+no_defs"]]"

What you could do instead of abusing the CRS is to provide a geotransform 
matrix with the affine transformation (assuming it is only in the X,Y 
projected coordinate space)


-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From ccrook at linz.govt.nz  Thu Dec 12 10:24:08 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Thu, 12 Dec 2019 18:24:08 +0000
Subject: [PROJ] Proposal for a geodetic deformation model format
In-Reply-To: <A87E66F06E86F14B857F2EB047CDF932314923BD@prdassexch01.ad.linz.govt.nz>
References: <A87E66F06E86F14B857F2EB047CDF93231488645@prdassexch01.ad.linz.govt.nz>
 <1938599.T2ET77XkLR@even-i700>
 <A87E66F06E86F14B857F2EB047CDF9323148BF58@prdassexch01.ad.linz.govt.nz>
 <1658685.kqhlzQxNDr@even-i700> <c169afd342b84f74b87094109c4c3631@sdfe.dk>
 <A87E66F06E86F14B857F2EB047CDF9323148EAE1@prdassexch01.ad.linz.govt.nz>
 <dc4edc84717b42c9b7d6f028d11c7a19@sdfe.dk>,
 <A87E66F06E86F14B857F2EB047CDF932314923BD@prdassexch01.ad.linz.govt.nz>
Message-ID: <A87E66F06E86F14B857F2EB047CDF93231498125@prdassexch01.ad.linz.govt.nz>

Hi Kristian

I'm putting together the details of of the calculation formulae and implementation
approach for proj into the deformation format proposal, and I find I am revisiting
a couple of issues we have discussed previously,

* adapting the existing deformation code using geocentric coordinates
* using (possibly internally constructed) proj pipline for each component.

> > > I haven't raised the implementation approach in PROJ in this proposal
> > (yet!).
> > > I'm wondering if it would be better implemented as a
> > > +proj=(deformation is taken, something like that) method, rather than
> > > breaking up into multiple steps in in the pipeline as I think you are
> > suggesting.
> >
> > That is a possibility, yes. From a PROJ stand-point I could see this becoming
> > some sort of meta-operation that internally would expand to a pipeline.
> > Other applications would probably implement it differently.
>
> That sounds an ideal approach to me.  Definitely I would want a +defmodel=xxxx
> command line option to allow testing, otherwise I have no way confidently building
> the JSON master file.   But internally the using a pipeline sounds great as it>
> it is does possibly provide options for optimising conversion between different
> versions of the deformation model etc.

I was unsure that it made sense to use the current method of applying deformation
in a geocentric space.  This requires converting from geographic coordinates to
geocentric to apply the deformation (as well as converting the east north up
deformation to geocentric), and then converting back again to get the geographic
coordinates to use in the next step of the pipeline. The conversion back is
an iterative calculation, so this is potentially quite a lot of extra work at each
point.  In my deformation code I've used a simpler approach based on as
local conversion from metres to degrees at each point and use to scale the east/north
changes.

I haven't been able to access the paper you referenced for using the
geocentric method.  I'm guessing the main reason for the geocentric coordinates
is that at high latitudes the approximate approach I am using breaks down, so using geocentric
coordinates avoids dealing with this special case.

However this bears on the use of the pipeline.  If we convert the deformation model
into a pipeline with a separate step for each component, then this overhead
is incurred for each component of the model

In the LINZ deformation model there are 24 components.  Not all will apply at
every time and location, but potentially many could, so this overhead would
become very significant.

There is another potential issue with the pipeline approach, which is that most
of the components only cover a subset of the area covered by the deformation
model.  If we are converting points in the north of NZ then only a couple of grids
will include those points.  So we would have to add some extra logic to the pipeline
to skip a step if the calculation point is outside the defined area,   Otherwise the
pipeline would need to load the grid, only to discover that it didn't include the
calculation point (and so the deformation from the component is zero). To me it feels
like this is subverting a coordinate transformation pipeline to becoming a code language.

These two factors lead me to think that it makes more sense process the deformation
model as an "elemental operation" which takes the input geographic coordinate,
determines the total east, north, and up displacement at that  point, and then
applies that using the conversion to and from geocentric coordinates.

You mentioned conversion of that algorithm from 2d to 3d, but I don't think that
is necessary - the vertical change can just get added to the height.  Doing that in
geocentric coordinates has no value (and probably isn't the theorerically correct
thing to do in any case)

How does that sound to you?

Cheers
Chris

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From t.j.dijkema at gmail.com  Thu Dec 12 13:03:59 2019
From: t.j.dijkema at gmail.com (Tammo Jan Dijkema)
Date: Thu, 12 Dec 2019 22:03:59 +0100
Subject: [PROJ] Affine transformation in proj < 6?
In-Reply-To: <1627782.ps0Z9AkksQ@even-i700>
References: <C41921B7-1070-49DA-89A5-FE551D2E5D3A@gmail.com>
 <1627782.ps0Z9AkksQ@even-i700>
Message-ID: <3F2C2EC2-3AE3-47BF-84C2-80EAF1E25F1E@gmail.com>

Thanks for the replies! I realised later on that the Earth is locally quite flat, so I described in the geotiff only the latitude and longitude of the lower left corner. The metadata doesn't need to know everything about my coordinate system.

So I did the conversion from my coordinate system to lat-long before writing anything to tiff. There is an affine transform stored in geotiff.

I found the python library rasterio, which I use as follows. This does the trick for me:

    urc = pqr_to_latlongheight(urc_pqr) # upper right corner
    llc = pqr_to_latlongheight(llc_pqr) # lower left corner

    transform = Affine.translation(llc[0] - long_res / 2,
                                   llc[1] - lat_res / 2) * \
                  Affine.scale(long_res, lat_res)

    with rasterio.open(filename, "w", driver="GTiff",
                       height=height, width=width,
                       count=1,
                       dtype=image.dtype,
                       crs='+proj=latlong',
                       transform=transform) as gtif:
        gtif.write(image, 1)

Note that there is a proj string in the GeoTiff, but a quite simple one. I wouldn't be surprised if without the proj string, it would work as well (not tested).

The file produced this way loads perfectly into QGIS.

Tammo Jan


> Op 12 dec. 2019, om 17:32 heeft Even Rouault <even.rouault at spatialys.com> het volgende geschreven:
> 
> On jeudi 12 décembre 2019 13:31:37 CET Tammo Jan Dijkema wrote:
>> Hi all,
>> 
>> I'm new to proj, so sorry if this is a trivial question (I've read through
>> docs for an hour without finding an answer, so it can't be too trivial).
>> 
>> I have a custom coordinate system, which is defined through an affine
>> transformation from ETRS89. All coordinates I care about are in xyz
>> Cartesian coordinates. In particular, to get pqr coordinates, I do
>> 
>> coords_pqr = (coords_etrs - pqrorigin_etrs) * (3x3 matrix)
>> 
>> Now I'm generating raster images in PQR, which I would like to store in
>> GeoTiff, so if I understand correctly I need to write a proj string into
>> the GeoTiff to describe the projection.
> 
> Except that GeoTIFF doesn't store PROJ strings...
> Provided that you manage a PROJ string to expression what you want, you could 
> (ab)use a GDAL WKT1 string to embed a PROJ string in it, and embed that WKT 
> string into GeoTIFF, but the compatibility of this will be really limited.
> 
> Example of this:
> $ gdal_translate in.tif foo.tif -a_srs "+proj=geos +sweep=x +datum=WGS84 
> +h=123456789"
> 
> $ listgeo foo.tif
> 
>      PCSCitationGeoKey (Ascii,630): "ESRI PE String = 
> PROJCS["unknown",GEOGCS["unknown",DATUM["WGS_1984",SPHEROID["WGS 84",
> 6378137,298.257223563,AUTHORITY["EPSG","7030"]],AUTHORITY["EPSG","6326"]],PRIMEM["Greenwich",
> 0,AUTHORITY["EPSG","8901"]],UNIT["degree",
> 0.0174532925199433,AUTHORITY["EPSG","9122"]]],PROJECTION["Geostationary_Satellite"],PARAMETER["central_meridian",
> 0],PARAMETER["satellite_height",123456789],PARAMETER["false_easting",
> 0],PARAMETER["false_northing",0],UNIT["metre",
> 1,AUTHORITY["EPSG","9001"]],AXIS["Easting",EAST],AXIS["Northing",NORTH],EXTENSION["PROJ4","+proj=geos 
> +sweep=x +lon_0=0 +h=123456789 +x_0=0 +y_0=0 +datum=WGS84 +units=m 
> +no_defs"]]"
> 
> What you could do instead of abusing the CRS is to provide a geotransform 
> matrix with the affine transformation (assuming it is only in the X,Y 
> projected coordinate space)


From nyall.dawson at gmail.com  Thu Dec 12 15:38:49 2019
From: nyall.dawson at gmail.com (Nyall Dawson)
Date: Fri, 13 Dec 2019 09:38:49 +1000
Subject: [PROJ] Reversing a coordinate operation via proj api
Message-ID: <CAB28AsgxR=obFG=c_2c48V5O=L74UQXBg_4Aay42FP_FC=OQhw@mail.gmail.com>

Hi list,

If I have a proj string representing a coordinate operation, is there
any existing proj c api to reverse this operation?

Nyall

From even.rouault at spatialys.com  Thu Dec 12 15:55:13 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Fri, 13 Dec 2019 00:55:13 +0100
Subject: [PROJ] Reversing a coordinate operation via proj api
In-Reply-To: <CAB28AsgxR=obFG=c_2c48V5O=L74UQXBg_4Aay42FP_FC=OQhw@mail.gmail.com>
References: <CAB28AsgxR=obFG=c_2c48V5O=L74UQXBg_4Aay42FP_FC=OQhw@mail.gmail.com>
Message-ID: <3302018.LyHuapgbYO@even-i700>

Hi Nyall,

> If I have a proj string representing a coordinate operation, is there
> any existing proj c api to reverse this operation?

Do you mean, to get a PROJ string corresponding to the reverse operation ? 
Then, no. Although that would not be too complicated: separate the steps, 
reverse their order, for each step add +inv (or remove +inv if already 
present), and reconcatenate them.
(with PROJ master, you'd need also to change +omit_fwd to +omit_inv and 
+omit_inv to +omit_fwd)

If you just want to apply the reverse operation on coordinates, you can call 
proj_trans()/proj_trans_array()/etc. with direction = PJ_INV instead of PJ_FWD

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From nyall.dawson at gmail.com  Thu Dec 12 16:06:19 2019
From: nyall.dawson at gmail.com (Nyall Dawson)
Date: Fri, 13 Dec 2019 10:06:19 +1000
Subject: [PROJ] Reversing a coordinate operation via proj api
In-Reply-To: <3302018.LyHuapgbYO@even-i700>
References: <CAB28AsgxR=obFG=c_2c48V5O=L74UQXBg_4Aay42FP_FC=OQhw@mail.gmail.com>
 <3302018.LyHuapgbYO@even-i700>
Message-ID: <CAB28AsiJ9kEwLm=kvO+UWuGjDQ3NBTZbR9t-m7vr3b3d9gJADg@mail.gmail.com>

On Fri, 13 Dec 2019 at 09:55, Even Rouault <even.rouault at spatialys.com> wrote:
>
> Hi Nyall,
>
> > If I have a proj string representing a coordinate operation, is there
> > any existing proj c api to reverse this operation?
>
> Do you mean, to get a PROJ string corresponding to the reverse operation ?

Yes. Or even if there was a method to take any given PROJ coordinate
operation object which is already constructed and create a reversed
clone of it.

> Then, no. Although that would not be too complicated: separate the steps,
> reverse their order, for each step add +inv (or remove +inv if already
> present), and reconcatenate them.
> (with PROJ master, you'd need also to change +omit_fwd to +omit_inv and
> +omit_inv to +omit_fwd)
>
> If you just want to apply the reverse operation on coordinates, you can call
> proj_trans()/proj_trans_array()/etc. with direction = PJ_INV instead of PJ_FWD

That's was the fallback option. I'll use that instead of the
potentially fragile proj string parsing/mangling.

Nyall

From even.rouault at spatialys.com  Thu Dec 12 16:15:06 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Fri, 13 Dec 2019 01:15:06 +0100
Subject: [PROJ] Reversing a coordinate operation via proj api
In-Reply-To: <CAB28AsiJ9kEwLm=kvO+UWuGjDQ3NBTZbR9t-m7vr3b3d9gJADg@mail.gmail.com>
References: <CAB28AsgxR=obFG=c_2c48V5O=L74UQXBg_4Aay42FP_FC=OQhw@mail.gmail.com>
 <3302018.LyHuapgbYO@even-i700>
 <CAB28AsiJ9kEwLm=kvO+UWuGjDQ3NBTZbR9t-m7vr3b3d9gJADg@mail.gmail.com>
Message-ID: <7619013.WpUY7Kp1YZ@even-i700>

On vendredi 13 décembre 2019 10:06:19 CET Nyall Dawson wrote:
> On Fri, 13 Dec 2019 at 09:55, Even Rouault <even.rouault at spatialys.com> 
wrote:
> > Hi Nyall,
> > 
> > > If I have a proj string representing a coordinate operation, is there
> > > any existing proj c api to reverse this operation?
> > 
> > Do you mean, to get a PROJ string corresponding to the reverse operation ?
> 
> Yes. Or even if there was a method to take any given PROJ coordinate
> operation object which is already constructed and create a reversed
> clone of it.

Ah, a proj_coordoperation_create_inverse(PJ_CONTEXT*, PJ*) function should be 
easily doable as there's already the C++ counterpart as 
CoordinateOperation::inverse() (for a PROJ string, PROJ has already the logic 
internally to split & reconstruct)

That will not help you with dealing with released versions, but if it can make 
the future better, I can do that.

Even


-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From nyall.dawson at gmail.com  Thu Dec 12 16:45:01 2019
From: nyall.dawson at gmail.com (Nyall Dawson)
Date: Fri, 13 Dec 2019 10:45:01 +1000
Subject: [PROJ] Reversing a coordinate operation via proj api
In-Reply-To: <7619013.WpUY7Kp1YZ@even-i700>
References: <CAB28AsgxR=obFG=c_2c48V5O=L74UQXBg_4Aay42FP_FC=OQhw@mail.gmail.com>
 <3302018.LyHuapgbYO@even-i700>
 <CAB28AsiJ9kEwLm=kvO+UWuGjDQ3NBTZbR9t-m7vr3b3d9gJADg@mail.gmail.com>
 <7619013.WpUY7Kp1YZ@even-i700>
Message-ID: <CAB28AsgOtsBDeGFjFp9fxpLz5NgUvRK9TsLBr7Vz_X4G1oTigg@mail.gmail.com>

On Fri, 13 Dec 2019 at 10:15, Even Rouault <even.rouault at spatialys.com> wrote:
>
> On vendredi 13 décembre 2019 10:06:19 CET Nyall Dawson wrote:
> > On Fri, 13 Dec 2019 at 09:55, Even Rouault <even.rouault at spatialys.com>
> wrote:
> > > Hi Nyall,
> > >
> > > > If I have a proj string representing a coordinate operation, is there
> > > > any existing proj c api to reverse this operation?
> > >
> > > Do you mean, to get a PROJ string corresponding to the reverse operation ?
> >
> > Yes. Or even if there was a method to take any given PROJ coordinate
> > operation object which is already constructed and create a reversed
> > clone of it.
>
> Ah, a proj_coordoperation_create_inverse(PJ_CONTEXT*, PJ*) function should be
> easily doable as there's already the C++ counterpart as
> CoordinateOperation::inverse() (for a PROJ string, PROJ has already the logic
> internally to split & reconstruct)
>
> That will not help you with dealing with released versions, but if it can make
> the future better, I can do that.

It's certainly be handy for debugging, but I'd say it's low-priority
since the workaround is straightforward.

Actually, let me have a shot at implementing this first...

Nyall

>
> Even
>
>
> --
> Spatialys - Geospatial professional services
> http://www.spatialys.com

From peter at cesium.com  Fri Dec 13 04:59:48 2019
From: peter at cesium.com (Peter Gagliardi)
Date: Fri, 13 Dec 2019 07:59:48 -0500
Subject: [PROJ] Defining a WKT CRS with a custom grid shift file
Message-ID: <CAEF9c72FAh6Ji_g_kG=q96JK9vVfXBHoJa5XPQ2NNsWYOZ-vmg@mail.gmail.com>

I am working with custom CRS transformations using cs2cs. For typical EPSG
codes, this works well, but I now want to support custom grid shift files
that are not listed in proj.db. I currently am unsure how to do this with
WKT strings.

I want to describe the following two types of transformations:


   1.

   Transform from horizontal CRS EPSG:XXXX to EPSG:4979 using yyyyy.gsb to
   transform heights
   2.

   Transform from EPSG:4979 to EPSG:XXXX using yyyyy.gsb to transform
   heights


Where yyyyy.gsb is a custom grid shift file. Is there a way to do this
using a COMPOUNDCRS like the following?

COMPOUNDCRS[“User-Defined Projection”,

     <WKT for horizontal CRS EPSG:XXXX>,
    VERTCRS[<whatever parameters are needed to specify yyyyy.gsb>]

]

Or do grid shifts require different syntax entirely? From the WKT2
specifications, it looks like PARAMETERFILE is relevant, but this looks
like it is specified separately from the CRS definition.

Thank you,

Peter Gagliardi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191213/2cef7777/attachment-0001.html>

From even.rouault at spatialys.com  Fri Dec 13 05:17:11 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Fri, 13 Dec 2019 14:17:11 +0100
Subject: [PROJ] Defining a WKT CRS with a custom grid shift file
In-Reply-To: <CAEF9c72FAh6Ji_g_kG=q96JK9vVfXBHoJa5XPQ2NNsWYOZ-vmg@mail.gmail.com>
References: <CAEF9c72FAh6Ji_g_kG=q96JK9vVfXBHoJa5XPQ2NNsWYOZ-vmg@mail.gmail.com>
Message-ID: <98519729.no3MYtf9s2@even-i700>

Peter,

> I am working with custom CRS transformations using cs2cs. For typical EPSG
> codes, this works well, but I now want to support custom grid shift files
> that are not listed in proj.db. I currently am unsure how to do this with
> WKT strings.
> 
> I want to describe the following two types of transformations:
> 
> 
>    1.
> 
>    Transform from horizontal CRS EPSG:XXXX to EPSG:4979 using yyyyy.gsb to
>    transform heights
>    2.
> 
>    Transform from EPSG:4979 to EPSG:XXXX using yyyyy.gsb to transform
>    heights
> 
> 
> Where yyyyy.gsb is a custom grid shift file. Is there a way to do this
> using a COMPOUNDCRS like the following?
> 
> COMPOUNDCRS[“User-Defined Projection”,
> 
>      <WKT for horizontal CRS EPSG:XXXX>,
>     VERTCRS[<whatever parameters are needed to specify yyyyy.gsb>]
> 
> ]

If it is a .gsb file, then it is normally a horizontal shift, not a vertical 
one. You shouldn't use a compoundCRs for that.

You can use a BoundCRS for that. Easy way to bootstrap one is to run

$ projinfo "+proj=longlat +ellps=GRS80 +nadgrids=foo.gsb +type=crs" \
		-o WKT2_2019 -q

BOUNDCRS[
    SOURCECRS[
        GEOGCRS["unknown",
            DATUM["Unknown based on GRS80 ellipsoid",
                ELLIPSOID["GRS 1980",6378137,298.257222101,
                    LENGTHUNIT["metre",1],
                    ID["EPSG",7019]]],
            PRIMEM["Greenwich",0,
                ANGLEUNIT["degree",0.0174532925199433],
                ID["EPSG",8901]],
            CS[ellipsoidal,2],
                AXIS["longitude",east,
                    ORDER[1],
                    ANGLEUNIT["degree",0.0174532925199433,
                        ID["EPSG",9122]]],
                AXIS["latitude",north,
                    ORDER[2],
                    ANGLEUNIT["degree",0.0174532925199433,
                        ID["EPSG",9122]]]]],
    TARGETCRS[
        GEOGCRS["WGS 84",
            DATUM["World Geodetic System 1984",
                ELLIPSOID["WGS 84",6378137,298.257223563,
                    LENGTHUNIT["metre",1]]],
            PRIMEM["Greenwich",0,
                ANGLEUNIT["degree",0.0174532925199433]],
            CS[ellipsoidal,2],
                AXIS["latitude",north,
                    ORDER[1],
                    ANGLEUNIT["degree",0.0174532925199433]],
                AXIS["longitude",east,
                    ORDER[2],
                    ANGLEUNIT["degree",0.0174532925199433]],
            ID["EPSG",4326]]],
    ABRIDGEDTRANSFORMATION["unknown to WGS84",
        METHOD["NTv2",
            ID["EPSG",9615]],
        PARAMETERFILE["Latitude and longitude difference file","foo.gsb",
            ID["EPSG",8656]]]]

But if you just need it for cs2cs purposes, just use the above PROJ.4 style 
string.

Otherwise if it is really a vertical shift (should rather be a .gtx file), use 
"+proj=longlat +ellps=GRS80 +geoidgrids=foo.gtx +type=crs"

Even


-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From nyall.dawson at gmail.com  Sun Dec 15 22:36:52 2019
From: nyall.dawson at gmail.com (Nyall Dawson)
Date: Mon, 16 Dec 2019 16:36:52 +1000
Subject: [PROJ] Serious confusion regarding towgs or not...!
Message-ID: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>

Hi list,

Been struggling through fixing some QGIS proj 6 related issues today,
and I've got to admit - I'm very confused about the role of towgs84 in
the proj 6 world.

My current crop of questions:

1. Is towgs84 effectively deprecated? Should I be avoiding it as much
as possible? Is it inherently evil?

2. When I identify various WKT definitions using projinfo, I get a mix
between direct matches and matches to "BoundCRS of ...". I'm unsure
when this BoundCRS is important or not. Should I be treating a CRS
identified as a BoundCRS of ... as identical to the identified CRS,
and is it safe to just use the identified auth/code combo for that
CRS? Or is that losing important information and I should NEVER treat
a BoundCRS identified match as being an identified result in any
circumstance?

Thanks,
Nyall

From ccrook at linz.govt.nz  Sun Dec 15 23:22:21 2019
From: ccrook at linz.govt.nz (Chris Crook)
Date: Mon, 16 Dec 2019 07:22:21 +0000
Subject: [PROJ] Deformation model format proposal - part 2
Message-ID: <A87E66F06E86F14B857F2EB047CDF9323149BC4D@prdassexch01.ad.linz.govt.nz>

Hi All

Thanks to all who have commented on the proposal for a new format for describing geodetic deformation models for time dependent coordinate transformations.

Based on the feedback I got I have now updated the proposal with some minor changes and a lot more detail about calculations methods.  The most significant change is using a JSON format for the deformation model master file (thanks to Kristian Evers and Jeremy Palmer for recommending this).

I propose to add more information on the New Zealand use case to this, but that will not alter the main content of the proposal around format and usage of the model.

So once again I welcome any further feedback before formally submitting this as an enhancement request to PROJ.

Thanks for any suggestions you have

Best Regards
Chris Crook

________________________________

This message contains information, which may be in confidence and may be subject to legal privilege. If you are not the intended recipient, you must not peruse, use, disseminate, distribute or copy this message. If you have received this message in error, please notify us immediately (Phone 0800 665 463 or info at linz.govt.nz) and destroy the original message. LINZ accepts no responsibility for changes to this email, or for any attachments, after its transmission from LINZ. Thank You.

From even.rouault at spatialys.com  Mon Dec 16 03:24:17 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 16 Dec 2019 12:24:17 +0100
Subject: [PROJ] Serious confusion regarding towgs or not...!
In-Reply-To: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>
References: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>
Message-ID: <1779872.fhfcyu1men@even-i700>

On lundi 16 décembre 2019 16:36:52 CET Nyall Dawson wrote:
> Hi list,
> 
> Been struggling through fixing some QGIS proj 6 related issues today,
> and I've got to admit - I'm very confused about the role of towgs84 in
> the proj 6 world.
> 
> My current crop of questions:
> 
> 1. Is towgs84 effectively deprecated? Should I be avoiding it as much
> as possible? Is it inherently evil?

It is not "evil", but the world of coordinate transformations is full of traps 
and headaches.
It associates a transformation (among several potential ones) to WGS84 to a 
CRS. Once it is set, PROJ will honour it and will not try potentially better 
transformations that may use grids. So "+proj=longlat +ellps=GRS80 
+towgs84=,0,0,0" is a potential way of encoding boths GDA94 and GDA2020, but 
if you use it, then you'll get a null transformation between both.

> 2. When I identify various WKT definitions using projinfo, I get a mix
> between direct matches and matches to "BoundCRS of ...".

I should warn that identification is a divination exercice / best effort / 
fuzzy logic, not something meant to be fully bullet proof.

> I'm unsure
> when this BoundCRS is important or not. Should I be treating a CRS
> identified as a BoundCRS of ... as identical to the identified CRS,
> and is it safe to just use the identified auth/code combo for that
> CRS? Or is that losing important information and I should NEVER treat
> a BoundCRS identified match as being an identified result in any
> circumstance?

I don't think there's an absolute answer. Depends on the context and the 
origin of that WKT. If it is a legacy WKT from GDAL < 3 & PROJ < 6 world, then 
you might just keep the Source CRS of the BoundCRS as the most significant 
part, and consider that the transformation to WGS84 captured in the 
ABRIDGEDTRANSFORMATION was not necessarily intended to be the one used.

I see a number of tickets in PROJ tracker are files about several issues 
related to that. I'm not sure we'll ever manage to find the answer that 
satisfies both practicality and "correctness".

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From peter at cesium.com  Mon Dec 16 07:46:05 2019
From: peter at cesium.com (Peter Gagliardi)
Date: Mon, 16 Dec 2019 10:46:05 -0500
Subject: [PROJ] Defining a WKT CRS with a custom grid shift file
In-Reply-To: <98519729.no3MYtf9s2@even-i700>
References: <CAEF9c72FAh6Ji_g_kG=q96JK9vVfXBHoJa5XPQ2NNsWYOZ-vmg@mail.gmail.com>
 <98519729.no3MYtf9s2@even-i700>
Message-ID: <CAEF9c72w9O3e7=uJSWYvGdQwUf_Gvc1NedCvyDJbqEx0s8tq-A@mail.gmail.com>

Hi Even,

Thank you, your answer clears up a key confusion I had about the types of
grid shift files. It looks like I really meant .gtx wherever I said .gsb.
In my actual use case, I have files in a separate file format (.byn), which
I now assume will need to be converted to .gtx format to use with PROJ.

> You can use a BoundCRS for that.

Ah, I wasn’t aware of BOUNDCRS until now. To check my understanding, a
BoundCRS is one where you input coordinates in SOURCECRS (x, y, z). Then
when PROJ reads this in, it uses the ABRIDGEDTRANSFORMATION to convert to
TARGETCRS (x’, y’, z’) which becomes the effective source CRS. This can
then be converted +to some other CRS as usual. Is this an accurate
description?

> Otherwise if it is really a vertical shift (should rather be a .gtx
file), use
> "+proj=longlat +ellps=GRS80 +geoidgrids=foo.gtx +type=crs"

This command helped point me in the right direction. I experimented using
+geoidgrids with other PROJ strings, and used projinfo to examine the WKT.
In my case, it looks like I will use something along these lines to support
2D EPSG + custom geoid file:

COMPOUNDCRS[“User-defined CRS”,

    <WKT for 2D horizontal CRS>

    BOUNDCRS[

         SOURCECRS[

            VERTCRS["unknown",

                VDATUM["unknown"],

                CS[vertical,1],

                    AXIS["gravity-related height (H)",up,

                        LENGTHUNIT["metre",1,

                            ID["EPSG",9001]]]]],
        TARGETCRS[
            GEOGCRS["WGS 84",

                DATUM["World Geodetic System 1984",

                    ELLIPSOID["WGS 84",6378137,298.257223563,

                        LENGTHUNIT["metre",1]]],

                PRIMEM["Greenwich",0,

                    ANGLEUNIT["degree",0.0174532925199433]],

                CS[ellipsoidal,3],

                    AXIS["latitude",north,

                        ORDER[1],

                        ANGLEUNIT["degree",0.0174532925199433]],

                    AXIS["longitude",east,

                        ORDER[2],

                        ANGLEUNIT["degree",0.0174532925199433]],

                    AXIS["ellipsoidal height",up,

                        ORDER[3],

                        LENGTHUNIT["metre",1]],

                ID["EPSG",4979]]],

   ABRIDGEDTRANSFORMATION["unknown to WGS84 ellipsoidal height",

            METHOD["GravityRelatedHeight to Geographic3D"],

            PARAMETERFILE["Geoid (height correction) model file","<GTX
filename>",

                ID["EPSG",8666]]]]]

I also have a related follow-up question about some of the PROJ parameters.
I now see that +geoidgrids=foo.gtx is for vertical shifts while
+nadgrids=foo.gsb is for horizontal shifts. If this is the case, when would
you use +proj=vgridshift +grids=foo.gtx?

Thank you,

Peter

On Fri, Dec 13, 2019 at 8:17 AM Even Rouault <even.rouault at spatialys.com>
wrote:

> Peter,
>
> > I am working with custom CRS transformations using cs2cs. For typical
> EPSG
> > codes, this works well, but I now want to support custom grid shift files
> > that are not listed in proj.db. I currently am unsure how to do this with
> > WKT strings.
> >
> > I want to describe the following two types of transformations:
> >
> >
> >    1.
> >
> >    Transform from horizontal CRS EPSG:XXXX to EPSG:4979 using yyyyy.gsb
> to
> >    transform heights
> >    2.
> >
> >    Transform from EPSG:4979 to EPSG:XXXX using yyyyy.gsb to transform
> >    heights
> >
> >
> > Where yyyyy.gsb is a custom grid shift file. Is there a way to do this
> > using a COMPOUNDCRS like the following?
> >
> > COMPOUNDCRS[“User-Defined Projection”,
> >
> >      <WKT for horizontal CRS EPSG:XXXX>,
> >     VERTCRS[<whatever parameters are needed to specify yyyyy.gsb>]
> >
> > ]
>
> If it is a .gsb file, then it is normally a horizontal shift, not a
> vertical
> one. You shouldn't use a compoundCRs for that.
>
> You can use a BoundCRS for that. Easy way to bootstrap one is to run
>
> $ projinfo "+proj=longlat +ellps=GRS80 +nadgrids=foo.gsb +type=crs" \
>                 -o WKT2_2019 -q
>
> BOUNDCRS[
>     SOURCECRS[
>         GEOGCRS["unknown",
>             DATUM["Unknown based on GRS80 ellipsoid",
>                 ELLIPSOID["GRS 1980",6378137,298.257222101,
>                     LENGTHUNIT["metre",1],
>                     ID["EPSG",7019]]],
>             PRIMEM["Greenwich",0,
>                 ANGLEUNIT["degree",0.0174532925199433],
>                 ID["EPSG",8901]],
>             CS[ellipsoidal,2],
>                 AXIS["longitude",east,
>                     ORDER[1],
>                     ANGLEUNIT["degree",0.0174532925199433,
>                         ID["EPSG",9122]]],
>                 AXIS["latitude",north,
>                     ORDER[2],
>                     ANGLEUNIT["degree",0.0174532925199433,
>                         ID["EPSG",9122]]]]],
>     TARGETCRS[
>         GEOGCRS["WGS 84",
>             DATUM["World Geodetic System 1984",
>                 ELLIPSOID["WGS 84",6378137,298.257223563,
>                     LENGTHUNIT["metre",1]]],
>             PRIMEM["Greenwich",0,
>                 ANGLEUNIT["degree",0.0174532925199433]],
>             CS[ellipsoidal,2],
>                 AXIS["latitude",north,
>                     ORDER[1],
>                     ANGLEUNIT["degree",0.0174532925199433]],
>                 AXIS["longitude",east,
>                     ORDER[2],
>                     ANGLEUNIT["degree",0.0174532925199433]],
>             ID["EPSG",4326]]],
>     ABRIDGEDTRANSFORMATION["unknown to WGS84",
>         METHOD["NTv2",
>             ID["EPSG",9615]],
>         PARAMETERFILE["Latitude and longitude difference file","foo.gsb",
>             ID["EPSG",8656]]]]
>
> But if you just need it for cs2cs purposes, just use the above PROJ.4
> style
> string.
>
> Otherwise if it is really a vertical shift (should rather be a .gtx file),
> use
> "+proj=longlat +ellps=GRS80 +geoidgrids=foo.gtx +type=crs"
>
> Even
>
>
> --
> Spatialys - Geospatial professional services
> http://www.spatialys.com
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191216/bfbc79bf/attachment-0001.html>

From even.rouault at spatialys.com  Mon Dec 16 08:56:20 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 16 Dec 2019 17:56:20 +0100
Subject: [PROJ] Defining a WKT CRS with a custom grid shift file
In-Reply-To: <CAEF9c72w9O3e7=uJSWYvGdQwUf_Gvc1NedCvyDJbqEx0s8tq-A@mail.gmail.com>
References: <CAEF9c72FAh6Ji_g_kG=q96JK9vVfXBHoJa5XPQ2NNsWYOZ-vmg@mail.gmail.com>
 <98519729.no3MYtf9s2@even-i700>
 <CAEF9c72w9O3e7=uJSWYvGdQwUf_Gvc1NedCvyDJbqEx0s8tq-A@mail.gmail.com>
Message-ID: <9500841.jqJ1QZE5MP@even-i700>

> Ah, I wasn’t aware of BOUNDCRS until now. To check my understanding, a
> BoundCRS is one where you input coordinates in SOURCECRS (x, y, z). Then
> when PROJ reads this in, it uses the ABRIDGEDTRANSFORMATION to convert to
> TARGETCRS (x’, y’, z’) which becomes the effective source CRS. This can
> then be converted +to some other CRS as usual. Is this an accurate
> description?

I'd say that when wanting to transform the (geographic CRS underlying the) 
SOURECRS to TARGETCRS, then ABRIDGEDTRANSFORMATION is used.

> I also have a related follow-up question about some of the PROJ parameters.
> I now see that +geoidgrids=foo.gtx is for vertical shifts while
> +nadgrids=foo.gsb is for horizontal shifts. If this is the case, when would
> you use +proj=vgridshift +grids=foo.gtx?

+geoidgrids is a legacy syntax from the PROJ < 6 era when transformations 
where put in CRS definitions.

+proj=vgridshift +grids=foo.gtx is an operation part of a coordinate 
transformation pipeline to convert between CRS.

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From nyall.dawson at gmail.com  Mon Dec 16 16:19:04 2019
From: nyall.dawson at gmail.com (Nyall Dawson)
Date: Tue, 17 Dec 2019 10:19:04 +1000
Subject: [PROJ] Serious confusion regarding towgs or not...!
In-Reply-To: <1779872.fhfcyu1men@even-i700>
References: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>
 <1779872.fhfcyu1men@even-i700>
Message-ID: <CAB28AsirhkBDuqWEsUcZarkkbf3QJZbQLqAYu=A6YkW59XopKQ@mail.gmail.com>

On Mon, 16 Dec 2019 at 21:24, Even Rouault <even.rouault at spatialys.com> wrote:
>
> On lundi 16 décembre 2019 16:36:52 CET Nyall Dawson wrote:
> > Hi list,
> >
> > Been struggling through fixing some QGIS proj 6 related issues today,
> > and I've got to admit - I'm very confused about the role of towgs84 in
> > the proj 6 world.
> >
> > My current crop of questions:
> >
> > 1. Is towgs84 effectively deprecated? Should I be avoiding it as much
> > as possible? Is it inherently evil?
>
> It is not "evil", but the world of coordinate transformations is full of traps
> and headaches.
> It associates a transformation (among several potential ones) to WGS84 to a
> CRS. Once it is set, PROJ will honour it and will not try potentially better
> transformations that may use grids. So "+proj=longlat +ellps=GRS80
> +towgs84=,0,0,0" is a potential way of encoding boths GDA94 and GDA2020, but
> if you use it, then you'll get a null transformation between both.
>
> > 2. When I identify various WKT definitions using projinfo, I get a mix
> > between direct matches and matches to "BoundCRS of ...".
>
> I should warn that identification is a divination exercice / best effort /
> fuzzy logic, not something meant to be fully bullet proof.
>
> > I'm unsure
> > when this BoundCRS is important or not. Should I be treating a CRS
> > identified as a BoundCRS of ... as identical to the identified CRS,
> > and is it safe to just use the identified auth/code combo for that
> > CRS? Or is that losing important information and I should NEVER treat
> > a BoundCRS identified match as being an identified result in any
> > circumstance?
>
> I don't think there's an absolute answer. Depends on the context and the
> origin of that WKT. If it is a legacy WKT from GDAL < 3 & PROJ < 6 world, then
> you might just keep the Source CRS of the BoundCRS as the most significant
> part, and consider that the transformation to WGS84 captured in the
> ABRIDGEDTRANSFORMATION was not necessarily intended to be the one used.
>
> I see a number of tickets in PROJ tracker are files about several issues
> related to that. I'm not sure we'll ever manage to find the answer that
> satisfies both practicality and "correctness".

Hm. I'm now considering just dropping all code from QGIS which relates
to handling towgs and BoundCRS, and leaving the responsibility
entirely with GDAL and PROJ to insert or ignore TOWGS wherever it's
desirable/wanted.

>From a QGIS perspective I think the biggest downside of doing this
would be that many CRSes would be shown as "custom" user crses,
instead of via an auth/code identifier. But the alternative is that we
end up with a lot of logic relating to boundcrs downstream, and that
makes me nervous. So I'd be willing to live with the potential
mis-identification of known CRSes as custom CRS, and then deal with
each of those as upstream proj/gdal issues on a case-by-case basis.

But I'm keen to hear your thoughts on this approach...

Nyall

From even.rouault at spatialys.com  Tue Dec 17 03:22:11 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Tue, 17 Dec 2019 12:22:11 +0100
Subject: [PROJ] Serious confusion regarding towgs or not...!
In-Reply-To: <CAB28AsirhkBDuqWEsUcZarkkbf3QJZbQLqAYu=A6YkW59XopKQ@mail.gmail.com>
References: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>
 <1779872.fhfcyu1men@even-i700>
 <CAB28AsirhkBDuqWEsUcZarkkbf3QJZbQLqAYu=A6YkW59XopKQ@mail.gmail.com>
Message-ID: <1765623.PiSO7Hq4Nv@even-i700>

> Hm. I'm now considering just dropping all code from QGIS which relates
> to handling towgs and BoundCRS, and leaving the responsibility
> entirely with GDAL and PROJ to insert or ignore TOWGS wherever it's
> desirable/wanted.
> 
> From a QGIS perspective I think the biggest downside of doing this
> would be that many CRSes would be shown as "custom" user crses,
> instead of via an auth/code identifier. But the alternative is that we
> end up with a lot of logic relating to boundcrs downstream, and that
> makes me nervous. 

Ah ah, that you rely entirely on PROJ to do always the "right thing" makes me 
equally nervous :-)
Perhaps QGIS when ingesting a WKT1 string with a TOWGS84[] or a PROJ.4 string 
with a +towgs84 should ask the user: "do you really want to use that precise 
transformation to WGS84 (which will thus make it the pivot CRS for datum 
transformations) or just use the underlying CRS and being proposed with 
several possibilities when transforming to other datums ?" Of course most 
users will have no clue at answering that question... Sometimes users 
implictly relied on the more or less arbitrary +towgs84 values that were 
picked up previously, and they will suffer from misalignment if using other 
transformations. So perhaps there should be a different behaviour if opening 
old QGIS projects than if creating new ones ? Just brainstorming

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From even.rouault at spatialys.com  Tue Dec 17 05:39:17 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Tue, 17 Dec 2019 14:39:17 +0100
Subject: [PROJ] Serious confusion regarding towgs or not...!
In-Reply-To: <1765623.PiSO7Hq4Nv@even-i700>
References: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>
 <CAB28AsirhkBDuqWEsUcZarkkbf3QJZbQLqAYu=A6YkW59XopKQ@mail.gmail.com>
 <1765623.PiSO7Hq4Nv@even-i700>
Message-ID: <1704210.DSjo3xt6yG@even-i700>

> Ah ah, that you rely entirely on PROJ to do always the "right thing" makes
> me equally nervous :-)
> Perhaps QGIS when ingesting a WKT1 string with a TOWGS84[] or a PROJ.4
> string with a +towgs84 should ask the user: "do you really want to use that
> precise transformation to WGS84 (which will thus make it the pivot CRS for
> datum transformations) or just use the underlying CRS and being proposed
> with several possibilities when transforming to other datums ?" Of course
> most users will have no clue at answering that question... Sometimes users
> implictly relied on the more or less arbitrary +towgs84 values that were
> picked up previously, and they will suffer from misalignment if using other
> transformations. So perhaps there should be a different behaviour if
> opening old QGIS projects than if creating new ones ? Just brainstorming

Thining about all the above, regarding GDAL, I've issued the following pull 
request that will change its behaviour regarding importFromEPSG(), to no 
longer try to attach a transformation to WGS84:

https://github.com/OSGeo/gdal/pull/2113


-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From nyall.dawson at gmail.com  Tue Dec 17 19:42:27 2019
From: nyall.dawson at gmail.com (Nyall Dawson)
Date: Wed, 18 Dec 2019 13:42:27 +1000
Subject: [PROJ] Serious confusion regarding towgs or not...!
In-Reply-To: <1704210.DSjo3xt6yG@even-i700>
References: <CAB28AsjLrxAz3wUVOS1Oew78CuWQfUJHH4p=6dKaY4pmMgs9cg@mail.gmail.com>
 <CAB28AsirhkBDuqWEsUcZarkkbf3QJZbQLqAYu=A6YkW59XopKQ@mail.gmail.com>
 <1765623.PiSO7Hq4Nv@even-i700> <1704210.DSjo3xt6yG@even-i700>
Message-ID: <CAB28Asjr+MHOHynmMs=Vh-q9SHwOCkPnVjCXhdiKL1Y-h9Dmig@mail.gmail.com>

On Tue, 17 Dec 2019 at 23:39, Even Rouault <even.rouault at spatialys.com> wrote:
>
> > Ah ah, that you rely entirely on PROJ to do always the "right thing" makes
> > me equally nervous :-)
> > Perhaps QGIS when ingesting a WKT1 string with a TOWGS84[] or a PROJ.4
> > string with a +towgs84 should ask the user: "do you really want to use that
> > precise transformation to WGS84 (which will thus make it the pivot CRS for
> > datum transformations) or just use the underlying CRS and being proposed
> > with several possibilities when transforming to other datums ?" Of course
> > most users will have no clue at answering that question... Sometimes users
> > implictly relied on the more or less arbitrary +towgs84 values that were
> > picked up previously, and they will suffer from misalignment if using other
> > transformations. So perhaps there should be a different behaviour if
> > opening old QGIS projects than if creating new ones ? Just brainstorming
>
> Thining about all the above, regarding GDAL, I've issued the following pull
> request that will change its behaviour regarding importFromEPSG(), to no
> longer try to attach a transformation to WGS84:
>
> https://github.com/OSGeo/gdal/pull/2113

Thanks -- I'm confident this is a good move, and will adapt QGIS to
suit this logic too...

Nyall

From kristianevers at gmail.com  Sat Dec 28 04:20:43 2019
From: kristianevers at gmail.com (Kristian Evers)
Date: Sat, 28 Dec 2019 13:20:43 +0100
Subject: [PROJ] PROJ 6.3.0RC1
Message-ID: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>

All,

With a scheduled release of PROJ 6.3.0. on January 1st 2020 it is time to put out a 
release candidate.

Download the archives here:


https://download.osgeo.org/proj/proj-6.3.0RC1.tar.gz (https://download.osgeo.org/proj/proj-6.3.0RC1.tar.gz.md5
) 
https://download.osgeo.org/proj/proj-6.3.0RC1.zip    (https://download.osgeo.org/proj/proj-6.3.0RC1.zip.md5
)

See the release notes below.


In addition to the PROJ release candidate we are also issuing release candidates
for three regional datumgrid packages:

Oceania-1.1RC1:

http://download.osgeo.org/proj/proj-datumgrid-oceania-1.1RC1.zip

 - Added AUSGeoid98, AUSGeoid09 & AUSGeoid2020
 - Added various grids for New Zealand
 - Added GDA94_GDA2020 grids for Christmas and Coco Island

North-America-1.3RC1:

http://download.osgeo.org/proj/proj-datumgrid-north-america-latest.zip

 - Added Quebec grids na27na83.gsb and cq77na83.gsb
 - Added MAY76V20.gsb for Canadian NAD27(76)->NAD83 transformations
 - Added Canadian geoid models for CGVD28, CGVD2013(CGG2013) and CGVD2013(CGG2013a)
 - Regenerated g2018u0.gtx and g2018p0.gtx files prompted by error in previous versions
 - Added Canadian provincial horizontal shift grids
 - Added USA geoid models 1999, 2003, 2006, 2009 and 2018

  
Europe-1.5RC1:

http://download.osgeo.org/proj/proj-datumgrid-europe-1.5RC1.zip

- Added SeTa2016.gsb for DE_DHDN -> ETRS89 in Saarland
- Added 100800401.gsb for Catalonia ED50 -> ETRS89
- Added NTv2_SN.gsb for RD/83 to ETRS89 in Saxony
- Added AT_GIS_GRID.gsb for Austria MGI->ETRS89
- Added various grids for Iceland, including ISL init file
- Added BWTA2017.gsb for DHDN->ETRS89 for Baden-Württemberg
- Addedvertical grids for Saint-Pierre-et-Miquelon: RASPM2018 and CGVD2013RGSPM06
- Added NTv2 grids for convertion between Portuguese Lisbon Datum and Datum 73 to ETRS89


Please test the release candidates and report back any problems you
may encounter. 

/Kristian

————————————

6.3.0 Release Notes
-------------------

 Updates
 -------

 o Database: tune accuracy of Canadian NTv1 file w.r.t NTv2 (#1812)

 o Modify verbosity level of some debug/trace messages (#1811)

 o projinfo: no longer call createBoundCRSToWGS84IfPossible() for WKT1:GDAL
   (#1810)

 o proj_trans: add retry logic to select other transformation if the best one
   fails. (#1809)

 o BoundCRS::identify(): improvements to discard CRS that aren't relevant
   (#1802)

 o Database: update to IGNF v3.1.0 (#1785)

 o Build: Only export symbols if building DLL (#1773)

 o Database: update ESRI entries with ArcGIS Desktop version 10.8.0 database
   (#1762)

 o createOperations(): chain operations whose middle CRSs are not identical but
   have the same datum (#1734)

 o import/export PROJJSON: support a interpolation_crs key to geoid_model
   (#1732)

 o Database: update to EPSG v9.8.4 (#1725)

 o Build: require SQLite 3.11 (#1721)

 o Add support for GEOIDMODEL (#1710)

 o Better filtering based on extent and performance improvements (#1709)


 Bug fixes
 ---------

 o Horizontal grid shift: fix issue on iterative inverse computation when
   switching between (sub)grids (#1797)

 o createOperations(): make filtering out of 'uninteresting' operations less
   aggressive (#1788)

 o Make EPSG:102100 resolve to ESRI:102100 (#1786)

 o ob_tran: restore traditional handling of +to_meter with pj_transform() and
   proj utility (#1783)

 o CRS identification: use case insensitive comparison for authority name
   (#1780)

 o normalizeForVisualization() and other methods applying on a ProjectedCRS: do
   not mess the derivingConversion object of the original object (#1746)

 o createOperations(): fix transformation computation from/to a CRS with
   +geoidgrids and +vunits != m (#1731)

 o Fix proj_assign_context()/pj_set_ctx() with pipelines and alternative coord
   operations (#1726)

 o Database: add an auxiliary concatenated_operation_step table to allow
   arbitrary number of steps (#1696)

 o Fix errors running gie-based tests in Debug mode on Windows (#1688)


From even.rouault at spatialys.com  Sat Dec 28 05:04:20 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sat, 28 Dec 2019 14:04:20 +0100
Subject: [PROJ] PROJ 6.3.0RC1
In-Reply-To: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
References: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
Message-ID: <2714627.0hoXsUJNMV@even-i700>

> North-America-1.3RC1:
> 
> http://download.osgeo.org/proj/proj-datumgrid-north-america-latest.zip

Should read:
https://download.osgeo.org/proj/proj-datumgrid-north-america-1.3RC1.zip

> - Regenerated g2018u0.gtx and g2018p0.gtx files prompted by error in 
previous versions

This entry could be removed as Geoid2018 was just added in 1.3, so the 
incorrect grids were just a transient state in master.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From kreve at sdfe.dk  Sat Dec 28 05:24:38 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Sat, 28 Dec 2019 13:24:38 +0000
Subject: [PROJ] PROJ 6.3.0RC1
In-Reply-To: <2714627.0hoXsUJNMV@even-i700>
References: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
 <2714627.0hoXsUJNMV@even-i700>
Message-ID: <0D89CAA8-B778-4D2E-9F2A-F6651363C9F9@sdfe.dk>

Thanks Even. I’ll make sure to update those two points for the final release.

/Kristian

> On 28 Dec 2019, at 14:04, Even Rouault <even.rouault at spatialys.com> wrote:
> 
>> North-America-1.3RC1:
>> 
>> http://download.osgeo.org/proj/proj-datumgrid-north-america-latest.zip
> 
> Should read:
> https://download.osgeo.org/proj/proj-datumgrid-north-america-1.3RC1.zip
> 
>> - Regenerated g2018u0.gtx and g2018p0.gtx files prompted by error in 
> previous versions
> 
> This entry could be removed as Geoid2018 was just added in 1.3, so the 
> incorrect grids were just a transient state in master.
> 
> Even
> 
> -- 
> Spatialys - Geospatial professional services
> http://www.spatialys.com
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj


From even.rouault at spatialys.com  Sat Dec 28 05:56:07 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Sat, 28 Dec 2019 14:56:07 +0100
Subject: [PROJ] PROJ 6.3.0RC1
In-Reply-To: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
References: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
Message-ID: <2372761.ZJnVW7J6lZ@even-i700>

> Europe-1.5RC1:
> 
> http://download.osgeo.org/proj/proj-datumgrid-europe-1.5RC1.zip

A RC2 will be needed for that one. BWTA2017.gsb is the git-lfs text file, not 
the actual content.
I committed the .gitattributes file only on Dec 20 whereas the .gsb file was 
committed on Nov 30, so if you have "git pull" in-between, that might perhaps 
explain you only got the link and not the actual content. (Testing a fresh git 
clone with git-lfs in the PATH correctly retrieves the file.)

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From kreve at sdfe.dk  Sat Dec 28 06:30:11 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Sat, 28 Dec 2019 14:30:11 +0000
Subject: [PROJ] PROJ 6.3.0RC1
In-Reply-To: <2372761.ZJnVW7J6lZ@even-i700>
References: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
 <2372761.ZJnVW7J6lZ@even-i700>
Message-ID: <139CCC34-5E7F-4EC6-A171-1B5DE612CAFC@sdfe.dk>

Good catch! I was notified by git that git-lfs was missing from my path.
I fixed that and git pull’ed again to rectify the problem. I should have
done a ‘git lfs pull’ instead. Lesson learned :-)

I’ll update the europe package with a RC2 tonight. Hopefully that is
the only problem in this batch of release candidates.

/Kristian

> On 28 Dec 2019, at 14:56, Even Rouault <even.rouault at spatialys.com> wrote:
> 
>> Europe-1.5RC1:
>> 
>> http://download.osgeo.org/proj/proj-datumgrid-europe-1.5RC1.zip
> 
> A RC2 will be needed for that one. BWTA2017.gsb is the git-lfs text file, not 
> the actual content.
> I committed the .gitattributes file only on Dec 20 whereas the .gsb file was 
> committed on Nov 30, so if you have "git pull" in-between, that might perhaps 
> explain you only got the link and not the actual content. (Testing a fresh git 
> clone with git-lfs in the PATH correctly retrieves the file.)
> 
> -- 
> Spatialys - Geospatial professional services
> http://www.spatialys.com
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj


From a.furieri at lqt.it  Sat Dec 28 07:00:09 2019
From: a.furieri at lqt.it (a.furieri at lqt.it)
Date: Sat, 28 Dec 2019 16:00:09 +0100
Subject: [PROJ] PROJ 6.3.0RC1
In-Reply-To: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
References: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
Message-ID: <62f3b907da164b8b6d504aa357487a58@lqt.it>

On Sat, 28 Dec 2019 13:20:43 +0100, Kristian Evers wrote:
> All,
>
> With a scheduled release of PROJ 6.3.0. on January 1st 2020 it is
> time to put out a release candidate.
>

Hi Kristian,

the release candidate builds without any trouble on Windows MinGW
(both 32 and 64 bit) and Fedora 31.
the whole testcoverage of  SpatiaLite passes smoothly as usual.

bye Sandro

From kreve at sdfe.dk  Sat Dec 28 11:58:52 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Sat, 28 Dec 2019 19:58:52 +0000
Subject: [PROJ] PROJ 6.3.0RC1
In-Reply-To: <139CCC34-5E7F-4EC6-A171-1B5DE612CAFC@sdfe.dk>
References: <C7FE18A9-6AAB-4B9F-BE40-0E0866ED3762@gmail.com>
 <2372761.ZJnVW7J6lZ@even-i700> <139CCC34-5E7F-4EC6-A171-1B5DE612CAFC@sdfe.dk>
Message-ID: <27637933-B9A9-49FB-BAB5-A2D287320B39@sdfe.dk>

I’ve updated the datumgrid-europe package so that the BWTA2017.gsb
grid is properly included. RC2 here:

http://download.osgeo.org/proj/proj-datumgrid-europe-1.5RC2.zip

/Kristian

On 28 Dec 2019, at 15:30, Kristian Evers <kreve at sdfe.dk<mailto:kreve at sdfe.dk>> wrote:

Good catch! I was notified by git that git-lfs was missing from my path.
I fixed that and git pull’ed again to rectify the problem. I should have
done a ‘git lfs pull’ instead. Lesson learned :-)

I’ll update the europe package with a RC2 tonight. Hopefully that is
the only problem in this batch of release candidates.

/Kristian

On 28 Dec 2019, at 14:56, Even Rouault <even.rouault at spatialys.com<mailto:even.rouault at spatialys.com>> wrote:

Europe-1.5RC1:

http://download.osgeo.org/proj/proj-datumgrid-europe-1.5RC1.zip

A RC2 will be needed for that one. BWTA2017.gsb is the git-lfs text file, not
the actual content.
I committed the .gitattributes file only on Dec 20 whereas the .gsb file was
committed on Nov 30, so if you have "git pull" in-between, that might perhaps
explain you only got the link and not the actual content. (Testing a fresh git
clone with git-lfs in the PATH correctly retrieves the file.)

--
Spatialys - Geospatial professional services
http://www.spatialys.com
_______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org
https://lists.osgeo.org/mailman/listinfo/proj

_______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org<mailto:PROJ at lists.osgeo.org>
https://lists.osgeo.org/mailman/listinfo/proj

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191228/29442b02/attachment-0001.html>

From gdt at lexort.com  Sun Dec 29 18:56:40 2019
From: gdt at lexort.com (Greg Troxel)
Date: Sun, 29 Dec 2019 21:56:40 -0500
Subject: [PROJ] test failure with 6.2.1
Message-ID: <rmimuba4j2v.fsf@s1.lexort.com>

I ve been paged out from proj (for non-geo reasons :-( ) and am trying
to catch up.  I saw the RC notice, so my first step was to update
pkgsrc-wip from 6.0.0 to 6.2.1, before trying the RC.  It builds fine,
and tests go ok until:

  ============================================
  Running ../../test/cli/testntv2 using ../../src/cs2cs:
  ============================================
  doing tests into file ntv2_out, please wait
  diff ntv2_out with ntv2_out.dist
  --- ntv2_out    2019-12-29 21:50:04.247219652 -0500
  +++ ../../test/cli/ntv2_out.dist        2019-03-06 11:11:50.000000000 -0500
  @@ -7,5 +7,5 @@
   ##############################################################
   Try with NTv2 and NTv1 together ... falls back to NTv1
   99d00'00.000"W 65d00'00.000"N 0.0      99d0'1.5885"W   65d0'1.3482"N 0.000
  -111d00'00.000"W 46d00'00.000"N 0.0     111d0'3.1897"W  45d59'59.7489"N 0.000
  +111d00'00.000"W 46d00'00.000"N 0.0     111d0'3.1549"W  45d59'59.7528"N 0.000
   111d00'00.000"W 47d30'00.000"N 0.0     111d0'2.7989"W  47d29'59.9896"N 0.000

which seems non-trivial.  I would have thought this was in Canada, but
it seems like just north of Bozeman, Montana.

Are tests of 6.2.1 expected to pass?  With the following grids, or
something else?


This is with the following grids included:
                proj-datumgrid-1.8${EXTRACT_SUFX} \
                proj-datumgrid-europe-1.4${EXTRACT_SUFX} \
                proj-datumgrid-north-america-1.2${EXTRACT_SUFX} \
                proj-datumgrid-oceania-1.0${EXTRACT_SUFX} \
                proj-datumgrid-world-1.0${EXTRACT_SUFX}

From sebastic at xs4all.nl  Sun Dec 29 22:08:16 2019
From: sebastic at xs4all.nl (Sebastiaan Couwenberg)
Date: Mon, 30 Dec 2019 07:08:16 +0100
Subject: [PROJ] test failure with 6.2.1
In-Reply-To: <rmimuba4j2v.fsf@s1.lexort.com>
References: <rmimuba4j2v.fsf@s1.lexort.com>
Message-ID: <20f48cd8-daa9-87c6-60e4-8f63bad4e7f0@xs4all.nl>

On 12/30/19 3:56 AM, Greg Troxel wrote:
> Are tests of 6.2.1 expected to pass?  With the following grids, or
> something else?

The tests are expected to pass with only the grids from proj-datumgrid
available, when also adding grids from the regional archives you should
expect failures like these.

> This is with the following grids included:
>                 proj-datumgrid-1.8${EXTRACT_SUFX} \
>                 proj-datumgrid-europe-1.4${EXTRACT_SUFX} \
>                 proj-datumgrid-north-america-1.2${EXTRACT_SUFX} \
>                 proj-datumgrid-oceania-1.0${EXTRACT_SUFX} \
>                 proj-datumgrid-world-1.0${EXTRACT_SUFX}

Kind Regards,

Bas

-- 
 GPG Key ID: 4096R/6750F10AE88D4AF1
Fingerprint: 8182 DE41 7056 408D 6146  50D1 6750 F10A E88D 4AF1

From kreve at sdfe.dk  Mon Dec 30 02:06:39 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Mon, 30 Dec 2019 10:06:39 +0000
Subject: [PROJ] Motion: Promote 6.3.0 release candidates to final versions
Message-ID: <C466B0CD-B878-4E08-B28E-E19F95CDA8BB@sdfe.dk>

PSC Members,

It appears that no major issues in the release candidates

  PROJ 6.3.0RC1
  proj-datumgrid-europe 1.5RC2
  proj-datumgrid-north-america 1.3RC1
  proj-datumgrid-oceania 1.1RC1

so I hereby motion that they all be promoted to final releases.

I’ll start with my +1.

/Kristian

From even.rouault at spatialys.com  Mon Dec 30 02:14:07 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 30 Dec 2019 11:14:07 +0100
Subject: [PROJ] Motion: Promote 6.3.0 release candidates to final
	versions
In-Reply-To: <C466B0CD-B878-4E08-B28E-E19F95CDA8BB@sdfe.dk>
References: <C466B0CD-B878-4E08-B28E-E19F95CDA8BB@sdfe.dk>
Message-ID: <2490673.ABz3KE6i8B@even-i700>

Hi

+1

PROJ 6.3.0RC1 works fine with GDAL 3/master

Even

> PSC Members,
> 
> It appears that no major issues in the release candidates
> 
>   PROJ 6.3.0RC1
>   proj-datumgrid-europe 1.5RC2
>   proj-datumgrid-north-america 1.3RC1
>   proj-datumgrid-oceania 1.1RC1
> 
> so I hereby motion that they all be promoted to final releases.
> 
> I’ll start with my +1.
> 
> /Kristian
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj


-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From kreve at sdfe.dk  Mon Dec 30 04:00:03 2019
From: kreve at sdfe.dk (Kristian Evers)
Date: Mon, 30 Dec 2019 12:00:03 +0000
Subject: [PROJ] Motion: Promote 6.3.0 release candidates to final
	versions
In-Reply-To: <CAH0YoEOvDXbrztGDnN+TGqhh0bP_ZWYFO__engSiKOqien4xhQ@mail.gmail.com>
References: <C466B0CD-B878-4E08-B28E-E19F95CDA8BB@sdfe.dk>
 <CAH0YoEOvDXbrztGDnN+TGqhh0bP_ZWYFO__engSiKOqien4xhQ@mail.gmail.com>
Message-ID: <1391D5A1-013D-4984-8ACC-1CD1CA1FBB38@sdfe.dk>

Forwarding Thomas’ +1 to the list


On 30 Dec 2019, at 11:27, Thomas Knudsen <knudsen.thomas at gmail.com<mailto:knudsen.thomas at gmail.com>> wrote:

+1 Thomas

man. 30. dec. 2019 11.06 skrev Kristian Evers <kreve at sdfe.dk<mailto:kreve at sdfe.dk>>:
PSC Members,

It appears that no major issues in the release candidates

  PROJ 6.3.0RC1
  proj-datumgrid-europe 1.5RC2
  proj-datumgrid-north-america 1.3RC1
  proj-datumgrid-oceania 1.1RC1

so I hereby motion that they all be promoted to final releases.

I’ll start with my +1.

/Kristian
_______________________________________________
PROJ mailing list
PROJ at lists.osgeo.org<mailto:PROJ at lists.osgeo.org>
https://lists.osgeo.org/mailman/listinfo/proj

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191230/7a55b0e5/attachment.html>

From howard at hobu.co  Mon Dec 30 05:44:26 2019
From: howard at hobu.co (Howard Butler)
Date: Mon, 30 Dec 2019 07:44:26 -0600
Subject: [PROJ] Motion: Promote 6.3.0 release candidates to final
	versions
In-Reply-To: <2490673.ABz3KE6i8B@even-i700>
References: <C466B0CD-B878-4E08-B28E-E19F95CDA8BB@sdfe.dk>
 <2490673.ABz3KE6i8B@even-i700>
Message-ID: <5419DDAA-A3EB-4EC3-B5A0-2D416B052D1A@hobu.co>

+1

Howard

> On Dec 30, 2019, at 4:14 AM, Even Rouault <even.rouault at spatialys.com> wrote:
> 
> Hi
> 
> +1
> 
> PROJ 6.3.0RC1 works fine with GDAL 3/master
> 
> Even
> 
>> PSC Members,
>> 
>> It appears that no major issues in the release candidates
>> 
>>  PROJ 6.3.0RC1
>>  proj-datumgrid-europe 1.5RC2
>>  proj-datumgrid-north-america 1.3RC1
>>  proj-datumgrid-oceania 1.1RC1
>> 
>> so I hereby motion that they all be promoted to final releases.
>> 
>> I’ll start with my +1.
>> 
>> /Kristian
>> _______________________________________________
>> PROJ mailing list
>> PROJ at lists.osgeo.org
>> https://lists.osgeo.org/mailman/listinfo/proj
> 
> 
> -- 
> Spatialys - Geospatial professional services
> http://www.spatialys.com
> _______________________________________________
> PROJ mailing list
> PROJ at lists.osgeo.org
> https://lists.osgeo.org/mailman/listinfo/proj


From Roger.Bivand at nhh.no  Mon Dec 30 07:50:24 2019
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Dec 2019 16:50:24 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
Message-ID: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>

Having installed 6.3.0RC1 and built GDAL 3.0.2 and GDAL master against it, 
I'm seeing a possible regression in GDAL function exportToProj4() after 
instantiating with EPSG:27700. With 6.2.1 I see "+proj=tmerc +lat_0=49 
+lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m 
+no_defs" but with 6.3.0RC1 and both GDAL versions "+proj=tmerc +lat_0=49 
+lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +no_defs". The 
offending code is on R-Forge at: 
https://r-forge.r-project.org/scm/viewvc.php/pkg/R/ogr_sp.R?view=markup&root=rgdal
function showSRID(), calling C++ function P6_SRID_show() at 
https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogr_proj.cpp?view=markup&root=rgdal.

In the failing case, importFromEPSG() seems to work, but for 6.3.0RC1, the 
string exported to Proj4 has lost +ellps= and +units=.

I imagine that this is my mistake, but help would be appreciated.

Roger

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From even.rouault at spatialys.com  Mon Dec 30 08:14:24 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 30 Dec 2019 17:14:24 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
Message-ID: <12543264.oWFVrH0HhN@even-i700>

On lundi 30 décembre 2019 16:50:24 CET Roger Bivand wrote:
> Having installed 6.3.0RC1 and built GDAL 3.0.2 and GDAL master against it,
> I'm seeing a possible regression in GDAL function exportToProj4() after
> instantiating with EPSG:27700. With 6.2.1 I see "+proj=tmerc +lat_0=49
> +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m
> +no_defs" but with 6.3.0RC1 and both GDAL versions "+proj=tmerc +lat_0=49
> +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +no_defs". The
> offending code is on R-Forge at:
> https://r-forge.r-project.org/scm/viewvc.php/pkg/R/ogr_sp.R?view=markup&root
> =rgdal function showSRID(), calling C++ function P6_SRID_show() at
> https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogr_proj.cpp?view=marku
> p&root=rgdal.
> 
> In the failing case, importFromEPSG() seems to work, but for 6.3.0RC1, the
> string exported to Proj4 has lost +ellps= and +units=.
> 
> I imagine that this is my mistake, but help would be appreciated.

Perhaps make sure PROJ_LIB is correctly set up / that you point to the right proj.db

Things work correctly for me:

$ projinfo EPSG:27700 -o PROJ
PROJ.4 string:
+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs

$ gdalsrsinfo EPSG:27700 -o PROJ4

+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Roger.Bivand at nhh.no  Mon Dec 30 08:46:19 2019
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Dec 2019 17:46:19 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <12543264.oWFVrH0HhN@even-i700>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <12543264.oWFVrH0HhN@even-i700>
Message-ID: <alpine.LFD.2.21.1912301717360.191194@reclus.nhh.no>

On Mon, 30 Dec 2019, Even Rouault wrote:

> On lundi 30 décembre 2019 16:50:24 CET Roger Bivand wrote:
>> Having installed 6.3.0RC1 and built GDAL 3.0.2 and GDAL master against it,
>> I'm seeing a possible regression in GDAL function exportToProj4() after
>> instantiating with EPSG:27700. With 6.2.1 I see "+proj=tmerc +lat_0=49
>> +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m
>> +no_defs" but with 6.3.0RC1 and both GDAL versions "+proj=tmerc +lat_0=49
>> +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +no_defs". The
>> offending code is on R-Forge at:
>> https://r-forge.r-project.org/scm/viewvc.php/pkg/R/ogr_sp.R?view=markup&root
>> =rgdal function showSRID(), calling C++ function P6_SRID_show() at
>> https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogr_proj.cpp?view=marku
>> p&root=rgdal.
>>
>> In the failing case, importFromEPSG() seems to work, but for 6.3.0RC1, the
>> string exported to Proj4 has lost +ellps= and +units=.
>>
>> I imagine that this is my mistake, but help would be appreciated.
>
> Perhaps make sure PROJ_LIB is correctly set up / that you point to the 
> right proj.db

There is only one in /usr/local/share/proj/proj.db, it is not that.

>
> Things work correctly for me:
>
> $ projinfo EPSG:27700 -o PROJ
> PROJ.4 string:
> +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs
>

This also worked for me, but is only in PROJ, not in PROJ and GDAL.

> $ gdalsrsinfo EPSG:27700 -o PROJ4
>
> +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
>

Having re-made GDAL against 6.3.0RC1 to check this, yes, I see the same 
with pkg-config proj --modversion 6.3.0 & gdal-config --version 3.1.0.



Speculation, does using SetAxisMappingStrategy(OAMS_TRADITIONAL_GIS_ORDER) 
make a difference? I'm setting this without checking to see whether the 
CRS is already in this order, but turning this off has no effect.

I have to ensure that R sp-based legacy workflows stay easting-northing in 
all circumstances, sf workflows going forward may pivot to authority 
compliance.

Anyway, rgdal is definitely broken with 6.3.0RC1 with no +ellps=airy term, 
and no +units=m term after exportToProj4(). What is the name of +ellps to 
use with GDALs OGRSpatialReference GetAttrValue()? Then I can check that 
it is present in the input node (must be there).

Roger

> Even
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From even.rouault at spatialys.com  Mon Dec 30 08:57:29 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 30 Dec 2019 17:57:29 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <alpine.LFD.2.21.1912301717360.191194@reclus.nhh.no>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <12543264.oWFVrH0HhN@even-i700>
 <alpine.LFD.2.21.1912301717360.191194@reclus.nhh.no>
Message-ID: <2111360.8MCUMNsuay@even-i700>

> Speculation, does using SetAxisMappingStrategy(OAMS_TRADITIONAL_GIS_ORDER)
> make a difference?

No, this is only used by GDAL for coordinate reprojection. No impact normally 
on PROJ4 string export

> Anyway, rgdal is definitely broken with 6.3.0RC1 with no +ellps=airy term,
> and no +units=m term after exportToProj4(). What is the name of +ellps to
> use with GDALs OGRSpatialReference GetAttrValue()? 

You can't. GetAttrValue() is about asking value of WKT(1) nodes, of PROJ.4 
string

I suggest that you try to do a minimal .c/.cpp reproducer using only GDAL API 
to identify if the issue is in GDAL/PROJ or in your code.

Even

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From even.rouault at spatialys.com  Mon Dec 30 09:05:38 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 30 Dec 2019 18:05:38 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <2111360.8MCUMNsuay@even-i700>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <alpine.LFD.2.21.1912301717360.191194@reclus.nhh.no>
 <2111360.8MCUMNsuay@even-i700>
Message-ID: <3254702.Kiozt7fVSo@even-i700>

> You can't. GetAttrValue() is about asking value of WKT(1) nodes, of PROJ.4
> string

I meant *not parts* of PROJ.4 string



-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Roger.Bivand at nhh.no  Mon Dec 30 09:22:34 2019
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Dec 2019 18:22:34 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <3254702.Kiozt7fVSo@even-i700>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <alpine.LFD.2.21.1912301717360.191194@reclus.nhh.no>
 <2111360.8MCUMNsuay@even-i700> <3254702.Kiozt7fVSo@even-i700>
Message-ID: <alpine.LFD.2.21.1912301812360.191638@reclus.nhh.no>

On Mon, 30 Dec 2019, Even Rouault wrote:

>> You can't. GetAttrValue() is about asking value of WKT(1) nodes, of PROJ.4
>> string
>
> I meant *not parts* of PROJ.4 string
>

No, but I can (and do) retreive DATUM and TOWGS84 from objects 
instantiated when reading vector files to check whether they vanish from 
proj4 strings.

Status is that legacy R-sp-workflows and modern R-sf-workflows will move 
to WKT2 as the internal CRS representation, but in the R-sp-workflow case, 
PROJ4 strings must be retained too, even though the WKT2 representations 
will be used for transformation and file export. PROJ4 strings are also 
needed to define CRS.

Here I'm just reporting a very puzzling issue. Checking across some other 
EPSG, it seems to affect EPSG:27700 (the Snow Soho Cholera data set, the 
Scottish lip cancer data set), and not in all contexts. However, the 
problem was not present with 6.2.1, and is now.

Roger

>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From even.rouault at spatialys.com  Mon Dec 30 09:49:42 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 30 Dec 2019 18:49:42 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <alpine.LFD.2.21.1912301812360.191638@reclus.nhh.no>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <3254702.Kiozt7fVSo@even-i700>
 <alpine.LFD.2.21.1912301812360.191638@reclus.nhh.no>
Message-ID: <12444714.vHJlNCFFNn@even-i700>

> Here I'm just reporting a very puzzling issue. Checking across some other
> EPSG, it seems to affect EPSG:27700

Looking a bit at
https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogr_proj.cpp?annotate=906&root=rgdal
I see several issues, but I don't think they explain the issue you get:

- memory leak: papszOptions set at lines 381 and 382 is not freed with CSLDestroy()

- use after free: ToWGS84 and Datum are used at lines 427 and 42. But they are owned by hSRS,
  which is itself owned by poLayer which is itself owned by poDS, which is freed at line 423.
  Furthermore, even if that issue is fixed, values returned by GetAttrValue() should be
  considered as short-lived, and valid only until the next call to the OSR API
  (some methods might modify the internal WKT representation and invalidate them)

- the call to morphFromESRI() at line 412 is no longer needed with GDAL 3.

I'd suggest you run this code under Valgrind.

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From Roger.Bivand at nhh.no  Mon Dec 30 09:52:27 2019
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Dec 2019 18:52:27 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <alpine.LFD.2.21.1912301812360.191638@reclus.nhh.no>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <alpine.LFD.2.21.1912301717360.191194@reclus.nhh.no>
 <2111360.8MCUMNsuay@even-i700> <3254702.Kiozt7fVSo@even-i700>
 <alpine.LFD.2.21.1912301812360.191638@reclus.nhh.no>
Message-ID: <alpine.LFD.2.21.1912301841590.191945@reclus.nhh.no>

On Mon, 30 Dec 2019, Roger Bivand wrote:

> On Mon, 30 Dec 2019, Even Rouault wrote:
>
>>>  You can't. GetAttrValue() is about asking value of WKT(1) nodes, of
>>>  PROJ.4
>>>  string
>>
>>  I meant *not parts* of PROJ.4 string
>> 
>
> No, but I can (and do) retreive DATUM and TOWGS84 from objects instantiated 
> when reading vector files to check whether they vanish from proj4 strings.
>
> Status is that legacy R-sp-workflows and modern R-sf-workflows will move to 
> WKT2 as the internal CRS representation, but in the R-sp-workflow case, PROJ4 
> strings must be retained too, even though the WKT2 representations will be 
> used for transformation and file export. PROJ4 strings are also needed to 
> define CRS.
>
> Here I'm just reporting a very puzzling issue. Checking across some other 
> EPSG, it seems to affect EPSG:27700 (the Snow Soho Cholera data set, the 
> Scottish lip cancer data set), and not in all contexts. However, the problem 
> was not present with 6.2.1, and is now.

Code:

#include <stdio.h>
#include "ogrsf_frmts.h"
#include <ogr_spatialref.h>

int main() {

   OGRSpatialReference hSRS = (OGRSpatialReference) NULL;
   char *pszSRS = NULL, *wkt = NULL;
   const char *datum, *ellps;
   hSRS.importFromEPSG(27700);
   datum = hSRS.GetAttrValue("DATUM");
   printf("datum: %s\n", datum);
   ellps = hSRS.GetAttrValue("DATUM|SPHEROID");
   printf("ellps: %s\n", ellps);
   hSRS.exportToProj4(&pszSRS);
   printf("PROJ4: %s\n", pszSRS);
   hSRS.exportToWkt(&wkt);
   printf("WKT2: %s\n", wkt);
   exit(0);
}

This performs correctly for me, 6.3.0RC1, GDAL master from about 13:30 
today (git has no revision numbers). So there must be a fragility in my 
code somewhere, that something in 6.3.0RC1 has exposed, but that is 
unknown and mostly 27700 at the moment as far as I can see - other 
discarded datums seem to try to inject a towgs84 or similar.

Roger

>
> Roger
>
>> 
>> 
>> 
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger.Bivand at nhh.no  Mon Dec 30 09:59:34 2019
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Dec 2019 18:59:34 +0100
Subject: [PROJ] Odd 6.3.0RC1 issue
In-Reply-To: <12444714.vHJlNCFFNn@even-i700>
References: <alpine.LFD.2.21.1912301625300.190856@reclus.nhh.no>
 <3254702.Kiozt7fVSo@even-i700>
 <alpine.LFD.2.21.1912301812360.191638@reclus.nhh.no>
 <12444714.vHJlNCFFNn@even-i700>
Message-ID: <alpine.LFD.2.21.1912301852470.191945@reclus.nhh.no>

On Mon, 30 Dec 2019, Even Rouault wrote:

>> Here I'm just reporting a very puzzling issue. Checking across some other
>> EPSG, it seems to affect EPSG:27700
>
> Looking a bit at
> https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogr_proj.cpp?annotate=906&root=rgdal
> I see several issues, but I don't think they explain the issue you get:
>
> - memory leak: papszOptions set at lines 381 and 382 is not freed with
>   CSLDestroy()

OK

>
> - use after free: ToWGS84 and Datum are used at lines 427 and 42. But
>   they are owned by hSRS, which is itself owned by poLayer which is
>   itself owned by poDS, which is freed at line 423.  Furthermore, even
>   if that issue is fixed, values returned by GetAttrValue() should be
>   considered as short-lived, and valid only until the next call to the
>   OSR API (some methods might modify the internal WKT representation and
>   invalidate them)
>

They have seemed robust so far - they are used to see whether a DATUM or 
TOWGS84 is present in the instantiated object, and then check after 
exportToProj4() to see if they vanished. I'll see whether I shouldn't copy 
them to SEXP straight away.

> - the call to morphFromESRI() at line 412 is no longer needed with GDAL
>   3.

OK

>
> I'd suggest you run this code under Valgrind.

Thanks, I'm just trying to avoid falling too far behind, valgrind is lower 
down the to-do list.

Code example sent crossing your reply - conclusion there that this is in 
my code but has surfaced through something changing between 6.2.1 and 
6.3.0.

Roger

>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From julien.schroder at gmail.com  Mon Dec 30 12:04:50 2019
From: julien.schroder at gmail.com (Julien Schroder)
Date: Mon, 30 Dec 2019 13:04:50 -0700
Subject: [PROJ] Right way to do this transformation/projection : ITRF2014 ->
	WGS84 UTM
Message-ID: <CANhNitrArt3dM0h9v8df80ychCnGm=knWwFR3RPrwpuwOGgUSg@mail.gmail.com>

Hello,
First I want to apologize as this is a bit of a beginner question but after
hours of experimentation I am still confused.

I have coordinates in ITRF2014 (both in geocentric and geodetic
coordinates) with an associated EPOCH which I am trying to
transform/project to WGS84 UTM18N using pyproj/PROJ and EPSG codes. I am
working with surveying data, trying to maximize the accuracy. So far my
group made the approximation that coordinates in ITRF2014 were close enough
to WGS84 to not bother with the transformation but I would like to change
that.

I am looking for advice on how to do this properly :

   - Would you preferably use the geocentric coordinates for the
   transformation/projection?
   - Could I just do something like this :

#with EPSG 7789 +> ITRF2014 geocentric and EPSG 32618 WGS84 UTM18N

transformer = Transformer.from_crs(7789,32618)

X2,Y2, Z2, _ = transformer.transform(xx=X, yy=Y,zz=Z, tt=g.epoch)


During my experimentation I noticed that doing EPSG 7789 => EPSG 32618
holds different results (about 4cm diffrence) than  EPSG 7789  => EPSG 4919
=> EPSG 32614
Which surprised me as I thought that going from ITRF2014 to WGS84 involved
a step through ITRF2000.

In the same way converting between ITRF geocentric (EPSG 7789) and ITRF2014
lat long Height (EPSG 7912) gives me some 7cm differences which confused me
a lot and lead me to ask for help.

As a bonus question, do you know of a tool that support those
transformation and could help me validate my results?

Thanks a lot for your help!
Happy end of the year!
Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.osgeo.org/pipermail/proj/attachments/20191230/d4b279be/attachment.html>

From even.rouault at spatialys.com  Mon Dec 30 12:53:37 2019
From: even.rouault at spatialys.com (Even Rouault)
Date: Mon, 30 Dec 2019 21:53:37 +0100
Subject: [PROJ] Right way to do this transformation/projection :
	ITRF2014 -> WGS84 UTM
In-Reply-To: <CANhNitrArt3dM0h9v8df80ychCnGm=knWwFR3RPrwpuwOGgUSg@mail.gmail.com>
References: <CANhNitrArt3dM0h9v8df80ychCnGm=knWwFR3RPrwpuwOGgUSg@mail.gmail.com>
Message-ID: <1920261.PTuudJocza@even-i700>

On lundi 30 décembre 2019 13:04:50 CET Julien Schroder wrote:
> Hello,
> First I want to apologize as this is a bit of a beginner question but after
> hours of experimentation I am still confused.

I find this is a rather advanced one :-)

You didn't specify which PROJ version you use. You might get different results 
as we have refined such transformations over time.

Hint: the latest one will probably be the better.

> 
> I have coordinates in ITRF2014 (both in geocentric and geodetic
> coordinates) with an associated EPOCH which I am trying to
> transform/project to WGS84 UTM18N using pyproj/PROJ and EPSG codes.

The "WGS84" datum as used by EPSG:4326, EPSG:4979 or any projected CRS based 
on them such as EPSG:32618 is a "datum ensemble", that is one of the 6 
possible realizations of the "WGS84 (Gxxxx)" datums, and the accuracy of 
coordinates in this "datum ensemble" cannot be better than 2 meters, so you 
should only get null transformations when transforming between it and 
ITRF2014.

With PROJ 6.3, you'll get non-null transformations if transforming between 
ITRF2014 and "WGS84 (G1762)" (*)

$ projinfo -s ITRF2014 -t "WGS 84 (G1762)" --spatial-test intersects --summary
Candidate operations found: 2
unknown id, Conversion from ITRF2014 (geog2D) to ITRF2014 (geocentric) + 
Inverse of ITRF2008 to ITRF2014 (1) + Inverse of WGS 84 (G1762) to ITRF2008 
(1) + Conversion from WGS 84 (G1762) (geocentric) to WGS 84 (G1762) (geog2D), 
0.02 m, World
unknown id, Conversion from ITRF2014 (geog2D) to ITRF2014 (geocentric) + 
ITRF2014 to GDA2020 (1) + GDA2020 to WGS 84 (G1762) (1) + Conversion from WGS 
84 (G1762) (geocentric) to WGS 84 (G1762) (geog2D), 0.201 m, Australia - GDA

> During my experimentation I noticed that doing EPSG 7789 => EPSG 32618
> holds different results (about 4cm diffrence) than  EPSG 7789  => EPSG 4919
> => EPSG 32614

Hum, doesn't make sense to me. Can you demonstrate this with PROJ command line 
utilities ?

> Which surprised me as I thought that going from ITRF2014 to WGS84 involved
> a step through ITRF2000.

How did you see that ? In the above, the first result, which has worldwide 
validity, goes from ITRF2014 to WGS 84 (G1762) through ITRF2008. Because there 
is no direct transformation between ITRF2014 and WGS 84 (G1762) in the EPSG 
dataset (I presume the reason is that WGS 84 (G1762) must have been defined 
from ITRF2008)

Even

(*) well, this isn't completely true. If you try
projinfo -s ITRF2014 -t WGS84 --spatial-test intersects --summary
with latest PROJ 6.3, you'll get not-null transformations in the USA in the 
Gulf of Mexico, but I think they are questionable as they involve a lot of 
hoops through other CRS such as ITRF2008, NAD83(2011), NAD83 and NAD27 ! This 
is due to latest EPSG database having a complex NAD27->ITRF2014 concatenated 
operation for Gulf of Mexico (using the NAD83, NAD83(2011) and ITRF2008 
intermediates), and thus it is chained with transformations from NAD27 to 
WGS84...

-- 
Spatialys - Geospatial professional services
http://www.spatialys.com

From gdt at lexort.com  Mon Dec 30 15:59:12 2019
From: gdt at lexort.com (Greg Troxel)
Date: Mon, 30 Dec 2019 18:59:12 -0500
Subject: [PROJ] Right way to do this transformation/projection :
	ITRF2014 -> WGS84 UTM
In-Reply-To: <1920261.PTuudJocza@even-i700> (Even Rouault's message of "Mon,
 30 Dec 2019 21:53:37 +0100")
References: <CANhNitrArt3dM0h9v8df80ychCnGm=knWwFR3RPrwpuwOGgUSg@mail.gmail.com>
 <1920261.PTuudJocza@even-i700>
Message-ID: <rmipng5wejz.fsf@s1.lexort.com>

Even Rouault <even.rouault at spatialys.com> writes:

. On lundi 30 décembre 2019 13:04:50 CET Julien Schroder wrote:
> Hum, doesn't make sense to me. Can you demonstrate this with PROJ command line 
> utilities ?
>
>> Which surprised me as I thought that going from ITRF2014 to WGS84 involved
>> a step through ITRF2000.

Careful - I think you mean "from ITRF2014 to WGS84(G1762)".  Each WGS84
realization has a separate relationship to various ITRF and NAD83.   I
am not clear if this link is fully correct (although I suspect it is):

  https://www.e-education.psu.edu/geog862/node/1804

One of the earlier WGS84 realizations seems equivalent to ITRF2000.

> How did you see that ? In the above, the first result, which has worldwide 
> validity, goes from ITRF2014 to WGS 84 (G1762) through ITRF2008. Because there 
> is no direct transformation between ITRF2014 and WGS 84 (G1762) in the EPSG 
> dataset (I presume the reason is that WGS 84 (G1762) must have been defined 
> from ITRF2008)

It seems that WGS84(G1762) is defined by station coordinates at a number
of sites that also help to define ITRF2008.  This paper has the details
(and a reference that I have not looked for yet, but seems interesting).

  https://www.unoosa.org/pdf/icg/2016/icg11/wgd/02wgd.pdf

> (*) well, this isn't completely true. If you try
> projinfo -s ITRF2014 -t WGS84 --spatial-test intersects --summary
> with latest PROJ 6.3, you'll get not-null transformations in the USA in the 
> Gulf of Mexico, but I think they are questionable as they involve a lot of 
> hoops through other CRS such as ITRF2008, NAD83(2011), NAD83 and NAD27 ! This 
> is due to latest EPSG database having a complex NAD27->ITRF2014 concatenated 
> operation for Gulf of Mexico (using the NAD83, NAD83(2011) and ITRF2008 
> intermediates), and thus it is chained with transformations from NAD27 to 
> WGS84...

Wow!

